{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c730ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28cbbbe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>lang_abv</th>\n",
       "      <th>language</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5130fd2cb5</td>\n",
       "      <td>and these comments were considered in formulat...</td>\n",
       "      <td>The rules developed in the interim were put to...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5b72532a0b</td>\n",
       "      <td>These are issues that we wrestle with in pract...</td>\n",
       "      <td>Practice groups are not permitted to work on t...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3931fbe82a</td>\n",
       "      <td>Des petites choses comme celles-là font une di...</td>\n",
       "      <td>J'essayais d'accomplir quelque chose.</td>\n",
       "      <td>fr</td>\n",
       "      <td>French</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5622f0c60b</td>\n",
       "      <td>you know they can't really defend themselves l...</td>\n",
       "      <td>They can't defend themselves because of their ...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86aaa48b45</td>\n",
       "      <td>ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...</td>\n",
       "      <td>เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร</td>\n",
       "      <td>th</td>\n",
       "      <td>Thai</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                            premise  \\\n",
       "0  5130fd2cb5  and these comments were considered in formulat...   \n",
       "1  5b72532a0b  These are issues that we wrestle with in pract...   \n",
       "2  3931fbe82a  Des petites choses comme celles-là font une di...   \n",
       "3  5622f0c60b  you know they can't really defend themselves l...   \n",
       "4  86aaa48b45  ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...   \n",
       "\n",
       "                                          hypothesis lang_abv language  label  \n",
       "0  The rules developed in the interim were put to...       en  English      0  \n",
       "1  Practice groups are not permitted to work on t...       en  English      2  \n",
       "2              J'essayais d'accomplir quelque chose.       fr   French      0  \n",
       "3  They can't defend themselves because of their ...       en  English      0  \n",
       "4    เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร       th     Thai      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('augmented_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff9c7b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12120, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18d306fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "English       6870\n",
       "Chinese        411\n",
       "Arabic         401\n",
       "French         390\n",
       "Swahili        385\n",
       "Urdu           381\n",
       "Vietnamese     379\n",
       "Russian        376\n",
       "Hindi          374\n",
       "Greek          372\n",
       "Thai           371\n",
       "Spanish        366\n",
       "Turkish        351\n",
       "German         351\n",
       "Bulgarian      342\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd7696ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['premise'] = df['premise'].str.replace('.', '',regex=True)\n",
    "df['premise'] = df['premise'].str.replace('http\\S+|www.\\S+', '',regex=True)\n",
    "\n",
    "df['hypothesis'] = df['hypothesis'].str.replace('.', '',regex=True)\n",
    "df['hypothesis'] = df['hypothesis'].str.replace('http\\S+|www.\\S+', '',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba39616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "shuffled_df = shuffle(df)\n",
    "\n",
    "train, test = train_test_split(\n",
    "                    shuffled_df, train_size = 0.8, test_size = 0.2, shuffle=True, \n",
    "                    stratify = shuffled_df['label'])\n",
    "\n",
    "# x_train1, x_train2, y_train1, y_train2 = train_test_split(\n",
    "#                     x_train, y_train, \n",
    "#                     train_size = 0.5, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d624949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.drop(['label', 'id'], axis = 1)\n",
    "y_train = train['label']\n",
    "\n",
    "x_test = test.drop(['label', 'id'], axis = 1)\n",
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afe88a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df, shuffled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "999609a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9696, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab0c93dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9696,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cce9c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "from tqdm import tqdm\n",
    "def extract_mbert_cls_only(list_of_premises=[], list_of_hypothesis=[], batch_size=16):\n",
    "    cls_hidden_states = []\n",
    "    if len(list_of_premises)<batch_size: #go longer ones we should batch\n",
    "        tknzed= tokenizer(list_of_premises, list_of_hypothesis, return_tensors=\"pt\", padding=True)\n",
    "        b=model(**tknzed)\n",
    "        cls_hidden_states = torch.squeeze(b.last_hidden_state[:,0,:])\n",
    "    else:\n",
    "        for i in tqdm(range(0,len(list_of_premises),batch_size)):    \n",
    "            tknzed= tokenizer(list_of_premises[i:min(len(list_of_premises),i+batch_size)], list_of_hypothesis[i:min(len(list_of_hypothesis),i+batch_size)], return_tensors=\"pt\", padding=True)\n",
    "            b=model(**tknzed)\n",
    "            cls_hidden_states.append(torch.squeeze(b.last_hidden_state[:,0,:]))\n",
    "#             print(b.last_hidden_state[:,0,:].shape)\n",
    "#             print(b.last_hidden_state[:,1:,:].shape)\n",
    "    return cls_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f4fb5",
   "metadata": {},
   "source": [
    "### For only CLS features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53d91f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▊                                          | 1/25 [00:00<00:09,  2.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▌                                        | 2/25 [00:01<00:14,  1.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████▎                                      | 3/25 [00:01<00:12,  1.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|███████                                     | 4/25 [00:02<00:11,  1.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▊                                   | 5/25 [00:02<00:11,  1.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██████████▌                                 | 6/25 [00:03<00:09,  1.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|████████████▎                               | 7/25 [00:03<00:09,  1.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████                              | 8/25 [00:04<00:08,  1.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▊                            | 9/25 [00:04<00:08,  1.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|█████████████████▏                         | 10/25 [00:05<00:09,  1.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|██████████████████▉                        | 11/25 [00:06<00:09,  1.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████▋                      | 12/25 [00:07<00:09,  1.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████▎                    | 13/25 [00:08<00:09,  1.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|████████████████████████                   | 14/25 [00:08<00:08,  1.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:10<00:08,  1.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▌               | 16/25 [00:11<00:09,  1.01s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:12<00:07,  1.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:12<00:05,  1.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:14<00:06,  1.09s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:15<00:05,  1.01s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████       | 21/25 [00:15<00:03,  1.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:16<00:02,  1.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:17<00:01,  1.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:18<00:00,  1.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 25/25 [00:19<00:00,  1.30it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(400, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▊                                          | 1/25 [00:00<00:15,  1.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▌                                        | 2/25 [00:01<00:12,  1.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████▎                                      | 3/25 [00:01<00:12,  1.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|███████                                     | 4/25 [00:02<00:10,  1.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▊                                   | 5/25 [00:02<00:08,  2.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██████████▌                                 | 6/25 [00:02<00:08,  2.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|████████████▎                               | 7/25 [00:03<00:07,  2.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████                              | 8/25 [00:03<00:07,  2.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▊                            | 9/25 [00:04<00:10,  1.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|█████████████████▏                         | 10/25 [00:05<00:11,  1.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|██████████████████▉                        | 11/25 [00:06<00:10,  1.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████▋                      | 12/25 [00:07<00:11,  1.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████▎                    | 13/25 [00:08<00:10,  1.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|████████████████████████                   | 14/25 [00:09<00:09,  1.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:10<00:09,  1.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▌               | 16/25 [00:11<00:07,  1.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:11<00:06,  1.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:12<00:05,  1.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:13<00:04,  1.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:13<00:03,  1.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████       | 21/25 [00:14<00:02,  1.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:14<00:01,  1.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:15<00:01,  1.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:16<00:00,  1.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 25/25 [00:16<00:00,  1.48it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "(800, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▊                                          | 1/25 [00:00<00:23,  1.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▌                                        | 2/25 [00:01<00:16,  1.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████▎                                      | 3/25 [00:02<00:13,  1.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|███████                                     | 4/25 [00:02<00:11,  1.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▊                                   | 5/25 [00:02<00:10,  1.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██████████▌                                 | 6/25 [00:03<00:09,  1.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|████████████▎                               | 7/25 [00:03<00:09,  1.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████                              | 8/25 [00:04<00:08,  1.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▊                            | 9/25 [00:04<00:08,  1.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|█████████████████▏                         | 10/25 [00:05<00:08,  1.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|██████████████████▉                        | 11/25 [00:07<00:15,  1.08s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████▋                      | 12/25 [00:09<00:15,  1.18s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████▎                    | 13/25 [00:10<00:14,  1.20s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|████████████████████████                   | 14/25 [00:13<00:19,  1.76s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:14<00:14,  1.44s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▌               | 16/25 [00:15<00:11,  1.28s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:16<00:10,  1.28s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:17<00:08,  1.26s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:18<00:06,  1.11s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:19<00:04,  1.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████       | 21/25 [00:20<00:03,  1.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:20<00:02,  1.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:21<00:01,  1.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:21<00:00,  1.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 25/25 [00:22<00:00,  1.11it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "(1200, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▊                                          | 1/25 [00:00<00:13,  1.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▌                                        | 2/25 [00:01<00:12,  1.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████▎                                      | 3/25 [00:01<00:10,  2.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|███████                                     | 4/25 [00:01<00:09,  2.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▊                                   | 5/25 [00:02<00:08,  2.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██████████▌                                 | 6/25 [00:02<00:07,  2.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|████████████▎                               | 7/25 [00:03<00:08,  2.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████                              | 8/25 [00:04<00:11,  1.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▊                            | 9/25 [00:05<00:11,  1.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|█████████████████▏                         | 10/25 [00:05<00:11,  1.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|██████████████████▉                        | 11/25 [00:07<00:13,  1.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████▋                      | 12/25 [00:07<00:11,  1.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████▎                    | 13/25 [00:08<00:10,  1.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|████████████████████████                   | 14/25 [00:09<00:09,  1.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:10<00:08,  1.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▌               | 16/25 [00:11<00:06,  1.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:11<00:05,  1.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:12<00:04,  1.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:12<00:03,  1.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:13<00:03,  1.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████       | 21/25 [00:14<00:02,  1.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:14<00:01,  1.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:15<00:01,  1.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:16<00:00,  1.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 25/25 [00:16<00:00,  1.50it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "(1600, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▊                                          | 1/25 [00:00<00:15,  1.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▌                                        | 2/25 [00:01<00:11,  2.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████▎                                      | 3/25 [00:01<00:13,  1.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|███████                                     | 4/25 [00:02<00:10,  1.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▊                                   | 5/25 [00:02<00:09,  2.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██████████▌                                 | 6/25 [00:02<00:07,  2.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|████████████▎                               | 7/25 [00:03<00:07,  2.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████                              | 8/25 [00:03<00:07,  2.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▊                            | 9/25 [00:05<00:13,  1.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|█████████████████▏                         | 10/25 [00:06<00:11,  1.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|██████████████████▉                        | 11/25 [00:06<00:09,  1.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████▋                      | 12/25 [00:06<00:07,  1.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████▎                    | 13/25 [00:07<00:06,  1.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|████████████████████████                   | 14/25 [00:08<00:07,  1.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:09<00:07,  1.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▌               | 16/25 [00:09<00:06,  1.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:10<00:05,  1.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:11<00:05,  1.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:12<00:04,  1.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:12<00:03,  1.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████       | 21/25 [00:14<00:03,  1.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:14<00:02,  1.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:15<00:01,  1.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:16<00:00,  1.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 25/25 [00:16<00:00,  1.48it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n",
      "(2000, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▊                                          | 1/25 [00:00<00:11,  2.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▌                                        | 2/25 [00:00<00:09,  2.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████▎                                      | 3/25 [00:01<00:11,  1.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|███████                                     | 4/25 [00:01<00:09,  2.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▊                                   | 5/25 [00:02<00:10,  1.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██████████▌                                 | 6/25 [00:02<00:08,  2.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|████████████▎                               | 7/25 [00:03<00:08,  2.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████                              | 8/25 [00:03<00:07,  2.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▊                            | 9/25 [00:04<00:07,  2.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|█████████████████▏                         | 10/25 [00:04<00:06,  2.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|██████████████████▉                        | 11/25 [00:06<00:12,  1.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████▋                      | 12/25 [00:07<00:10,  1.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████▎                    | 13/25 [00:07<00:08,  1.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|████████████████████████                   | 14/25 [00:08<00:07,  1.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:09<00:07,  1.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▌               | 16/25 [00:09<00:06,  1.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:11<00:07,  1.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:12<00:06,  1.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:13<00:05,  1.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:13<00:04,  1.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████       | 21/25 [00:14<00:02,  1.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:15<00:02,  1.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:15<00:01,  1.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:16<00:00,  1.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 25/25 [00:17<00:00,  1.42it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "(2400, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▊                                          | 1/25 [00:00<00:10,  2.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▌                                        | 2/25 [00:01<00:11,  1.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████▎                                      | 3/25 [00:01<00:11,  1.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|███████                                     | 4/25 [00:02<00:11,  1.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▊                                   | 5/25 [00:02<00:10,  1.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██████████▌                                 | 6/25 [00:03<00:09,  1.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|████████████▎                               | 7/25 [00:03<00:08,  2.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████                              | 8/25 [00:03<00:08,  2.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▊                            | 9/25 [00:04<00:07,  2.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|█████████████████▏                         | 10/25 [00:05<00:11,  1.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|██████████████████▉                        | 11/25 [00:06<00:10,  1.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████▋                      | 12/25 [00:07<00:09,  1.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████▎                    | 13/25 [00:08<00:10,  1.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|████████████████████████                   | 14/25 [00:09<00:08,  1.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:10<00:09,  1.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▌               | 16/25 [00:11<00:09,  1.02s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:13<00:10,  1.31s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:14<00:07,  1.06s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:14<00:05,  1.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:16<00:06,  1.31s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████       | 21/25 [00:17<00:04,  1.16s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:18<00:03,  1.01s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:19<00:01,  1.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:19<00:00,  1.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 25/25 [00:20<00:00,  1.22it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "(2800, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▊                                          | 1/25 [00:00<00:10,  2.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▌                                        | 2/25 [00:01<00:12,  1.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████▎                                      | 3/25 [00:01<00:11,  1.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|███████                                     | 4/25 [00:02<00:10,  1.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▊                                   | 5/25 [00:02<00:10,  1.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██████████▌                                 | 6/25 [00:02<00:08,  2.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|████████████▎                               | 7/25 [00:03<00:08,  2.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████                              | 8/25 [00:03<00:07,  2.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▊                            | 9/25 [00:05<00:12,  1.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|█████████████████▏                         | 10/25 [00:05<00:10,  1.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|██████████████████▉                        | 11/25 [00:06<00:09,  1.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████▋                      | 12/25 [00:07<00:08,  1.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████▎                    | 13/25 [00:07<00:08,  1.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|████████████████████████                   | 14/25 [00:08<00:07,  1.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:09<00:06,  1.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▌               | 16/25 [00:09<00:05,  1.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:10<00:04,  1.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:11<00:05,  1.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:12<00:04,  1.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:13<00:04,  1.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████       | 21/25 [00:13<00:03,  1.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:14<00:02,  1.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:15<00:01,  1.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:15<00:00,  1.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 25/25 [00:16<00:00,  1.52it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800\n",
      "(3200, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▊                                          | 1/25 [00:00<00:12,  1.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▌                                        | 2/25 [00:00<00:09,  2.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████▎                                      | 3/25 [00:01<00:09,  2.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|███████                                     | 4/25 [00:01<00:08,  2.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▊                                   | 5/25 [00:02<00:07,  2.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██████████▌                                 | 6/25 [00:02<00:07,  2.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|████████████▎                               | 7/25 [00:02<00:07,  2.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████                              | 8/25 [00:03<00:07,  2.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▊                            | 9/25 [00:03<00:07,  2.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|█████████████████▏                         | 10/25 [00:04<00:07,  2.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|██████████████████▉                        | 11/25 [00:06<00:11,  1.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████▋                      | 12/25 [00:06<00:10,  1.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████▎                    | 13/25 [00:07<00:09,  1.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|████████████████████████                   | 14/25 [00:07<00:07,  1.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:08<00:07,  1.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▌               | 16/25 [00:09<00:06,  1.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:10<00:06,  1.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:10<00:05,  1.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:11<00:04,  1.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:12<00:03,  1.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████       | 21/25 [00:12<00:02,  1.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:14<00:02,  1.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:15<00:01,  1.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:15<00:00,  1.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 25/25 [00:16<00:00,  1.50it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200\n",
      "(3600, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▊                                          | 1/25 [00:00<00:12,  1.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▌                                        | 2/25 [00:00<00:11,  2.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████▎                                      | 3/25 [00:01<00:09,  2.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|███████                                     | 4/25 [00:01<00:10,  2.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▊                                   | 5/25 [00:02<00:10,  1.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██████████▌                                 | 6/25 [00:02<00:09,  2.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|████████████▎                               | 7/25 [00:03<00:08,  2.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████                              | 8/25 [00:03<00:08,  1.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▊                            | 9/25 [00:05<00:11,  1.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|█████████████████▏                         | 10/25 [00:06<00:11,  1.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|██████████████████▉                        | 11/25 [00:07<00:11,  1.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████▋                      | 12/25 [00:07<00:10,  1.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████▎                    | 13/25 [00:08<00:09,  1.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|████████████████████████                   | 14/25 [00:09<00:08,  1.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:10<00:07,  1.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▌               | 16/25 [00:10<00:06,  1.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:11<00:05,  1.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:11<00:04,  1.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:12<00:03,  1.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:12<00:02,  1.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████       | 21/25 [00:13<00:02,  1.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:14<00:01,  1.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:14<00:01,  1.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:15<00:00,  1.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 25/25 [00:16<00:00,  1.55it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600\n",
      "(4000, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▊                                          | 1/25 [00:00<00:12,  1.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▌                                        | 2/25 [00:00<00:10,  2.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████▎                                      | 3/25 [00:01<00:08,  2.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|███████                                     | 4/25 [00:02<00:12,  1.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▊                                   | 5/25 [00:02<00:13,  1.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██████████▌                                 | 6/25 [00:03<00:10,  1.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|████████████▎                               | 7/25 [00:03<00:08,  2.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████                              | 8/25 [00:04<00:09,  1.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▊                            | 9/25 [00:05<00:13,  1.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|█████████████████▏                         | 10/25 [00:06<00:12,  1.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|██████████████████▉                        | 11/25 [00:07<00:10,  1.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████▋                      | 12/25 [00:07<00:09,  1.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████▎                    | 13/25 [00:08<00:08,  1.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|████████████████████████                   | 14/25 [00:09<00:07,  1.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:09<00:06,  1.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▌               | 16/25 [00:10<00:06,  1.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:11<00:05,  1.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:12<00:05,  1.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:13<00:05,  1.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:13<00:03,  1.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████       | 21/25 [00:14<00:02,  1.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:14<00:02,  1.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:15<00:01,  1.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:16<00:00,  1.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 25/25 [00:17<00:00,  1.41it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "(4400, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▊                                          | 1/25 [00:00<00:12,  1.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▌                                        | 2/25 [00:01<00:13,  1.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████▎                                      | 3/25 [00:01<00:11,  2.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|███████                                     | 4/25 [00:01<00:09,  2.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▊                                   | 5/25 [00:02<00:11,  1.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██████████▌                                 | 6/25 [00:03<00:09,  1.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|████████████▎                               | 7/25 [00:03<00:08,  2.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████                              | 8/25 [00:04<00:08,  1.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▊                            | 9/25 [00:04<00:07,  2.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|█████████████████▏                         | 10/25 [00:06<00:12,  1.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|██████████████████▉                        | 11/25 [00:06<00:10,  1.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████▋                      | 12/25 [00:07<00:08,  1.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████▎                    | 13/25 [00:07<00:07,  1.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|████████████████████████                   | 14/25 [00:08<00:06,  1.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:08<00:06,  1.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▌               | 16/25 [00:09<00:05,  1.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:10<00:05,  1.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:10<00:04,  1.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:11<00:03,  1.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:12<00:03,  1.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████       | 21/25 [00:12<00:02,  1.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:13<00:01,  1.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:13<00:01,  1.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:14<00:00,  1.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 25/25 [00:14<00:00,  1.68it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4400\n",
      "(4800, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▊                                          | 1/25 [00:00<00:10,  2.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▌                                        | 2/25 [00:00<00:10,  2.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████▎                                      | 3/25 [00:01<00:11,  1.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|███████                                     | 4/25 [00:01<00:10,  2.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▊                                   | 5/25 [00:02<00:08,  2.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██████████▌                                 | 6/25 [00:02<00:08,  2.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|████████████▎                               | 7/25 [00:03<00:10,  1.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████                              | 8/25 [00:03<00:08,  2.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▊                            | 9/25 [00:04<00:07,  2.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|█████████████████▏                         | 10/25 [00:05<00:11,  1.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|██████████████████▉                        | 11/25 [00:06<00:11,  1.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████▋                      | 12/25 [00:07<00:09,  1.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████▎                    | 13/25 [00:07<00:09,  1.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|████████████████████████                   | 14/25 [00:08<00:07,  1.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:08<00:06,  1.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▌               | 16/25 [00:09<00:05,  1.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:10<00:04,  1.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:10<00:04,  1.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:11<00:03,  1.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:12<00:03,  1.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████       | 21/25 [00:12<00:02,  1.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:13<00:01,  1.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:14<00:01,  1.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:16<00:01,  1.05s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 25/25 [00:16<00:00,  1.50it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800\n",
      "(5200, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▊                                          | 1/25 [00:00<00:10,  2.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▌                                        | 2/25 [00:00<00:11,  2.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████▎                                      | 3/25 [00:01<00:09,  2.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|███████                                     | 4/25 [00:01<00:08,  2.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▊                                   | 5/25 [00:02<00:08,  2.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██████████▌                                 | 6/25 [00:02<00:09,  1.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|████████████▎                               | 7/25 [00:03<00:09,  1.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████                              | 8/25 [00:03<00:09,  1.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▊                            | 9/25 [00:04<00:09,  1.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|█████████████████▏                         | 10/25 [00:06<00:13,  1.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|██████████████████▉                        | 11/25 [00:06<00:11,  1.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████▋                      | 12/25 [00:07<00:09,  1.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████▎                    | 13/25 [00:07<00:08,  1.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|████████████████████████                   | 14/25 [00:08<00:08,  1.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:09<00:07,  1.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▌               | 16/25 [00:09<00:05,  1.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:10<00:04,  1.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:11<00:04,  1.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:11<00:03,  1.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:12<00:03,  1.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████       | 21/25 [00:13<00:02,  1.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:14<00:02,  1.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:14<00:01,  1.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:15<00:00,  1.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 25/25 [00:16<00:00,  1.54it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5200\n",
      "(5600, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▊                                          | 1/25 [00:00<00:16,  1.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▌                                        | 2/25 [00:01<00:11,  1.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████▎                                      | 3/25 [00:01<00:09,  2.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|███████                                     | 4/25 [00:01<00:08,  2.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▊                                   | 5/25 [00:02<00:07,  2.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██████████▌                                 | 6/25 [00:02<00:07,  2.51it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|████████████▎                               | 7/25 [00:03<00:12,  1.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████                              | 8/25 [00:04<00:10,  1.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▊                            | 9/25 [00:05<00:12,  1.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|█████████████████▏                         | 10/25 [00:06<00:12,  1.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|██████████████████▉                        | 11/25 [00:07<00:11,  1.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████▋                      | 12/25 [00:07<00:09,  1.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████▎                    | 13/25 [00:08<00:08,  1.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|████████████████████████                   | 14/25 [00:08<00:07,  1.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:09<00:06,  1.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▌               | 16/25 [00:10<00:05,  1.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:10<00:04,  1.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:11<00:04,  1.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:12<00:04,  1.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:12<00:03,  1.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████       | 21/25 [00:13<00:02,  1.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:13<00:01,  1.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:14<00:01,  1.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:15<00:00,  1.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 25/25 [00:15<00:00,  1.59it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600\n",
      "(6000, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▊                                          | 1/25 [00:00<00:14,  1.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▌                                        | 2/25 [00:01<00:12,  1.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████▎                                      | 3/25 [00:01<00:12,  1.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|███████                                     | 4/25 [00:02<00:10,  2.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▊                                   | 5/25 [00:02<00:08,  2.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██████████▌                                 | 6/25 [00:03<00:09,  1.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|████████████▎                               | 7/25 [00:03<00:11,  1.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████                              | 8/25 [00:04<00:09,  1.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▊                            | 9/25 [00:04<00:07,  2.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|█████████████████▏                         | 10/25 [00:05<00:10,  1.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|██████████████████▉                        | 11/25 [00:06<00:10,  1.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████▋                      | 12/25 [00:07<00:08,  1.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████▎                    | 13/25 [00:07<00:07,  1.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|████████████████████████                   | 14/25 [00:08<00:06,  1.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:08<00:05,  1.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▌               | 16/25 [00:09<00:05,  1.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:10<00:05,  1.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:10<00:04,  1.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:11<00:04,  1.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:12<00:03,  1.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████       | 21/25 [00:12<00:02,  1.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:13<00:01,  1.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:13<00:01,  1.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:14<00:00,  1.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 25/25 [00:15<00:00,  1.66it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "(6400, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▊                                          | 1/25 [00:00<00:11,  2.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▌                                        | 2/25 [00:00<00:10,  2.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████▎                                      | 3/25 [00:01<00:09,  2.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|███████                                     | 4/25 [00:01<00:08,  2.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▊                                   | 5/25 [00:02<00:10,  1.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██████████▌                                 | 6/25 [00:02<00:09,  2.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|████████████▎                               | 7/25 [00:03<00:08,  2.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████                              | 8/25 [00:03<00:08,  2.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▊                            | 9/25 [00:04<00:07,  2.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|█████████████████▏                         | 10/25 [00:05<00:12,  1.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|██████████████████▉                        | 11/25 [00:06<00:10,  1.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████▋                      | 12/25 [00:07<00:09,  1.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████▎                    | 13/25 [00:08<00:10,  1.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|████████████████████████                   | 14/25 [00:08<00:08,  1.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:09<00:07,  1.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▌               | 16/25 [00:10<00:06,  1.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:10<00:05,  1.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:11<00:04,  1.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:11<00:03,  1.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:12<00:02,  1.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████       | 21/25 [00:13<00:02,  1.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:13<00:01,  1.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:14<00:01,  1.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:15<00:00,  1.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 25/25 [00:15<00:00,  1.59it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400\n",
      "(6800, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▊                                          | 1/25 [00:00<00:15,  1.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▌                                        | 2/25 [00:01<00:12,  1.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████▎                                      | 3/25 [00:01<00:14,  1.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|███████                                     | 4/25 [00:02<00:12,  1.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▊                                   | 5/25 [00:02<00:10,  1.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██████████▌                                 | 6/25 [00:03<00:08,  2.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|████████████▎                               | 7/25 [00:03<00:08,  2.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████                              | 8/25 [00:04<00:07,  2.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▊                            | 9/25 [00:04<00:06,  2.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|█████████████████▏                         | 10/25 [00:04<00:06,  2.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|██████████████████▉                        | 11/25 [00:06<00:10,  1.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████▋                      | 12/25 [00:07<00:10,  1.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████▎                    | 13/25 [00:07<00:08,  1.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|████████████████████████                   | 14/25 [00:08<00:09,  1.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:09<00:07,  1.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▌               | 16/25 [00:10<00:06,  1.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:10<00:05,  1.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:11<00:04,  1.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:11<00:03,  1.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:12<00:03,  1.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████       | 21/25 [00:12<00:02,  1.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:13<00:01,  1.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:14<00:01,  1.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:15<00:00,  1.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 25/25 [00:15<00:00,  1.57it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6800\n",
      "(7200, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:14<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200\n",
      "(7600, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:14<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7600\n",
      "(8000, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:14<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "(8400, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:16<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8400\n",
      "(8800, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:15<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8800\n",
      "(9200, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:16<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9200\n",
      "(9600, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 6/6 [00:02<00:00,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9600\n",
      "(9696, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(x_train), 400):\n",
    "    reduced_x_train = x_train[i: i + 400]\n",
    "    cls_reduced_x_train = extract_mbert_cls_only(list(reduced_x_train['premise']), list(reduced_x_train['hypothesis']))\n",
    "    cls_reduced_x_train_torch = torch.cat(cls_reduced_x_train)\n",
    "    cls_reduced_x_train_features = cls_reduced_x_train_torch.cpu().detach().numpy()\n",
    "    print(i)\n",
    "    if i != 0:\n",
    "        cls_x_train_features = np.concatenate((cls_x_train_features, cls_reduced_x_train_features), axis = 0)\n",
    "    else:\n",
    "        cls_x_train_features = cls_reduced_x_train_features\n",
    "#     reduced_y_train = np.asarray(reduced_y_train)\n",
    "    print(cls_x_train_features.shape)\n",
    "    del reduced_x_train, cls_reduced_x_train, cls_reduced_x_train_torch, cls_reduced_x_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0559f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_cls_features_mbert.npy', 'wb') as f:\n",
    "    np.save(f, cls_x_train_features)\n",
    "    \n",
    "with open('train_labels_mbert.npy', 'wb') as f:\n",
    "    np.save(f, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a24eafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:16<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(400, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:17<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "(800, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:18<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "(1200, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:16<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "(1600, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:17<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n",
      "(2000, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:15<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "(2400, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "(2424, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(x_test), 400):\n",
    "    reduced_x_test = x_test[i: i + 400]\n",
    "    cls_reduced_x_test = extract_mbert_cls_only(list(reduced_x_test['premise']), list(reduced_x_test['hypothesis']))\n",
    "    cls_reduced_x_test_torch = torch.cat(cls_reduced_x_test)\n",
    "    cls_reduced_x_test_features = cls_reduced_x_test_torch.cpu().detach().numpy()\n",
    "    print(i)\n",
    "    if i != 0:\n",
    "        cls_x_test_features = np.concatenate((cls_x_test_features, cls_reduced_x_test_features), axis = 0)\n",
    "    else:\n",
    "        cls_x_test_features = cls_reduced_x_test_features\n",
    "#     reduced_y_train = np.asarray(reduced_y_train)\n",
    "    print(cls_x_test_features.shape)\n",
    "    del reduced_x_test, cls_reduced_x_test, cls_reduced_x_test_torch, cls_reduced_x_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cef0e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_cls_features_mbert.npy', 'wb') as f:\n",
    "    np.save(f, cls_x_test_features)\n",
    "    \n",
    "with open('test_labels_mbert.npy', 'wb') as f:\n",
    "    np.save(f, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1f302d",
   "metadata": {},
   "source": [
    "## Loading features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63f42d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_x_train = np.load('train_cls_features_mbert.npy', allow_pickle=True)\n",
    "y_train = np.load('train_labels_mbert.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c1a90d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9696, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94302bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9696,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "569349ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_x_test = np.load('test_cls_features_mbert.npy', allow_pickle=True)\n",
    "y_test = np.load('test_labels_mbert.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e36a7dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(cls_x_train)\n",
    "simple_x_train = scaler.transform(cls_x_train)\n",
    "\n",
    "simple_x_test =scaler.transform(cls_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31199f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9696, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ae701f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.57      0.56       835\n",
      "           1       0.54      0.50      0.52       776\n",
      "           2       0.51      0.52      0.51       813\n",
      "\n",
      "    accuracy                           0.53      2424\n",
      "   macro avg       0.53      0.53      0.53      2424\n",
      "weighted avg       0.53      0.53      0.53      2424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(solver = \"liblinear\")\n",
    "clf.fit(simple_x_train, y_train)\n",
    "pred = clf.predict(simple_x_test)\n",
    "pred = classification_report(y_test, pred)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e40a32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geetshingi/miniconda3/envs/torch/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.57      0.55       835\n",
      "           1       0.54      0.50      0.52       776\n",
      "           2       0.51      0.52      0.52       813\n",
      "\n",
      "    accuracy                           0.53      2424\n",
      "   macro avg       0.53      0.53      0.53      2424\n",
      "weighted avg       0.53      0.53      0.53      2424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(max_iter = 10000)\n",
    "\n",
    "model.fit(simple_x_train, y_train)\n",
    "pred = model.predict(simple_x_test)\n",
    "pred = classification_report(y_test, pred)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b71bf0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.51      0.51       835\n",
      "           1       0.47      0.54      0.50       776\n",
      "           2       0.47      0.41      0.44       813\n",
      "\n",
      "    accuracy                           0.48      2424\n",
      "   macro avg       0.48      0.48      0.48      2424\n",
      "weighted avg       0.48      0.48      0.48      2424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Perceptron()\n",
    "\n",
    "model.fit(simple_x_train, y_train)\n",
    "pred = model.predict(simple_x_test)\n",
    "pred = classification_report(y_test, pred)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9db08a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.53      0.52       835\n",
      "           1       0.44      0.41      0.43       776\n",
      "           2       0.45      0.46      0.46       813\n",
      "\n",
      "    accuracy                           0.47      2424\n",
      "   macro avg       0.47      0.47      0.47      2424\n",
      "weighted avg       0.47      0.47      0.47      2424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(simple_x_train, y_train)\n",
    "pred = model.predict(simple_x_test)\n",
    "pred = classification_report(y_test, pred)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31b64dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_torch = torch.from_numpy(cls_x_train)\n",
    "x_test_torch = torch.from_numpy(cls_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d60de158",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipper = lambda x,y : list(zip(x,list(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab7fe796",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_torch = zipper(x_train_torch, y_train)\n",
    "test_torch = zipper(x_test_torch, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_torch, batch_size=256)\n",
    "test_loader = torch.utils.data.DataLoader(test_torch, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b43787d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module): \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__() \n",
    "#         self.gru = nn.GRU(input_size=768, hidden_size=32, batch_first=True) \n",
    "        self.fc1 = nn.Linear(768,3)\n",
    "#         self.fc2 = nn.Linear(256, 64)\n",
    "#         self.fc3 = nn.Linear(128, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "#         self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         out, hn = self.gru(x, torch.randn(1, len(x), 32))\n",
    "#         out = self.fc1(self.relu(x))\n",
    "#         out = self.fc2(self.relu(out))\n",
    "        out = self.fc1(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d967119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=768, out_features=3, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "linear_model = Net()\n",
    "print(linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fdfa8700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(linear_model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4625a41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.113914 \tValidation Loss: 1.109428\n",
      "Validation loss decreased (inf --> 1.109428).         Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.108770 \tValidation Loss: 1.103706\n",
      "Validation loss decreased (1.109428 --> 1.103706).         Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.101950 \tValidation Loss: 1.098257\n",
      "Validation loss decreased (1.103706 --> 1.098257).         Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.095780 \tValidation Loss: 1.093183\n",
      "Validation loss decreased (1.098257 --> 1.093183).         Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.090151 \tValidation Loss: 1.088501\n",
      "Validation loss decreased (1.093183 --> 1.088501).         Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.084995 \tValidation Loss: 1.084190\n",
      "Validation loss decreased (1.088501 --> 1.084190).         Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.080256 \tValidation Loss: 1.080219\n",
      "Validation loss decreased (1.084190 --> 1.080219).         Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.075885 \tValidation Loss: 1.076553\n",
      "Validation loss decreased (1.080219 --> 1.076553).         Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.071841 \tValidation Loss: 1.073161\n",
      "Validation loss decreased (1.076553 --> 1.073161).         Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.068088 \tValidation Loss: 1.070013\n",
      "Validation loss decreased (1.073161 --> 1.070013).         Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.064594 \tValidation Loss: 1.067086\n",
      "Validation loss decreased (1.070013 --> 1.067086).         Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.061332 \tValidation Loss: 1.064355\n",
      "Validation loss decreased (1.067086 --> 1.064355).         Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.058278 \tValidation Loss: 1.061803\n",
      "Validation loss decreased (1.064355 --> 1.061803).         Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.055411 \tValidation Loss: 1.059410\n",
      "Validation loss decreased (1.061803 --> 1.059410).         Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.052713 \tValidation Loss: 1.057163\n",
      "Validation loss decreased (1.059410 --> 1.057163).         Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.050167 \tValidation Loss: 1.055047\n",
      "Validation loss decreased (1.057163 --> 1.055047).         Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.047759 \tValidation Loss: 1.053049\n",
      "Validation loss decreased (1.055047 --> 1.053049).         Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.045477 \tValidation Loss: 1.051161\n",
      "Validation loss decreased (1.053049 --> 1.051161).         Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.043309 \tValidation Loss: 1.049371\n",
      "Validation loss decreased (1.051161 --> 1.049371).         Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.041246 \tValidation Loss: 1.047671\n",
      "Validation loss decreased (1.049371 --> 1.047671).         Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.039278 \tValidation Loss: 1.046054\n",
      "Validation loss decreased (1.047671 --> 1.046054).         Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.037398 \tValidation Loss: 1.044513\n",
      "Validation loss decreased (1.046054 --> 1.044513).         Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.035598 \tValidation Loss: 1.043042\n",
      "Validation loss decreased (1.044513 --> 1.043042).         Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.033873 \tValidation Loss: 1.041635\n",
      "Validation loss decreased (1.043042 --> 1.041635).         Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.032217 \tValidation Loss: 1.040288\n",
      "Validation loss decreased (1.041635 --> 1.040288).         Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.030624 \tValidation Loss: 1.038996\n",
      "Validation loss decreased (1.040288 --> 1.038996).         Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.029090 \tValidation Loss: 1.037755\n",
      "Validation loss decreased (1.038996 --> 1.037755).         Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.027611 \tValidation Loss: 1.036562\n",
      "Validation loss decreased (1.037755 --> 1.036562).         Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.026183 \tValidation Loss: 1.035412\n",
      "Validation loss decreased (1.036562 --> 1.035412).         Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.024802 \tValidation Loss: 1.034304\n",
      "Validation loss decreased (1.035412 --> 1.034304).         Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.023467 \tValidation Loss: 1.033234\n",
      "Validation loss decreased (1.034304 --> 1.033234).         Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.022173 \tValidation Loss: 1.032200\n",
      "Validation loss decreased (1.033234 --> 1.032200).         Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.020918 \tValidation Loss: 1.031200\n",
      "Validation loss decreased (1.032200 --> 1.031200).         Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.019699 \tValidation Loss: 1.030232\n",
      "Validation loss decreased (1.031200 --> 1.030232).         Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.018516 \tValidation Loss: 1.029293\n",
      "Validation loss decreased (1.030232 --> 1.029293).         Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.017365 \tValidation Loss: 1.028383\n",
      "Validation loss decreased (1.029293 --> 1.028383).         Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.016245 \tValidation Loss: 1.027499\n",
      "Validation loss decreased (1.028383 --> 1.027499).         Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.015154 \tValidation Loss: 1.026640\n",
      "Validation loss decreased (1.027499 --> 1.026640).         Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.014090 \tValidation Loss: 1.025805\n",
      "Validation loss decreased (1.026640 --> 1.025805).         Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.013053 \tValidation Loss: 1.024992\n",
      "Validation loss decreased (1.025805 --> 1.024992).         Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.012041 \tValidation Loss: 1.024201\n",
      "Validation loss decreased (1.024992 --> 1.024201).         Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.011052 \tValidation Loss: 1.023429\n",
      "Validation loss decreased (1.024201 --> 1.023429).         Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 1.010086 \tValidation Loss: 1.022677\n",
      "Validation loss decreased (1.023429 --> 1.022677).         Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.009141 \tValidation Loss: 1.021943\n",
      "Validation loss decreased (1.022677 --> 1.021943).         Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.008217 \tValidation Loss: 1.021227\n",
      "Validation loss decreased (1.021943 --> 1.021227).         Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.007312 \tValidation Loss: 1.020528\n",
      "Validation loss decreased (1.021227 --> 1.020528).         Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 1.006426 \tValidation Loss: 1.019844\n",
      "Validation loss decreased (1.020528 --> 1.019844).         Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.005558 \tValidation Loss: 1.019175\n",
      "Validation loss decreased (1.019844 --> 1.019175).         Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.004707 \tValidation Loss: 1.018522\n",
      "Validation loss decreased (1.019175 --> 1.018522).         Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 1.003872 \tValidation Loss: 1.017882\n",
      "Validation loss decreased (1.018522 --> 1.017882).         Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 1.003053 \tValidation Loss: 1.017255\n",
      "Validation loss decreased (1.017882 --> 1.017255).         Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 1.002250 \tValidation Loss: 1.016642\n",
      "Validation loss decreased (1.017255 --> 1.016642).         Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 1.001461 \tValidation Loss: 1.016041\n",
      "Validation loss decreased (1.016642 --> 1.016041).         Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 1.000686 \tValidation Loss: 1.015452\n",
      "Validation loss decreased (1.016041 --> 1.015452).         Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 0.999924 \tValidation Loss: 1.014874\n",
      "Validation loss decreased (1.015452 --> 1.014874).         Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 0.999176 \tValidation Loss: 1.014308\n",
      "Validation loss decreased (1.014874 --> 1.014308).         Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 0.998441 \tValidation Loss: 1.013752\n",
      "Validation loss decreased (1.014308 --> 1.013752).         Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 0.997717 \tValidation Loss: 1.013206\n",
      "Validation loss decreased (1.013752 --> 1.013206).         Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 0.997006 \tValidation Loss: 1.012671\n",
      "Validation loss decreased (1.013206 --> 1.012671).         Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 0.996305 \tValidation Loss: 1.012145\n",
      "Validation loss decreased (1.012671 --> 1.012145).         Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 0.995616 \tValidation Loss: 1.011628\n",
      "Validation loss decreased (1.012145 --> 1.011628).         Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 0.994938 \tValidation Loss: 1.011120\n",
      "Validation loss decreased (1.011628 --> 1.011120).         Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 0.994270 \tValidation Loss: 1.010622\n",
      "Validation loss decreased (1.011120 --> 1.010622).         Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 0.993612 \tValidation Loss: 1.010131\n",
      "Validation loss decreased (1.010622 --> 1.010131).         Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 0.992964 \tValidation Loss: 1.009649\n",
      "Validation loss decreased (1.010131 --> 1.009649).         Saving model ...\n",
      "Epoch: 66 \tTraining Loss: 0.992325 \tValidation Loss: 1.009175\n",
      "Validation loss decreased (1.009649 --> 1.009175).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 \tTraining Loss: 0.991695 \tValidation Loss: 1.008709\n",
      "Validation loss decreased (1.009175 --> 1.008709).         Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 0.991075 \tValidation Loss: 1.008250\n",
      "Validation loss decreased (1.008709 --> 1.008250).         Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 0.990463 \tValidation Loss: 1.007798\n",
      "Validation loss decreased (1.008250 --> 1.007798).         Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 0.989860 \tValidation Loss: 1.007354\n",
      "Validation loss decreased (1.007798 --> 1.007354).         Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 0.989264 \tValidation Loss: 1.006917\n",
      "Validation loss decreased (1.007354 --> 1.006917).         Saving model ...\n",
      "Epoch: 72 \tTraining Loss: 0.988677 \tValidation Loss: 1.006486\n",
      "Validation loss decreased (1.006917 --> 1.006486).         Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 0.988098 \tValidation Loss: 1.006062\n",
      "Validation loss decreased (1.006486 --> 1.006062).         Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 0.987526 \tValidation Loss: 1.005644\n",
      "Validation loss decreased (1.006062 --> 1.005644).         Saving model ...\n",
      "Epoch: 75 \tTraining Loss: 0.986962 \tValidation Loss: 1.005233\n",
      "Validation loss decreased (1.005644 --> 1.005233).         Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 0.986405 \tValidation Loss: 1.004827\n",
      "Validation loss decreased (1.005233 --> 1.004827).         Saving model ...\n",
      "Epoch: 77 \tTraining Loss: 0.985855 \tValidation Loss: 1.004428\n",
      "Validation loss decreased (1.004827 --> 1.004428).         Saving model ...\n",
      "Epoch: 78 \tTraining Loss: 0.985313 \tValidation Loss: 1.004034\n",
      "Validation loss decreased (1.004428 --> 1.004034).         Saving model ...\n",
      "Epoch: 79 \tTraining Loss: 0.984776 \tValidation Loss: 1.003646\n",
      "Validation loss decreased (1.004034 --> 1.003646).         Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 0.984247 \tValidation Loss: 1.003263\n",
      "Validation loss decreased (1.003646 --> 1.003263).         Saving model ...\n",
      "Epoch: 81 \tTraining Loss: 0.983724 \tValidation Loss: 1.002886\n",
      "Validation loss decreased (1.003263 --> 1.002886).         Saving model ...\n",
      "Epoch: 82 \tTraining Loss: 0.983207 \tValidation Loss: 1.002514\n",
      "Validation loss decreased (1.002886 --> 1.002514).         Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 0.982696 \tValidation Loss: 1.002148\n",
      "Validation loss decreased (1.002514 --> 1.002148).         Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 0.982191 \tValidation Loss: 1.001786\n",
      "Validation loss decreased (1.002148 --> 1.001786).         Saving model ...\n",
      "Epoch: 85 \tTraining Loss: 0.981692 \tValidation Loss: 1.001429\n",
      "Validation loss decreased (1.001786 --> 1.001429).         Saving model ...\n",
      "Epoch: 86 \tTraining Loss: 0.981199 \tValidation Loss: 1.001077\n",
      "Validation loss decreased (1.001429 --> 1.001077).         Saving model ...\n",
      "Epoch: 87 \tTraining Loss: 0.980712 \tValidation Loss: 1.000730\n",
      "Validation loss decreased (1.001077 --> 1.000730).         Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 0.980230 \tValidation Loss: 1.000387\n",
      "Validation loss decreased (1.000730 --> 1.000387).         Saving model ...\n",
      "Epoch: 89 \tTraining Loss: 0.979754 \tValidation Loss: 1.000049\n",
      "Validation loss decreased (1.000387 --> 1.000049).         Saving model ...\n",
      "Epoch: 90 \tTraining Loss: 0.979283 \tValidation Loss: 0.999716\n",
      "Validation loss decreased (1.000049 --> 0.999716).         Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 0.978817 \tValidation Loss: 0.999386\n",
      "Validation loss decreased (0.999716 --> 0.999386).         Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 0.978356 \tValidation Loss: 0.999061\n",
      "Validation loss decreased (0.999386 --> 0.999061).         Saving model ...\n",
      "Epoch: 93 \tTraining Loss: 0.977900 \tValidation Loss: 0.998740\n",
      "Validation loss decreased (0.999061 --> 0.998740).         Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 0.977449 \tValidation Loss: 0.998423\n",
      "Validation loss decreased (0.998740 --> 0.998423).         Saving model ...\n",
      "Epoch: 95 \tTraining Loss: 0.977003 \tValidation Loss: 0.998110\n",
      "Validation loss decreased (0.998423 --> 0.998110).         Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 0.976561 \tValidation Loss: 0.997801\n",
      "Validation loss decreased (0.998110 --> 0.997801).         Saving model ...\n",
      "Epoch: 97 \tTraining Loss: 0.976124 \tValidation Loss: 0.997496\n",
      "Validation loss decreased (0.997801 --> 0.997496).         Saving model ...\n",
      "Epoch: 98 \tTraining Loss: 0.975692 \tValidation Loss: 0.997195\n",
      "Validation loss decreased (0.997496 --> 0.997195).         Saving model ...\n",
      "Epoch: 99 \tTraining Loss: 0.975264 \tValidation Loss: 0.996897\n",
      "Validation loss decreased (0.997195 --> 0.996897).         Saving model ...\n",
      "Epoch: 100 \tTraining Loss: 0.974841 \tValidation Loss: 0.996603\n",
      "Validation loss decreased (0.996897 --> 0.996603).         Saving model ...\n",
      "Epoch: 101 \tTraining Loss: 0.974421 \tValidation Loss: 0.996313\n",
      "Validation loss decreased (0.996603 --> 0.996313).         Saving model ...\n",
      "Epoch: 102 \tTraining Loss: 0.974006 \tValidation Loss: 0.996025\n",
      "Validation loss decreased (0.996313 --> 0.996025).         Saving model ...\n",
      "Epoch: 103 \tTraining Loss: 0.973595 \tValidation Loss: 0.995742\n",
      "Validation loss decreased (0.996025 --> 0.995742).         Saving model ...\n",
      "Epoch: 104 \tTraining Loss: 0.973189 \tValidation Loss: 0.995462\n",
      "Validation loss decreased (0.995742 --> 0.995462).         Saving model ...\n",
      "Epoch: 105 \tTraining Loss: 0.972786 \tValidation Loss: 0.995185\n",
      "Validation loss decreased (0.995462 --> 0.995185).         Saving model ...\n",
      "Epoch: 106 \tTraining Loss: 0.972387 \tValidation Loss: 0.994911\n",
      "Validation loss decreased (0.995185 --> 0.994911).         Saving model ...\n",
      "Epoch: 107 \tTraining Loss: 0.971992 \tValidation Loss: 0.994641\n",
      "Validation loss decreased (0.994911 --> 0.994641).         Saving model ...\n",
      "Epoch: 108 \tTraining Loss: 0.971600 \tValidation Loss: 0.994373\n",
      "Validation loss decreased (0.994641 --> 0.994373).         Saving model ...\n",
      "Epoch: 109 \tTraining Loss: 0.971213 \tValidation Loss: 0.994109\n",
      "Validation loss decreased (0.994373 --> 0.994109).         Saving model ...\n",
      "Epoch: 110 \tTraining Loss: 0.970829 \tValidation Loss: 0.993848\n",
      "Validation loss decreased (0.994109 --> 0.993848).         Saving model ...\n",
      "Epoch: 111 \tTraining Loss: 0.970449 \tValidation Loss: 0.993589\n",
      "Validation loss decreased (0.993848 --> 0.993589).         Saving model ...\n",
      "Epoch: 112 \tTraining Loss: 0.970072 \tValidation Loss: 0.993334\n",
      "Validation loss decreased (0.993589 --> 0.993334).         Saving model ...\n",
      "Epoch: 113 \tTraining Loss: 0.969698 \tValidation Loss: 0.993082\n",
      "Validation loss decreased (0.993334 --> 0.993082).         Saving model ...\n",
      "Epoch: 114 \tTraining Loss: 0.969328 \tValidation Loss: 0.992832\n",
      "Validation loss decreased (0.993082 --> 0.992832).         Saving model ...\n",
      "Epoch: 115 \tTraining Loss: 0.968962 \tValidation Loss: 0.992585\n",
      "Validation loss decreased (0.992832 --> 0.992585).         Saving model ...\n",
      "Epoch: 116 \tTraining Loss: 0.968599 \tValidation Loss: 0.992341\n",
      "Validation loss decreased (0.992585 --> 0.992341).         Saving model ...\n",
      "Epoch: 117 \tTraining Loss: 0.968239 \tValidation Loss: 0.992099\n",
      "Validation loss decreased (0.992341 --> 0.992099).         Saving model ...\n",
      "Epoch: 118 \tTraining Loss: 0.967882 \tValidation Loss: 0.991861\n",
      "Validation loss decreased (0.992099 --> 0.991861).         Saving model ...\n",
      "Epoch: 119 \tTraining Loss: 0.967528 \tValidation Loss: 0.991624\n",
      "Validation loss decreased (0.991861 --> 0.991624).         Saving model ...\n",
      "Epoch: 120 \tTraining Loss: 0.967178 \tValidation Loss: 0.991391\n",
      "Validation loss decreased (0.991624 --> 0.991391).         Saving model ...\n",
      "Epoch: 121 \tTraining Loss: 0.966830 \tValidation Loss: 0.991160\n",
      "Validation loss decreased (0.991391 --> 0.991160).         Saving model ...\n",
      "Epoch: 122 \tTraining Loss: 0.966486 \tValidation Loss: 0.990931\n",
      "Validation loss decreased (0.991160 --> 0.990931).         Saving model ...\n",
      "Epoch: 123 \tTraining Loss: 0.966145 \tValidation Loss: 0.990705\n",
      "Validation loss decreased (0.990931 --> 0.990705).         Saving model ...\n",
      "Epoch: 124 \tTraining Loss: 0.965806 \tValidation Loss: 0.990481\n",
      "Validation loss decreased (0.990705 --> 0.990481).         Saving model ...\n",
      "Epoch: 125 \tTraining Loss: 0.965470 \tValidation Loss: 0.990259\n",
      "Validation loss decreased (0.990481 --> 0.990259).         Saving model ...\n",
      "Epoch: 126 \tTraining Loss: 0.965137 \tValidation Loss: 0.990040\n",
      "Validation loss decreased (0.990259 --> 0.990040).         Saving model ...\n",
      "Epoch: 127 \tTraining Loss: 0.964807 \tValidation Loss: 0.989823\n",
      "Validation loss decreased (0.990040 --> 0.989823).         Saving model ...\n",
      "Epoch: 128 \tTraining Loss: 0.964480 \tValidation Loss: 0.989609\n",
      "Validation loss decreased (0.989823 --> 0.989609).         Saving model ...\n",
      "Epoch: 129 \tTraining Loss: 0.964156 \tValidation Loss: 0.989397\n",
      "Validation loss decreased (0.989609 --> 0.989397).         Saving model ...\n",
      "Epoch: 130 \tTraining Loss: 0.963834 \tValidation Loss: 0.989186\n",
      "Validation loss decreased (0.989397 --> 0.989186).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 131 \tTraining Loss: 0.963514 \tValidation Loss: 0.988978\n",
      "Validation loss decreased (0.989186 --> 0.988978).         Saving model ...\n",
      "Epoch: 132 \tTraining Loss: 0.963198 \tValidation Loss: 0.988772\n",
      "Validation loss decreased (0.988978 --> 0.988772).         Saving model ...\n",
      "Epoch: 133 \tTraining Loss: 0.962883 \tValidation Loss: 0.988569\n",
      "Validation loss decreased (0.988772 --> 0.988569).         Saving model ...\n",
      "Epoch: 134 \tTraining Loss: 0.962572 \tValidation Loss: 0.988367\n",
      "Validation loss decreased (0.988569 --> 0.988367).         Saving model ...\n",
      "Epoch: 135 \tTraining Loss: 0.962263 \tValidation Loss: 0.988167\n",
      "Validation loss decreased (0.988367 --> 0.988167).         Saving model ...\n",
      "Epoch: 136 \tTraining Loss: 0.961956 \tValidation Loss: 0.987969\n",
      "Validation loss decreased (0.988167 --> 0.987969).         Saving model ...\n",
      "Epoch: 137 \tTraining Loss: 0.961652 \tValidation Loss: 0.987774\n",
      "Validation loss decreased (0.987969 --> 0.987774).         Saving model ...\n",
      "Epoch: 138 \tTraining Loss: 0.961350 \tValidation Loss: 0.987580\n",
      "Validation loss decreased (0.987774 --> 0.987580).         Saving model ...\n",
      "Epoch: 139 \tTraining Loss: 0.961050 \tValidation Loss: 0.987388\n",
      "Validation loss decreased (0.987580 --> 0.987388).         Saving model ...\n",
      "Epoch: 140 \tTraining Loss: 0.960753 \tValidation Loss: 0.987198\n",
      "Validation loss decreased (0.987388 --> 0.987198).         Saving model ...\n",
      "Epoch: 141 \tTraining Loss: 0.960458 \tValidation Loss: 0.987010\n",
      "Validation loss decreased (0.987198 --> 0.987010).         Saving model ...\n",
      "Epoch: 142 \tTraining Loss: 0.960165 \tValidation Loss: 0.986823\n",
      "Validation loss decreased (0.987010 --> 0.986823).         Saving model ...\n",
      "Epoch: 143 \tTraining Loss: 0.959875 \tValidation Loss: 0.986639\n",
      "Validation loss decreased (0.986823 --> 0.986639).         Saving model ...\n",
      "Epoch: 144 \tTraining Loss: 0.959587 \tValidation Loss: 0.986456\n",
      "Validation loss decreased (0.986639 --> 0.986456).         Saving model ...\n",
      "Epoch: 145 \tTraining Loss: 0.959301 \tValidation Loss: 0.986275\n",
      "Validation loss decreased (0.986456 --> 0.986275).         Saving model ...\n",
      "Epoch: 146 \tTraining Loss: 0.959017 \tValidation Loss: 0.986096\n",
      "Validation loss decreased (0.986275 --> 0.986096).         Saving model ...\n",
      "Epoch: 147 \tTraining Loss: 0.958735 \tValidation Loss: 0.985918\n",
      "Validation loss decreased (0.986096 --> 0.985918).         Saving model ...\n",
      "Epoch: 148 \tTraining Loss: 0.958455 \tValidation Loss: 0.985742\n",
      "Validation loss decreased (0.985918 --> 0.985742).         Saving model ...\n",
      "Epoch: 149 \tTraining Loss: 0.958177 \tValidation Loss: 0.985568\n",
      "Validation loss decreased (0.985742 --> 0.985568).         Saving model ...\n",
      "Epoch: 150 \tTraining Loss: 0.957901 \tValidation Loss: 0.985396\n",
      "Validation loss decreased (0.985568 --> 0.985396).         Saving model ...\n",
      "Epoch: 151 \tTraining Loss: 0.957628 \tValidation Loss: 0.985225\n",
      "Validation loss decreased (0.985396 --> 0.985225).         Saving model ...\n",
      "Epoch: 152 \tTraining Loss: 0.957356 \tValidation Loss: 0.985055\n",
      "Validation loss decreased (0.985225 --> 0.985055).         Saving model ...\n",
      "Epoch: 153 \tTraining Loss: 0.957086 \tValidation Loss: 0.984888\n",
      "Validation loss decreased (0.985055 --> 0.984888).         Saving model ...\n",
      "Epoch: 154 \tTraining Loss: 0.956818 \tValidation Loss: 0.984721\n",
      "Validation loss decreased (0.984888 --> 0.984721).         Saving model ...\n",
      "Epoch: 155 \tTraining Loss: 0.956552 \tValidation Loss: 0.984557\n",
      "Validation loss decreased (0.984721 --> 0.984557).         Saving model ...\n",
      "Epoch: 156 \tTraining Loss: 0.956288 \tValidation Loss: 0.984394\n",
      "Validation loss decreased (0.984557 --> 0.984394).         Saving model ...\n",
      "Epoch: 157 \tTraining Loss: 0.956026 \tValidation Loss: 0.984232\n",
      "Validation loss decreased (0.984394 --> 0.984232).         Saving model ...\n",
      "Epoch: 158 \tTraining Loss: 0.955765 \tValidation Loss: 0.984072\n",
      "Validation loss decreased (0.984232 --> 0.984072).         Saving model ...\n",
      "Epoch: 159 \tTraining Loss: 0.955507 \tValidation Loss: 0.983913\n",
      "Validation loss decreased (0.984072 --> 0.983913).         Saving model ...\n",
      "Epoch: 160 \tTraining Loss: 0.955250 \tValidation Loss: 0.983756\n",
      "Validation loss decreased (0.983913 --> 0.983756).         Saving model ...\n",
      "Epoch: 161 \tTraining Loss: 0.954995 \tValidation Loss: 0.983600\n",
      "Validation loss decreased (0.983756 --> 0.983600).         Saving model ...\n",
      "Epoch: 162 \tTraining Loss: 0.954741 \tValidation Loss: 0.983445\n",
      "Validation loss decreased (0.983600 --> 0.983445).         Saving model ...\n",
      "Epoch: 163 \tTraining Loss: 0.954490 \tValidation Loss: 0.983292\n",
      "Validation loss decreased (0.983445 --> 0.983292).         Saving model ...\n",
      "Epoch: 164 \tTraining Loss: 0.954240 \tValidation Loss: 0.983140\n",
      "Validation loss decreased (0.983292 --> 0.983140).         Saving model ...\n",
      "Epoch: 165 \tTraining Loss: 0.953992 \tValidation Loss: 0.982990\n",
      "Validation loss decreased (0.983140 --> 0.982990).         Saving model ...\n",
      "Epoch: 166 \tTraining Loss: 0.953745 \tValidation Loss: 0.982841\n",
      "Validation loss decreased (0.982990 --> 0.982841).         Saving model ...\n",
      "Epoch: 167 \tTraining Loss: 0.953500 \tValidation Loss: 0.982693\n",
      "Validation loss decreased (0.982841 --> 0.982693).         Saving model ...\n",
      "Epoch: 168 \tTraining Loss: 0.953257 \tValidation Loss: 0.982546\n",
      "Validation loss decreased (0.982693 --> 0.982546).         Saving model ...\n",
      "Epoch: 169 \tTraining Loss: 0.953015 \tValidation Loss: 0.982401\n",
      "Validation loss decreased (0.982546 --> 0.982401).         Saving model ...\n",
      "Epoch: 170 \tTraining Loss: 0.952775 \tValidation Loss: 0.982257\n",
      "Validation loss decreased (0.982401 --> 0.982257).         Saving model ...\n",
      "Epoch: 171 \tTraining Loss: 0.952536 \tValidation Loss: 0.982114\n",
      "Validation loss decreased (0.982257 --> 0.982114).         Saving model ...\n",
      "Epoch: 172 \tTraining Loss: 0.952299 \tValidation Loss: 0.981973\n",
      "Validation loss decreased (0.982114 --> 0.981973).         Saving model ...\n",
      "Epoch: 173 \tTraining Loss: 0.952064 \tValidation Loss: 0.981833\n",
      "Validation loss decreased (0.981973 --> 0.981833).         Saving model ...\n",
      "Epoch: 174 \tTraining Loss: 0.951830 \tValidation Loss: 0.981694\n",
      "Validation loss decreased (0.981833 --> 0.981694).         Saving model ...\n",
      "Epoch: 175 \tTraining Loss: 0.951597 \tValidation Loss: 0.981556\n",
      "Validation loss decreased (0.981694 --> 0.981556).         Saving model ...\n",
      "Epoch: 176 \tTraining Loss: 0.951366 \tValidation Loss: 0.981419\n",
      "Validation loss decreased (0.981556 --> 0.981419).         Saving model ...\n",
      "Epoch: 177 \tTraining Loss: 0.951136 \tValidation Loss: 0.981283\n",
      "Validation loss decreased (0.981419 --> 0.981283).         Saving model ...\n",
      "Epoch: 178 \tTraining Loss: 0.950908 \tValidation Loss: 0.981149\n",
      "Validation loss decreased (0.981283 --> 0.981149).         Saving model ...\n",
      "Epoch: 179 \tTraining Loss: 0.950681 \tValidation Loss: 0.981016\n",
      "Validation loss decreased (0.981149 --> 0.981016).         Saving model ...\n",
      "Epoch: 180 \tTraining Loss: 0.950456 \tValidation Loss: 0.980883\n",
      "Validation loss decreased (0.981016 --> 0.980883).         Saving model ...\n",
      "Epoch: 181 \tTraining Loss: 0.950232 \tValidation Loss: 0.980752\n",
      "Validation loss decreased (0.980883 --> 0.980752).         Saving model ...\n",
      "Epoch: 182 \tTraining Loss: 0.950010 \tValidation Loss: 0.980622\n",
      "Validation loss decreased (0.980752 --> 0.980622).         Saving model ...\n",
      "Epoch: 183 \tTraining Loss: 0.949788 \tValidation Loss: 0.980493\n",
      "Validation loss decreased (0.980622 --> 0.980493).         Saving model ...\n",
      "Epoch: 184 \tTraining Loss: 0.949568 \tValidation Loss: 0.980366\n",
      "Validation loss decreased (0.980493 --> 0.980366).         Saving model ...\n",
      "Epoch: 185 \tTraining Loss: 0.949350 \tValidation Loss: 0.980239\n",
      "Validation loss decreased (0.980366 --> 0.980239).         Saving model ...\n",
      "Epoch: 186 \tTraining Loss: 0.949133 \tValidation Loss: 0.980113\n",
      "Validation loss decreased (0.980239 --> 0.980113).         Saving model ...\n",
      "Epoch: 187 \tTraining Loss: 0.948917 \tValidation Loss: 0.979988\n",
      "Validation loss decreased (0.980113 --> 0.979988).         Saving model ...\n",
      "Epoch: 188 \tTraining Loss: 0.948702 \tValidation Loss: 0.979865\n",
      "Validation loss decreased (0.979988 --> 0.979865).         Saving model ...\n",
      "Epoch: 189 \tTraining Loss: 0.948489 \tValidation Loss: 0.979742\n",
      "Validation loss decreased (0.979865 --> 0.979742).         Saving model ...\n",
      "Epoch: 190 \tTraining Loss: 0.948277 \tValidation Loss: 0.979620\n",
      "Validation loss decreased (0.979742 --> 0.979620).         Saving model ...\n",
      "Epoch: 191 \tTraining Loss: 0.948066 \tValidation Loss: 0.979499\n",
      "Validation loss decreased (0.979620 --> 0.979499).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 192 \tTraining Loss: 0.947856 \tValidation Loss: 0.979380\n",
      "Validation loss decreased (0.979499 --> 0.979380).         Saving model ...\n",
      "Epoch: 193 \tTraining Loss: 0.947648 \tValidation Loss: 0.979261\n",
      "Validation loss decreased (0.979380 --> 0.979261).         Saving model ...\n",
      "Epoch: 194 \tTraining Loss: 0.947441 \tValidation Loss: 0.979143\n",
      "Validation loss decreased (0.979261 --> 0.979143).         Saving model ...\n",
      "Epoch: 195 \tTraining Loss: 0.947235 \tValidation Loss: 0.979026\n",
      "Validation loss decreased (0.979143 --> 0.979026).         Saving model ...\n",
      "Epoch: 196 \tTraining Loss: 0.947030 \tValidation Loss: 0.978910\n",
      "Validation loss decreased (0.979026 --> 0.978910).         Saving model ...\n",
      "Epoch: 197 \tTraining Loss: 0.946826 \tValidation Loss: 0.978795\n",
      "Validation loss decreased (0.978910 --> 0.978795).         Saving model ...\n",
      "Epoch: 198 \tTraining Loss: 0.946624 \tValidation Loss: 0.978681\n",
      "Validation loss decreased (0.978795 --> 0.978681).         Saving model ...\n",
      "Epoch: 199 \tTraining Loss: 0.946423 \tValidation Loss: 0.978567\n",
      "Validation loss decreased (0.978681 --> 0.978567).         Saving model ...\n",
      "Epoch: 200 \tTraining Loss: 0.946222 \tValidation Loss: 0.978455\n",
      "Validation loss decreased (0.978567 --> 0.978455).         Saving model ...\n",
      "Epoch: 201 \tTraining Loss: 0.946023 \tValidation Loss: 0.978343\n",
      "Validation loss decreased (0.978455 --> 0.978343).         Saving model ...\n",
      "Epoch: 202 \tTraining Loss: 0.945825 \tValidation Loss: 0.978233\n",
      "Validation loss decreased (0.978343 --> 0.978233).         Saving model ...\n",
      "Epoch: 203 \tTraining Loss: 0.945628 \tValidation Loss: 0.978123\n",
      "Validation loss decreased (0.978233 --> 0.978123).         Saving model ...\n",
      "Epoch: 204 \tTraining Loss: 0.945433 \tValidation Loss: 0.978014\n",
      "Validation loss decreased (0.978123 --> 0.978014).         Saving model ...\n",
      "Epoch: 205 \tTraining Loss: 0.945238 \tValidation Loss: 0.977906\n",
      "Validation loss decreased (0.978014 --> 0.977906).         Saving model ...\n",
      "Epoch: 206 \tTraining Loss: 0.945044 \tValidation Loss: 0.977798\n",
      "Validation loss decreased (0.977906 --> 0.977798).         Saving model ...\n",
      "Epoch: 207 \tTraining Loss: 0.944852 \tValidation Loss: 0.977692\n",
      "Validation loss decreased (0.977798 --> 0.977692).         Saving model ...\n",
      "Epoch: 208 \tTraining Loss: 0.944660 \tValidation Loss: 0.977586\n",
      "Validation loss decreased (0.977692 --> 0.977586).         Saving model ...\n",
      "Epoch: 209 \tTraining Loss: 0.944470 \tValidation Loss: 0.977481\n",
      "Validation loss decreased (0.977586 --> 0.977481).         Saving model ...\n",
      "Epoch: 210 \tTraining Loss: 0.944280 \tValidation Loss: 0.977377\n",
      "Validation loss decreased (0.977481 --> 0.977377).         Saving model ...\n",
      "Epoch: 211 \tTraining Loss: 0.944092 \tValidation Loss: 0.977274\n",
      "Validation loss decreased (0.977377 --> 0.977274).         Saving model ...\n",
      "Epoch: 212 \tTraining Loss: 0.943905 \tValidation Loss: 0.977171\n",
      "Validation loss decreased (0.977274 --> 0.977171).         Saving model ...\n",
      "Epoch: 213 \tTraining Loss: 0.943718 \tValidation Loss: 0.977069\n",
      "Validation loss decreased (0.977171 --> 0.977069).         Saving model ...\n",
      "Epoch: 214 \tTraining Loss: 0.943533 \tValidation Loss: 0.976968\n",
      "Validation loss decreased (0.977069 --> 0.976968).         Saving model ...\n",
      "Epoch: 215 \tTraining Loss: 0.943348 \tValidation Loss: 0.976868\n",
      "Validation loss decreased (0.976968 --> 0.976868).         Saving model ...\n",
      "Epoch: 216 \tTraining Loss: 0.943165 \tValidation Loss: 0.976769\n",
      "Validation loss decreased (0.976868 --> 0.976769).         Saving model ...\n",
      "Epoch: 217 \tTraining Loss: 0.942982 \tValidation Loss: 0.976670\n",
      "Validation loss decreased (0.976769 --> 0.976670).         Saving model ...\n",
      "Epoch: 218 \tTraining Loss: 0.942801 \tValidation Loss: 0.976572\n",
      "Validation loss decreased (0.976670 --> 0.976572).         Saving model ...\n",
      "Epoch: 219 \tTraining Loss: 0.942620 \tValidation Loss: 0.976474\n",
      "Validation loss decreased (0.976572 --> 0.976474).         Saving model ...\n",
      "Epoch: 220 \tTraining Loss: 0.942440 \tValidation Loss: 0.976378\n",
      "Validation loss decreased (0.976474 --> 0.976378).         Saving model ...\n",
      "Epoch: 221 \tTraining Loss: 0.942261 \tValidation Loss: 0.976282\n",
      "Validation loss decreased (0.976378 --> 0.976282).         Saving model ...\n",
      "Epoch: 222 \tTraining Loss: 0.942084 \tValidation Loss: 0.976187\n",
      "Validation loss decreased (0.976282 --> 0.976187).         Saving model ...\n",
      "Epoch: 223 \tTraining Loss: 0.941907 \tValidation Loss: 0.976092\n",
      "Validation loss decreased (0.976187 --> 0.976092).         Saving model ...\n",
      "Epoch: 224 \tTraining Loss: 0.941730 \tValidation Loss: 0.975998\n",
      "Validation loss decreased (0.976092 --> 0.975998).         Saving model ...\n",
      "Epoch: 225 \tTraining Loss: 0.941555 \tValidation Loss: 0.975905\n",
      "Validation loss decreased (0.975998 --> 0.975905).         Saving model ...\n",
      "Epoch: 226 \tTraining Loss: 0.941381 \tValidation Loss: 0.975813\n",
      "Validation loss decreased (0.975905 --> 0.975813).         Saving model ...\n",
      "Epoch: 227 \tTraining Loss: 0.941208 \tValidation Loss: 0.975721\n",
      "Validation loss decreased (0.975813 --> 0.975721).         Saving model ...\n",
      "Epoch: 228 \tTraining Loss: 0.941035 \tValidation Loss: 0.975630\n",
      "Validation loss decreased (0.975721 --> 0.975630).         Saving model ...\n",
      "Epoch: 229 \tTraining Loss: 0.940863 \tValidation Loss: 0.975539\n",
      "Validation loss decreased (0.975630 --> 0.975539).         Saving model ...\n",
      "Epoch: 230 \tTraining Loss: 0.940692 \tValidation Loss: 0.975449\n",
      "Validation loss decreased (0.975539 --> 0.975449).         Saving model ...\n",
      "Epoch: 231 \tTraining Loss: 0.940522 \tValidation Loss: 0.975360\n",
      "Validation loss decreased (0.975449 --> 0.975360).         Saving model ...\n",
      "Epoch: 232 \tTraining Loss: 0.940353 \tValidation Loss: 0.975271\n",
      "Validation loss decreased (0.975360 --> 0.975271).         Saving model ...\n",
      "Epoch: 233 \tTraining Loss: 0.940185 \tValidation Loss: 0.975183\n",
      "Validation loss decreased (0.975271 --> 0.975183).         Saving model ...\n",
      "Epoch: 234 \tTraining Loss: 0.940017 \tValidation Loss: 0.975096\n",
      "Validation loss decreased (0.975183 --> 0.975096).         Saving model ...\n",
      "Epoch: 235 \tTraining Loss: 0.939850 \tValidation Loss: 0.975009\n",
      "Validation loss decreased (0.975096 --> 0.975009).         Saving model ...\n",
      "Epoch: 236 \tTraining Loss: 0.939685 \tValidation Loss: 0.974923\n",
      "Validation loss decreased (0.975009 --> 0.974923).         Saving model ...\n",
      "Epoch: 237 \tTraining Loss: 0.939519 \tValidation Loss: 0.974837\n",
      "Validation loss decreased (0.974923 --> 0.974837).         Saving model ...\n",
      "Epoch: 238 \tTraining Loss: 0.939355 \tValidation Loss: 0.974752\n",
      "Validation loss decreased (0.974837 --> 0.974752).         Saving model ...\n",
      "Epoch: 239 \tTraining Loss: 0.939191 \tValidation Loss: 0.974668\n",
      "Validation loss decreased (0.974752 --> 0.974668).         Saving model ...\n",
      "Epoch: 240 \tTraining Loss: 0.939029 \tValidation Loss: 0.974584\n",
      "Validation loss decreased (0.974668 --> 0.974584).         Saving model ...\n",
      "Epoch: 241 \tTraining Loss: 0.938867 \tValidation Loss: 0.974501\n",
      "Validation loss decreased (0.974584 --> 0.974501).         Saving model ...\n",
      "Epoch: 242 \tTraining Loss: 0.938705 \tValidation Loss: 0.974418\n",
      "Validation loss decreased (0.974501 --> 0.974418).         Saving model ...\n",
      "Epoch: 243 \tTraining Loss: 0.938545 \tValidation Loss: 0.974336\n",
      "Validation loss decreased (0.974418 --> 0.974336).         Saving model ...\n",
      "Epoch: 244 \tTraining Loss: 0.938385 \tValidation Loss: 0.974254\n",
      "Validation loss decreased (0.974336 --> 0.974254).         Saving model ...\n",
      "Epoch: 245 \tTraining Loss: 0.938226 \tValidation Loss: 0.974173\n",
      "Validation loss decreased (0.974254 --> 0.974173).         Saving model ...\n",
      "Epoch: 246 \tTraining Loss: 0.938068 \tValidation Loss: 0.974093\n",
      "Validation loss decreased (0.974173 --> 0.974093).         Saving model ...\n",
      "Epoch: 247 \tTraining Loss: 0.937910 \tValidation Loss: 0.974013\n",
      "Validation loss decreased (0.974093 --> 0.974013).         Saving model ...\n",
      "Epoch: 248 \tTraining Loss: 0.937753 \tValidation Loss: 0.973934\n",
      "Validation loss decreased (0.974013 --> 0.973934).         Saving model ...\n",
      "Epoch: 249 \tTraining Loss: 0.937597 \tValidation Loss: 0.973855\n",
      "Validation loss decreased (0.973934 --> 0.973855).         Saving model ...\n",
      "Epoch: 250 \tTraining Loss: 0.937442 \tValidation Loss: 0.973777\n",
      "Validation loss decreased (0.973855 --> 0.973777).         Saving model ...\n",
      "Epoch: 251 \tTraining Loss: 0.937287 \tValidation Loss: 0.973699\n",
      "Validation loss decreased (0.973777 --> 0.973699).         Saving model ...\n",
      "Epoch: 252 \tTraining Loss: 0.937133 \tValidation Loss: 0.973621\n",
      "Validation loss decreased (0.973699 --> 0.973621).         Saving model ...\n",
      "Epoch: 253 \tTraining Loss: 0.936980 \tValidation Loss: 0.973545\n",
      "Validation loss decreased (0.973621 --> 0.973545).         Saving model ...\n",
      "Epoch: 254 \tTraining Loss: 0.936827 \tValidation Loss: 0.973468\n",
      "Validation loss decreased (0.973545 --> 0.973468).         Saving model ...\n",
      "Epoch: 255 \tTraining Loss: 0.936675 \tValidation Loss: 0.973393\n",
      "Validation loss decreased (0.973468 --> 0.973393).         Saving model ...\n",
      "Epoch: 256 \tTraining Loss: 0.936524 \tValidation Loss: 0.973317\n",
      "Validation loss decreased (0.973393 --> 0.973317).         Saving model ...\n",
      "Epoch: 257 \tTraining Loss: 0.936373 \tValidation Loss: 0.973243\n",
      "Validation loss decreased (0.973317 --> 0.973243).         Saving model ...\n",
      "Epoch: 258 \tTraining Loss: 0.936223 \tValidation Loss: 0.973168\n",
      "Validation loss decreased (0.973243 --> 0.973168).         Saving model ...\n",
      "Epoch: 259 \tTraining Loss: 0.936074 \tValidation Loss: 0.973095\n",
      "Validation loss decreased (0.973168 --> 0.973095).         Saving model ...\n",
      "Epoch: 260 \tTraining Loss: 0.935926 \tValidation Loss: 0.973021\n",
      "Validation loss decreased (0.973095 --> 0.973021).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 261 \tTraining Loss: 0.935778 \tValidation Loss: 0.972948\n",
      "Validation loss decreased (0.973021 --> 0.972948).         Saving model ...\n",
      "Epoch: 262 \tTraining Loss: 0.935630 \tValidation Loss: 0.972876\n",
      "Validation loss decreased (0.972948 --> 0.972876).         Saving model ...\n",
      "Epoch: 263 \tTraining Loss: 0.935484 \tValidation Loss: 0.972804\n",
      "Validation loss decreased (0.972876 --> 0.972804).         Saving model ...\n",
      "Epoch: 264 \tTraining Loss: 0.935338 \tValidation Loss: 0.972733\n",
      "Validation loss decreased (0.972804 --> 0.972733).         Saving model ...\n",
      "Epoch: 265 \tTraining Loss: 0.935192 \tValidation Loss: 0.972662\n",
      "Validation loss decreased (0.972733 --> 0.972662).         Saving model ...\n",
      "Epoch: 266 \tTraining Loss: 0.935047 \tValidation Loss: 0.972591\n",
      "Validation loss decreased (0.972662 --> 0.972591).         Saving model ...\n",
      "Epoch: 267 \tTraining Loss: 0.934903 \tValidation Loss: 0.972521\n",
      "Validation loss decreased (0.972591 --> 0.972521).         Saving model ...\n",
      "Epoch: 268 \tTraining Loss: 0.934760 \tValidation Loss: 0.972452\n",
      "Validation loss decreased (0.972521 --> 0.972452).         Saving model ...\n",
      "Epoch: 269 \tTraining Loss: 0.934617 \tValidation Loss: 0.972382\n",
      "Validation loss decreased (0.972452 --> 0.972382).         Saving model ...\n",
      "Epoch: 270 \tTraining Loss: 0.934474 \tValidation Loss: 0.972314\n",
      "Validation loss decreased (0.972382 --> 0.972314).         Saving model ...\n",
      "Epoch: 271 \tTraining Loss: 0.934332 \tValidation Loss: 0.972245\n",
      "Validation loss decreased (0.972314 --> 0.972245).         Saving model ...\n",
      "Epoch: 272 \tTraining Loss: 0.934191 \tValidation Loss: 0.972177\n",
      "Validation loss decreased (0.972245 --> 0.972177).         Saving model ...\n",
      "Epoch: 273 \tTraining Loss: 0.934051 \tValidation Loss: 0.972110\n",
      "Validation loss decreased (0.972177 --> 0.972110).         Saving model ...\n",
      "Epoch: 274 \tTraining Loss: 0.933911 \tValidation Loss: 0.972043\n",
      "Validation loss decreased (0.972110 --> 0.972043).         Saving model ...\n",
      "Epoch: 275 \tTraining Loss: 0.933771 \tValidation Loss: 0.971976\n",
      "Validation loss decreased (0.972043 --> 0.971976).         Saving model ...\n",
      "Epoch: 276 \tTraining Loss: 0.933632 \tValidation Loss: 0.971910\n",
      "Validation loss decreased (0.971976 --> 0.971910).         Saving model ...\n",
      "Epoch: 277 \tTraining Loss: 0.933494 \tValidation Loss: 0.971844\n",
      "Validation loss decreased (0.971910 --> 0.971844).         Saving model ...\n",
      "Epoch: 278 \tTraining Loss: 0.933357 \tValidation Loss: 0.971779\n",
      "Validation loss decreased (0.971844 --> 0.971779).         Saving model ...\n",
      "Epoch: 279 \tTraining Loss: 0.933219 \tValidation Loss: 0.971714\n",
      "Validation loss decreased (0.971779 --> 0.971714).         Saving model ...\n",
      "Epoch: 280 \tTraining Loss: 0.933083 \tValidation Loss: 0.971649\n",
      "Validation loss decreased (0.971714 --> 0.971649).         Saving model ...\n",
      "Epoch: 281 \tTraining Loss: 0.932947 \tValidation Loss: 0.971585\n",
      "Validation loss decreased (0.971649 --> 0.971585).         Saving model ...\n",
      "Epoch: 282 \tTraining Loss: 0.932811 \tValidation Loss: 0.971521\n",
      "Validation loss decreased (0.971585 --> 0.971521).         Saving model ...\n",
      "Epoch: 283 \tTraining Loss: 0.932676 \tValidation Loss: 0.971458\n",
      "Validation loss decreased (0.971521 --> 0.971458).         Saving model ...\n",
      "Epoch: 284 \tTraining Loss: 0.932542 \tValidation Loss: 0.971395\n",
      "Validation loss decreased (0.971458 --> 0.971395).         Saving model ...\n",
      "Epoch: 285 \tTraining Loss: 0.932408 \tValidation Loss: 0.971332\n",
      "Validation loss decreased (0.971395 --> 0.971332).         Saving model ...\n",
      "Epoch: 286 \tTraining Loss: 0.932275 \tValidation Loss: 0.971270\n",
      "Validation loss decreased (0.971332 --> 0.971270).         Saving model ...\n",
      "Epoch: 287 \tTraining Loss: 0.932142 \tValidation Loss: 0.971208\n",
      "Validation loss decreased (0.971270 --> 0.971208).         Saving model ...\n",
      "Epoch: 288 \tTraining Loss: 0.932010 \tValidation Loss: 0.971147\n",
      "Validation loss decreased (0.971208 --> 0.971147).         Saving model ...\n",
      "Epoch: 289 \tTraining Loss: 0.931878 \tValidation Loss: 0.971086\n",
      "Validation loss decreased (0.971147 --> 0.971086).         Saving model ...\n",
      "Epoch: 290 \tTraining Loss: 0.931747 \tValidation Loss: 0.971025\n",
      "Validation loss decreased (0.971086 --> 0.971025).         Saving model ...\n",
      "Epoch: 291 \tTraining Loss: 0.931616 \tValidation Loss: 0.970965\n",
      "Validation loss decreased (0.971025 --> 0.970965).         Saving model ...\n",
      "Epoch: 292 \tTraining Loss: 0.931486 \tValidation Loss: 0.970905\n",
      "Validation loss decreased (0.970965 --> 0.970905).         Saving model ...\n",
      "Epoch: 293 \tTraining Loss: 0.931356 \tValidation Loss: 0.970845\n",
      "Validation loss decreased (0.970905 --> 0.970845).         Saving model ...\n",
      "Epoch: 294 \tTraining Loss: 0.931227 \tValidation Loss: 0.970786\n",
      "Validation loss decreased (0.970845 --> 0.970786).         Saving model ...\n",
      "Epoch: 295 \tTraining Loss: 0.931098 \tValidation Loss: 0.970727\n",
      "Validation loss decreased (0.970786 --> 0.970727).         Saving model ...\n",
      "Epoch: 296 \tTraining Loss: 0.930970 \tValidation Loss: 0.970668\n",
      "Validation loss decreased (0.970727 --> 0.970668).         Saving model ...\n",
      "Epoch: 297 \tTraining Loss: 0.930842 \tValidation Loss: 0.970610\n",
      "Validation loss decreased (0.970668 --> 0.970610).         Saving model ...\n",
      "Epoch: 298 \tTraining Loss: 0.930715 \tValidation Loss: 0.970552\n",
      "Validation loss decreased (0.970610 --> 0.970552).         Saving model ...\n",
      "Epoch: 299 \tTraining Loss: 0.930588 \tValidation Loss: 0.970494\n",
      "Validation loss decreased (0.970552 --> 0.970494).         Saving model ...\n",
      "Epoch: 300 \tTraining Loss: 0.930462 \tValidation Loss: 0.970437\n",
      "Validation loss decreased (0.970494 --> 0.970437).         Saving model ...\n",
      "Epoch: 301 \tTraining Loss: 0.930336 \tValidation Loss: 0.970380\n",
      "Validation loss decreased (0.970437 --> 0.970380).         Saving model ...\n",
      "Epoch: 302 \tTraining Loss: 0.930211 \tValidation Loss: 0.970323\n",
      "Validation loss decreased (0.970380 --> 0.970323).         Saving model ...\n",
      "Epoch: 303 \tTraining Loss: 0.930086 \tValidation Loss: 0.970267\n",
      "Validation loss decreased (0.970323 --> 0.970267).         Saving model ...\n",
      "Epoch: 304 \tTraining Loss: 0.929962 \tValidation Loss: 0.970211\n",
      "Validation loss decreased (0.970267 --> 0.970211).         Saving model ...\n",
      "Epoch: 305 \tTraining Loss: 0.929838 \tValidation Loss: 0.970156\n",
      "Validation loss decreased (0.970211 --> 0.970156).         Saving model ...\n",
      "Epoch: 306 \tTraining Loss: 0.929714 \tValidation Loss: 0.970100\n",
      "Validation loss decreased (0.970156 --> 0.970100).         Saving model ...\n",
      "Epoch: 307 \tTraining Loss: 0.929591 \tValidation Loss: 0.970045\n",
      "Validation loss decreased (0.970100 --> 0.970045).         Saving model ...\n",
      "Epoch: 308 \tTraining Loss: 0.929469 \tValidation Loss: 0.969991\n",
      "Validation loss decreased (0.970045 --> 0.969991).         Saving model ...\n",
      "Epoch: 309 \tTraining Loss: 0.929347 \tValidation Loss: 0.969936\n",
      "Validation loss decreased (0.969991 --> 0.969936).         Saving model ...\n",
      "Epoch: 310 \tTraining Loss: 0.929225 \tValidation Loss: 0.969882\n",
      "Validation loss decreased (0.969936 --> 0.969882).         Saving model ...\n",
      "Epoch: 311 \tTraining Loss: 0.929104 \tValidation Loss: 0.969829\n",
      "Validation loss decreased (0.969882 --> 0.969829).         Saving model ...\n",
      "Epoch: 312 \tTraining Loss: 0.928983 \tValidation Loss: 0.969775\n",
      "Validation loss decreased (0.969829 --> 0.969775).         Saving model ...\n",
      "Epoch: 313 \tTraining Loss: 0.928863 \tValidation Loss: 0.969722\n",
      "Validation loss decreased (0.969775 --> 0.969722).         Saving model ...\n",
      "Epoch: 314 \tTraining Loss: 0.928743 \tValidation Loss: 0.969669\n",
      "Validation loss decreased (0.969722 --> 0.969669).         Saving model ...\n",
      "Epoch: 315 \tTraining Loss: 0.928624 \tValidation Loss: 0.969617\n",
      "Validation loss decreased (0.969669 --> 0.969617).         Saving model ...\n",
      "Epoch: 316 \tTraining Loss: 0.928505 \tValidation Loss: 0.969564\n",
      "Validation loss decreased (0.969617 --> 0.969564).         Saving model ...\n",
      "Epoch: 317 \tTraining Loss: 0.928386 \tValidation Loss: 0.969513\n",
      "Validation loss decreased (0.969564 --> 0.969513).         Saving model ...\n",
      "Epoch: 318 \tTraining Loss: 0.928268 \tValidation Loss: 0.969461\n",
      "Validation loss decreased (0.969513 --> 0.969461).         Saving model ...\n",
      "Epoch: 319 \tTraining Loss: 0.928150 \tValidation Loss: 0.969410\n",
      "Validation loss decreased (0.969461 --> 0.969410).         Saving model ...\n",
      "Epoch: 320 \tTraining Loss: 0.928033 \tValidation Loss: 0.969358\n",
      "Validation loss decreased (0.969410 --> 0.969358).         Saving model ...\n",
      "Epoch: 321 \tTraining Loss: 0.927916 \tValidation Loss: 0.969308\n",
      "Validation loss decreased (0.969358 --> 0.969308).         Saving model ...\n",
      "Epoch: 322 \tTraining Loss: 0.927799 \tValidation Loss: 0.969257\n",
      "Validation loss decreased (0.969308 --> 0.969257).         Saving model ...\n",
      "Epoch: 323 \tTraining Loss: 0.927683 \tValidation Loss: 0.969207\n",
      "Validation loss decreased (0.969257 --> 0.969207).         Saving model ...\n",
      "Epoch: 324 \tTraining Loss: 0.927568 \tValidation Loss: 0.969157\n",
      "Validation loss decreased (0.969207 --> 0.969157).         Saving model ...\n",
      "Epoch: 325 \tTraining Loss: 0.927452 \tValidation Loss: 0.969107\n",
      "Validation loss decreased (0.969157 --> 0.969107).         Saving model ...\n",
      "Epoch: 326 \tTraining Loss: 0.927337 \tValidation Loss: 0.969058\n",
      "Validation loss decreased (0.969107 --> 0.969058).         Saving model ...\n",
      "Epoch: 327 \tTraining Loss: 0.927223 \tValidation Loss: 0.969009\n",
      "Validation loss decreased (0.969058 --> 0.969009).         Saving model ...\n",
      "Epoch: 328 \tTraining Loss: 0.927109 \tValidation Loss: 0.968960\n",
      "Validation loss decreased (0.969009 --> 0.968960).         Saving model ...\n",
      "Epoch: 329 \tTraining Loss: 0.926995 \tValidation Loss: 0.968911\n",
      "Validation loss decreased (0.968960 --> 0.968911).         Saving model ...\n",
      "Epoch: 330 \tTraining Loss: 0.926882 \tValidation Loss: 0.968863\n",
      "Validation loss decreased (0.968911 --> 0.968863).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 331 \tTraining Loss: 0.926769 \tValidation Loss: 0.968815\n",
      "Validation loss decreased (0.968863 --> 0.968815).         Saving model ...\n",
      "Epoch: 332 \tTraining Loss: 0.926656 \tValidation Loss: 0.968767\n",
      "Validation loss decreased (0.968815 --> 0.968767).         Saving model ...\n",
      "Epoch: 333 \tTraining Loss: 0.926544 \tValidation Loss: 0.968720\n",
      "Validation loss decreased (0.968767 --> 0.968720).         Saving model ...\n",
      "Epoch: 334 \tTraining Loss: 0.926433 \tValidation Loss: 0.968672\n",
      "Validation loss decreased (0.968720 --> 0.968672).         Saving model ...\n",
      "Epoch: 335 \tTraining Loss: 0.926321 \tValidation Loss: 0.968625\n",
      "Validation loss decreased (0.968672 --> 0.968625).         Saving model ...\n",
      "Epoch: 336 \tTraining Loss: 0.926210 \tValidation Loss: 0.968579\n",
      "Validation loss decreased (0.968625 --> 0.968579).         Saving model ...\n",
      "Epoch: 337 \tTraining Loss: 0.926100 \tValidation Loss: 0.968532\n",
      "Validation loss decreased (0.968579 --> 0.968532).         Saving model ...\n",
      "Epoch: 338 \tTraining Loss: 0.925989 \tValidation Loss: 0.968486\n",
      "Validation loss decreased (0.968532 --> 0.968486).         Saving model ...\n",
      "Epoch: 339 \tTraining Loss: 0.925879 \tValidation Loss: 0.968440\n",
      "Validation loss decreased (0.968486 --> 0.968440).         Saving model ...\n",
      "Epoch: 340 \tTraining Loss: 0.925770 \tValidation Loss: 0.968394\n",
      "Validation loss decreased (0.968440 --> 0.968394).         Saving model ...\n",
      "Epoch: 341 \tTraining Loss: 0.925661 \tValidation Loss: 0.968349\n",
      "Validation loss decreased (0.968394 --> 0.968349).         Saving model ...\n",
      "Epoch: 342 \tTraining Loss: 0.925552 \tValidation Loss: 0.968303\n",
      "Validation loss decreased (0.968349 --> 0.968303).         Saving model ...\n",
      "Epoch: 343 \tTraining Loss: 0.925443 \tValidation Loss: 0.968258\n",
      "Validation loss decreased (0.968303 --> 0.968258).         Saving model ...\n",
      "Epoch: 344 \tTraining Loss: 0.925335 \tValidation Loss: 0.968213\n",
      "Validation loss decreased (0.968258 --> 0.968213).         Saving model ...\n",
      "Epoch: 345 \tTraining Loss: 0.925228 \tValidation Loss: 0.968169\n",
      "Validation loss decreased (0.968213 --> 0.968169).         Saving model ...\n",
      "Epoch: 346 \tTraining Loss: 0.925120 \tValidation Loss: 0.968124\n",
      "Validation loss decreased (0.968169 --> 0.968124).         Saving model ...\n",
      "Epoch: 347 \tTraining Loss: 0.925013 \tValidation Loss: 0.968080\n",
      "Validation loss decreased (0.968124 --> 0.968080).         Saving model ...\n",
      "Epoch: 348 \tTraining Loss: 0.924907 \tValidation Loss: 0.968036\n",
      "Validation loss decreased (0.968080 --> 0.968036).         Saving model ...\n",
      "Epoch: 349 \tTraining Loss: 0.924800 \tValidation Loss: 0.967993\n",
      "Validation loss decreased (0.968036 --> 0.967993).         Saving model ...\n",
      "Epoch: 350 \tTraining Loss: 0.924694 \tValidation Loss: 0.967949\n",
      "Validation loss decreased (0.967993 --> 0.967949).         Saving model ...\n",
      "Epoch: 351 \tTraining Loss: 0.924589 \tValidation Loss: 0.967906\n",
      "Validation loss decreased (0.967949 --> 0.967906).         Saving model ...\n",
      "Epoch: 352 \tTraining Loss: 0.924483 \tValidation Loss: 0.967863\n",
      "Validation loss decreased (0.967906 --> 0.967863).         Saving model ...\n",
      "Epoch: 353 \tTraining Loss: 0.924378 \tValidation Loss: 0.967820\n",
      "Validation loss decreased (0.967863 --> 0.967820).         Saving model ...\n",
      "Epoch: 354 \tTraining Loss: 0.924274 \tValidation Loss: 0.967778\n",
      "Validation loss decreased (0.967820 --> 0.967778).         Saving model ...\n",
      "Epoch: 355 \tTraining Loss: 0.924169 \tValidation Loss: 0.967736\n",
      "Validation loss decreased (0.967778 --> 0.967736).         Saving model ...\n",
      "Epoch: 356 \tTraining Loss: 0.924065 \tValidation Loss: 0.967693\n",
      "Validation loss decreased (0.967736 --> 0.967693).         Saving model ...\n",
      "Epoch: 357 \tTraining Loss: 0.923962 \tValidation Loss: 0.967652\n",
      "Validation loss decreased (0.967693 --> 0.967652).         Saving model ...\n",
      "Epoch: 358 \tTraining Loss: 0.923858 \tValidation Loss: 0.967610\n",
      "Validation loss decreased (0.967652 --> 0.967610).         Saving model ...\n",
      "Epoch: 359 \tTraining Loss: 0.923755 \tValidation Loss: 0.967568\n",
      "Validation loss decreased (0.967610 --> 0.967568).         Saving model ...\n",
      "Epoch: 360 \tTraining Loss: 0.923653 \tValidation Loss: 0.967527\n",
      "Validation loss decreased (0.967568 --> 0.967527).         Saving model ...\n",
      "Epoch: 361 \tTraining Loss: 0.923551 \tValidation Loss: 0.967486\n",
      "Validation loss decreased (0.967527 --> 0.967486).         Saving model ...\n",
      "Epoch: 362 \tTraining Loss: 0.923448 \tValidation Loss: 0.967445\n",
      "Validation loss decreased (0.967486 --> 0.967445).         Saving model ...\n",
      "Epoch: 363 \tTraining Loss: 0.923347 \tValidation Loss: 0.967405\n",
      "Validation loss decreased (0.967445 --> 0.967405).         Saving model ...\n",
      "Epoch: 364 \tTraining Loss: 0.923245 \tValidation Loss: 0.967364\n",
      "Validation loss decreased (0.967405 --> 0.967364).         Saving model ...\n",
      "Epoch: 365 \tTraining Loss: 0.923144 \tValidation Loss: 0.967324\n",
      "Validation loss decreased (0.967364 --> 0.967324).         Saving model ...\n",
      "Epoch: 366 \tTraining Loss: 0.923044 \tValidation Loss: 0.967284\n",
      "Validation loss decreased (0.967324 --> 0.967284).         Saving model ...\n",
      "Epoch: 367 \tTraining Loss: 0.922943 \tValidation Loss: 0.967244\n",
      "Validation loss decreased (0.967284 --> 0.967244).         Saving model ...\n",
      "Epoch: 368 \tTraining Loss: 0.922843 \tValidation Loss: 0.967205\n",
      "Validation loss decreased (0.967244 --> 0.967205).         Saving model ...\n",
      "Epoch: 369 \tTraining Loss: 0.922743 \tValidation Loss: 0.967165\n",
      "Validation loss decreased (0.967205 --> 0.967165).         Saving model ...\n",
      "Epoch: 370 \tTraining Loss: 0.922644 \tValidation Loss: 0.967126\n",
      "Validation loss decreased (0.967165 --> 0.967126).         Saving model ...\n",
      "Epoch: 371 \tTraining Loss: 0.922544 \tValidation Loss: 0.967087\n",
      "Validation loss decreased (0.967126 --> 0.967087).         Saving model ...\n",
      "Epoch: 372 \tTraining Loss: 0.922445 \tValidation Loss: 0.967048\n",
      "Validation loss decreased (0.967087 --> 0.967048).         Saving model ...\n",
      "Epoch: 373 \tTraining Loss: 0.922347 \tValidation Loss: 0.967010\n",
      "Validation loss decreased (0.967048 --> 0.967010).         Saving model ...\n",
      "Epoch: 374 \tTraining Loss: 0.922248 \tValidation Loss: 0.966971\n",
      "Validation loss decreased (0.967010 --> 0.966971).         Saving model ...\n",
      "Epoch: 375 \tTraining Loss: 0.922150 \tValidation Loss: 0.966933\n",
      "Validation loss decreased (0.966971 --> 0.966933).         Saving model ...\n",
      "Epoch: 376 \tTraining Loss: 0.922053 \tValidation Loss: 0.966895\n",
      "Validation loss decreased (0.966933 --> 0.966895).         Saving model ...\n",
      "Epoch: 377 \tTraining Loss: 0.921955 \tValidation Loss: 0.966857\n",
      "Validation loss decreased (0.966895 --> 0.966857).         Saving model ...\n",
      "Epoch: 378 \tTraining Loss: 0.921858 \tValidation Loss: 0.966819\n",
      "Validation loss decreased (0.966857 --> 0.966819).         Saving model ...\n",
      "Epoch: 379 \tTraining Loss: 0.921761 \tValidation Loss: 0.966782\n",
      "Validation loss decreased (0.966819 --> 0.966782).         Saving model ...\n",
      "Epoch: 380 \tTraining Loss: 0.921665 \tValidation Loss: 0.966745\n",
      "Validation loss decreased (0.966782 --> 0.966745).         Saving model ...\n",
      "Epoch: 381 \tTraining Loss: 0.921568 \tValidation Loss: 0.966707\n",
      "Validation loss decreased (0.966745 --> 0.966707).         Saving model ...\n",
      "Epoch: 382 \tTraining Loss: 0.921472 \tValidation Loss: 0.966670\n",
      "Validation loss decreased (0.966707 --> 0.966670).         Saving model ...\n",
      "Epoch: 383 \tTraining Loss: 0.921377 \tValidation Loss: 0.966634\n",
      "Validation loss decreased (0.966670 --> 0.966634).         Saving model ...\n",
      "Epoch: 384 \tTraining Loss: 0.921281 \tValidation Loss: 0.966597\n",
      "Validation loss decreased (0.966634 --> 0.966597).         Saving model ...\n",
      "Epoch: 385 \tTraining Loss: 0.921186 \tValidation Loss: 0.966561\n",
      "Validation loss decreased (0.966597 --> 0.966561).         Saving model ...\n",
      "Epoch: 386 \tTraining Loss: 0.921091 \tValidation Loss: 0.966524\n",
      "Validation loss decreased (0.966561 --> 0.966524).         Saving model ...\n",
      "Epoch: 387 \tTraining Loss: 0.920997 \tValidation Loss: 0.966488\n",
      "Validation loss decreased (0.966524 --> 0.966488).         Saving model ...\n",
      "Epoch: 388 \tTraining Loss: 0.920902 \tValidation Loss: 0.966453\n",
      "Validation loss decreased (0.966488 --> 0.966453).         Saving model ...\n",
      "Epoch: 389 \tTraining Loss: 0.920808 \tValidation Loss: 0.966417\n",
      "Validation loss decreased (0.966453 --> 0.966417).         Saving model ...\n",
      "Epoch: 390 \tTraining Loss: 0.920714 \tValidation Loss: 0.966381\n",
      "Validation loss decreased (0.966417 --> 0.966381).         Saving model ...\n",
      "Epoch: 391 \tTraining Loss: 0.920621 \tValidation Loss: 0.966346\n",
      "Validation loss decreased (0.966381 --> 0.966346).         Saving model ...\n",
      "Epoch: 392 \tTraining Loss: 0.920528 \tValidation Loss: 0.966311\n",
      "Validation loss decreased (0.966346 --> 0.966311).         Saving model ...\n",
      "Epoch: 393 \tTraining Loss: 0.920435 \tValidation Loss: 0.966276\n",
      "Validation loss decreased (0.966311 --> 0.966276).         Saving model ...\n",
      "Epoch: 394 \tTraining Loss: 0.920342 \tValidation Loss: 0.966241\n",
      "Validation loss decreased (0.966276 --> 0.966241).         Saving model ...\n",
      "Epoch: 395 \tTraining Loss: 0.920249 \tValidation Loss: 0.966206\n",
      "Validation loss decreased (0.966241 --> 0.966206).         Saving model ...\n",
      "Epoch: 396 \tTraining Loss: 0.920157 \tValidation Loss: 0.966171\n",
      "Validation loss decreased (0.966206 --> 0.966171).         Saving model ...\n",
      "Epoch: 397 \tTraining Loss: 0.920065 \tValidation Loss: 0.966137\n",
      "Validation loss decreased (0.966171 --> 0.966137).         Saving model ...\n",
      "Epoch: 398 \tTraining Loss: 0.919974 \tValidation Loss: 0.966103\n",
      "Validation loss decreased (0.966137 --> 0.966103).         Saving model ...\n",
      "Epoch: 399 \tTraining Loss: 0.919882 \tValidation Loss: 0.966069\n",
      "Validation loss decreased (0.966103 --> 0.966069).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 400 \tTraining Loss: 0.919791 \tValidation Loss: 0.966035\n",
      "Validation loss decreased (0.966069 --> 0.966035).         Saving model ...\n",
      "Epoch: 401 \tTraining Loss: 0.919700 \tValidation Loss: 0.966001\n",
      "Validation loss decreased (0.966035 --> 0.966001).         Saving model ...\n",
      "Epoch: 402 \tTraining Loss: 0.919609 \tValidation Loss: 0.965968\n",
      "Validation loss decreased (0.966001 --> 0.965968).         Saving model ...\n",
      "Epoch: 403 \tTraining Loss: 0.919519 \tValidation Loss: 0.965934\n",
      "Validation loss decreased (0.965968 --> 0.965934).         Saving model ...\n",
      "Epoch: 404 \tTraining Loss: 0.919429 \tValidation Loss: 0.965901\n",
      "Validation loss decreased (0.965934 --> 0.965901).         Saving model ...\n",
      "Epoch: 405 \tTraining Loss: 0.919339 \tValidation Loss: 0.965868\n",
      "Validation loss decreased (0.965901 --> 0.965868).         Saving model ...\n",
      "Epoch: 406 \tTraining Loss: 0.919249 \tValidation Loss: 0.965835\n",
      "Validation loss decreased (0.965868 --> 0.965835).         Saving model ...\n",
      "Epoch: 407 \tTraining Loss: 0.919160 \tValidation Loss: 0.965802\n",
      "Validation loss decreased (0.965835 --> 0.965802).         Saving model ...\n",
      "Epoch: 408 \tTraining Loss: 0.919071 \tValidation Loss: 0.965769\n",
      "Validation loss decreased (0.965802 --> 0.965769).         Saving model ...\n",
      "Epoch: 409 \tTraining Loss: 0.918982 \tValidation Loss: 0.965737\n",
      "Validation loss decreased (0.965769 --> 0.965737).         Saving model ...\n",
      "Epoch: 410 \tTraining Loss: 0.918893 \tValidation Loss: 0.965705\n",
      "Validation loss decreased (0.965737 --> 0.965705).         Saving model ...\n",
      "Epoch: 411 \tTraining Loss: 0.918805 \tValidation Loss: 0.965672\n",
      "Validation loss decreased (0.965705 --> 0.965672).         Saving model ...\n",
      "Epoch: 412 \tTraining Loss: 0.918717 \tValidation Loss: 0.965640\n",
      "Validation loss decreased (0.965672 --> 0.965640).         Saving model ...\n",
      "Epoch: 413 \tTraining Loss: 0.918629 \tValidation Loss: 0.965608\n",
      "Validation loss decreased (0.965640 --> 0.965608).         Saving model ...\n",
      "Epoch: 414 \tTraining Loss: 0.918541 \tValidation Loss: 0.965577\n",
      "Validation loss decreased (0.965608 --> 0.965577).         Saving model ...\n",
      "Epoch: 415 \tTraining Loss: 0.918454 \tValidation Loss: 0.965545\n",
      "Validation loss decreased (0.965577 --> 0.965545).         Saving model ...\n",
      "Epoch: 416 \tTraining Loss: 0.918366 \tValidation Loss: 0.965514\n",
      "Validation loss decreased (0.965545 --> 0.965514).         Saving model ...\n",
      "Epoch: 417 \tTraining Loss: 0.918279 \tValidation Loss: 0.965482\n",
      "Validation loss decreased (0.965514 --> 0.965482).         Saving model ...\n",
      "Epoch: 418 \tTraining Loss: 0.918193 \tValidation Loss: 0.965451\n",
      "Validation loss decreased (0.965482 --> 0.965451).         Saving model ...\n",
      "Epoch: 419 \tTraining Loss: 0.918106 \tValidation Loss: 0.965420\n",
      "Validation loss decreased (0.965451 --> 0.965420).         Saving model ...\n",
      "Epoch: 420 \tTraining Loss: 0.918020 \tValidation Loss: 0.965389\n",
      "Validation loss decreased (0.965420 --> 0.965389).         Saving model ...\n",
      "Epoch: 421 \tTraining Loss: 0.917934 \tValidation Loss: 0.965358\n",
      "Validation loss decreased (0.965389 --> 0.965358).         Saving model ...\n",
      "Epoch: 422 \tTraining Loss: 0.917848 \tValidation Loss: 0.965328\n",
      "Validation loss decreased (0.965358 --> 0.965328).         Saving model ...\n",
      "Epoch: 423 \tTraining Loss: 0.917762 \tValidation Loss: 0.965297\n",
      "Validation loss decreased (0.965328 --> 0.965297).         Saving model ...\n",
      "Epoch: 424 \tTraining Loss: 0.917677 \tValidation Loss: 0.965267\n",
      "Validation loss decreased (0.965297 --> 0.965267).         Saving model ...\n",
      "Epoch: 425 \tTraining Loss: 0.917592 \tValidation Loss: 0.965237\n",
      "Validation loss decreased (0.965267 --> 0.965237).         Saving model ...\n",
      "Epoch: 426 \tTraining Loss: 0.917507 \tValidation Loss: 0.965207\n",
      "Validation loss decreased (0.965237 --> 0.965207).         Saving model ...\n",
      "Epoch: 427 \tTraining Loss: 0.917422 \tValidation Loss: 0.965177\n",
      "Validation loss decreased (0.965207 --> 0.965177).         Saving model ...\n",
      "Epoch: 428 \tTraining Loss: 0.917338 \tValidation Loss: 0.965147\n",
      "Validation loss decreased (0.965177 --> 0.965147).         Saving model ...\n",
      "Epoch: 429 \tTraining Loss: 0.917253 \tValidation Loss: 0.965117\n",
      "Validation loss decreased (0.965147 --> 0.965117).         Saving model ...\n",
      "Epoch: 430 \tTraining Loss: 0.917169 \tValidation Loss: 0.965088\n",
      "Validation loss decreased (0.965117 --> 0.965088).         Saving model ...\n",
      "Epoch: 431 \tTraining Loss: 0.917086 \tValidation Loss: 0.965058\n",
      "Validation loss decreased (0.965088 --> 0.965058).         Saving model ...\n",
      "Epoch: 432 \tTraining Loss: 0.917002 \tValidation Loss: 0.965029\n",
      "Validation loss decreased (0.965058 --> 0.965029).         Saving model ...\n",
      "Epoch: 433 \tTraining Loss: 0.916919 \tValidation Loss: 0.965000\n",
      "Validation loss decreased (0.965029 --> 0.965000).         Saving model ...\n",
      "Epoch: 434 \tTraining Loss: 0.916835 \tValidation Loss: 0.964971\n",
      "Validation loss decreased (0.965000 --> 0.964971).         Saving model ...\n",
      "Epoch: 435 \tTraining Loss: 0.916752 \tValidation Loss: 0.964942\n",
      "Validation loss decreased (0.964971 --> 0.964942).         Saving model ...\n",
      "Epoch: 436 \tTraining Loss: 0.916670 \tValidation Loss: 0.964913\n",
      "Validation loss decreased (0.964942 --> 0.964913).         Saving model ...\n",
      "Epoch: 437 \tTraining Loss: 0.916587 \tValidation Loss: 0.964885\n",
      "Validation loss decreased (0.964913 --> 0.964885).         Saving model ...\n",
      "Epoch: 438 \tTraining Loss: 0.916505 \tValidation Loss: 0.964856\n",
      "Validation loss decreased (0.964885 --> 0.964856).         Saving model ...\n",
      "Epoch: 439 \tTraining Loss: 0.916423 \tValidation Loss: 0.964828\n",
      "Validation loss decreased (0.964856 --> 0.964828).         Saving model ...\n",
      "Epoch: 440 \tTraining Loss: 0.916341 \tValidation Loss: 0.964799\n",
      "Validation loss decreased (0.964828 --> 0.964799).         Saving model ...\n",
      "Epoch: 441 \tTraining Loss: 0.916259 \tValidation Loss: 0.964771\n",
      "Validation loss decreased (0.964799 --> 0.964771).         Saving model ...\n",
      "Epoch: 442 \tTraining Loss: 0.916178 \tValidation Loss: 0.964743\n",
      "Validation loss decreased (0.964771 --> 0.964743).         Saving model ...\n",
      "Epoch: 443 \tTraining Loss: 0.916096 \tValidation Loss: 0.964715\n",
      "Validation loss decreased (0.964743 --> 0.964715).         Saving model ...\n",
      "Epoch: 444 \tTraining Loss: 0.916015 \tValidation Loss: 0.964688\n",
      "Validation loss decreased (0.964715 --> 0.964688).         Saving model ...\n",
      "Epoch: 445 \tTraining Loss: 0.915934 \tValidation Loss: 0.964660\n",
      "Validation loss decreased (0.964688 --> 0.964660).         Saving model ...\n",
      "Epoch: 446 \tTraining Loss: 0.915854 \tValidation Loss: 0.964632\n",
      "Validation loss decreased (0.964660 --> 0.964632).         Saving model ...\n",
      "Epoch: 447 \tTraining Loss: 0.915773 \tValidation Loss: 0.964605\n",
      "Validation loss decreased (0.964632 --> 0.964605).         Saving model ...\n",
      "Epoch: 448 \tTraining Loss: 0.915693 \tValidation Loss: 0.964578\n",
      "Validation loss decreased (0.964605 --> 0.964578).         Saving model ...\n",
      "Epoch: 449 \tTraining Loss: 0.915613 \tValidation Loss: 0.964551\n",
      "Validation loss decreased (0.964578 --> 0.964551).         Saving model ...\n",
      "Epoch: 450 \tTraining Loss: 0.915533 \tValidation Loss: 0.964523\n",
      "Validation loss decreased (0.964551 --> 0.964523).         Saving model ...\n",
      "Epoch: 451 \tTraining Loss: 0.915453 \tValidation Loss: 0.964496\n",
      "Validation loss decreased (0.964523 --> 0.964496).         Saving model ...\n",
      "Epoch: 452 \tTraining Loss: 0.915374 \tValidation Loss: 0.964470\n",
      "Validation loss decreased (0.964496 --> 0.964470).         Saving model ...\n",
      "Epoch: 453 \tTraining Loss: 0.915295 \tValidation Loss: 0.964443\n",
      "Validation loss decreased (0.964470 --> 0.964443).         Saving model ...\n",
      "Epoch: 454 \tTraining Loss: 0.915216 \tValidation Loss: 0.964416\n",
      "Validation loss decreased (0.964443 --> 0.964416).         Saving model ...\n",
      "Epoch: 455 \tTraining Loss: 0.915137 \tValidation Loss: 0.964390\n",
      "Validation loss decreased (0.964416 --> 0.964390).         Saving model ...\n",
      "Epoch: 456 \tTraining Loss: 0.915058 \tValidation Loss: 0.964364\n",
      "Validation loss decreased (0.964390 --> 0.964364).         Saving model ...\n",
      "Epoch: 457 \tTraining Loss: 0.914979 \tValidation Loss: 0.964337\n",
      "Validation loss decreased (0.964364 --> 0.964337).         Saving model ...\n",
      "Epoch: 458 \tTraining Loss: 0.914901 \tValidation Loss: 0.964311\n",
      "Validation loss decreased (0.964337 --> 0.964311).         Saving model ...\n",
      "Epoch: 459 \tTraining Loss: 0.914823 \tValidation Loss: 0.964285\n",
      "Validation loss decreased (0.964311 --> 0.964285).         Saving model ...\n",
      "Epoch: 460 \tTraining Loss: 0.914745 \tValidation Loss: 0.964259\n",
      "Validation loss decreased (0.964285 --> 0.964259).         Saving model ...\n",
      "Epoch: 461 \tTraining Loss: 0.914667 \tValidation Loss: 0.964233\n",
      "Validation loss decreased (0.964259 --> 0.964233).         Saving model ...\n",
      "Epoch: 462 \tTraining Loss: 0.914590 \tValidation Loss: 0.964208\n",
      "Validation loss decreased (0.964233 --> 0.964208).         Saving model ...\n",
      "Epoch: 463 \tTraining Loss: 0.914512 \tValidation Loss: 0.964182\n",
      "Validation loss decreased (0.964208 --> 0.964182).         Saving model ...\n",
      "Epoch: 464 \tTraining Loss: 0.914435 \tValidation Loss: 0.964157\n",
      "Validation loss decreased (0.964182 --> 0.964157).         Saving model ...\n",
      "Epoch: 465 \tTraining Loss: 0.914358 \tValidation Loss: 0.964131\n",
      "Validation loss decreased (0.964157 --> 0.964131).         Saving model ...\n",
      "Epoch: 466 \tTraining Loss: 0.914282 \tValidation Loss: 0.964106\n",
      "Validation loss decreased (0.964131 --> 0.964106).         Saving model ...\n",
      "Epoch: 467 \tTraining Loss: 0.914205 \tValidation Loss: 0.964081\n",
      "Validation loss decreased (0.964106 --> 0.964081).         Saving model ...\n",
      "Epoch: 468 \tTraining Loss: 0.914128 \tValidation Loss: 0.964056\n",
      "Validation loss decreased (0.964081 --> 0.964056).         Saving model ...\n",
      "Epoch: 469 \tTraining Loss: 0.914052 \tValidation Loss: 0.964031\n",
      "Validation loss decreased (0.964056 --> 0.964031).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 470 \tTraining Loss: 0.913976 \tValidation Loss: 0.964006\n",
      "Validation loss decreased (0.964031 --> 0.964006).         Saving model ...\n",
      "Epoch: 471 \tTraining Loss: 0.913900 \tValidation Loss: 0.963981\n",
      "Validation loss decreased (0.964006 --> 0.963981).         Saving model ...\n",
      "Epoch: 472 \tTraining Loss: 0.913824 \tValidation Loss: 0.963956\n",
      "Validation loss decreased (0.963981 --> 0.963956).         Saving model ...\n",
      "Epoch: 473 \tTraining Loss: 0.913749 \tValidation Loss: 0.963932\n",
      "Validation loss decreased (0.963956 --> 0.963932).         Saving model ...\n",
      "Epoch: 474 \tTraining Loss: 0.913674 \tValidation Loss: 0.963907\n",
      "Validation loss decreased (0.963932 --> 0.963907).         Saving model ...\n",
      "Epoch: 475 \tTraining Loss: 0.913598 \tValidation Loss: 0.963883\n",
      "Validation loss decreased (0.963907 --> 0.963883).         Saving model ...\n",
      "Epoch: 476 \tTraining Loss: 0.913523 \tValidation Loss: 0.963859\n",
      "Validation loss decreased (0.963883 --> 0.963859).         Saving model ...\n",
      "Epoch: 477 \tTraining Loss: 0.913449 \tValidation Loss: 0.963835\n",
      "Validation loss decreased (0.963859 --> 0.963835).         Saving model ...\n",
      "Epoch: 478 \tTraining Loss: 0.913374 \tValidation Loss: 0.963811\n",
      "Validation loss decreased (0.963835 --> 0.963811).         Saving model ...\n",
      "Epoch: 479 \tTraining Loss: 0.913299 \tValidation Loss: 0.963787\n",
      "Validation loss decreased (0.963811 --> 0.963787).         Saving model ...\n",
      "Epoch: 480 \tTraining Loss: 0.913225 \tValidation Loss: 0.963763\n",
      "Validation loss decreased (0.963787 --> 0.963763).         Saving model ...\n",
      "Epoch: 481 \tTraining Loss: 0.913151 \tValidation Loss: 0.963739\n",
      "Validation loss decreased (0.963763 --> 0.963739).         Saving model ...\n",
      "Epoch: 482 \tTraining Loss: 0.913077 \tValidation Loss: 0.963716\n",
      "Validation loss decreased (0.963739 --> 0.963716).         Saving model ...\n",
      "Epoch: 483 \tTraining Loss: 0.913003 \tValidation Loss: 0.963692\n",
      "Validation loss decreased (0.963716 --> 0.963692).         Saving model ...\n",
      "Epoch: 484 \tTraining Loss: 0.912930 \tValidation Loss: 0.963669\n",
      "Validation loss decreased (0.963692 --> 0.963669).         Saving model ...\n",
      "Epoch: 485 \tTraining Loss: 0.912856 \tValidation Loss: 0.963645\n",
      "Validation loss decreased (0.963669 --> 0.963645).         Saving model ...\n",
      "Epoch: 486 \tTraining Loss: 0.912783 \tValidation Loss: 0.963622\n",
      "Validation loss decreased (0.963645 --> 0.963622).         Saving model ...\n",
      "Epoch: 487 \tTraining Loss: 0.912710 \tValidation Loss: 0.963599\n",
      "Validation loss decreased (0.963622 --> 0.963599).         Saving model ...\n",
      "Epoch: 488 \tTraining Loss: 0.912637 \tValidation Loss: 0.963576\n",
      "Validation loss decreased (0.963599 --> 0.963576).         Saving model ...\n",
      "Epoch: 489 \tTraining Loss: 0.912564 \tValidation Loss: 0.963553\n",
      "Validation loss decreased (0.963576 --> 0.963553).         Saving model ...\n",
      "Epoch: 490 \tTraining Loss: 0.912491 \tValidation Loss: 0.963530\n",
      "Validation loss decreased (0.963553 --> 0.963530).         Saving model ...\n",
      "Epoch: 491 \tTraining Loss: 0.912419 \tValidation Loss: 0.963507\n",
      "Validation loss decreased (0.963530 --> 0.963507).         Saving model ...\n",
      "Epoch: 492 \tTraining Loss: 0.912347 \tValidation Loss: 0.963484\n",
      "Validation loss decreased (0.963507 --> 0.963484).         Saving model ...\n",
      "Epoch: 493 \tTraining Loss: 0.912274 \tValidation Loss: 0.963462\n",
      "Validation loss decreased (0.963484 --> 0.963462).         Saving model ...\n",
      "Epoch: 494 \tTraining Loss: 0.912202 \tValidation Loss: 0.963439\n",
      "Validation loss decreased (0.963462 --> 0.963439).         Saving model ...\n",
      "Epoch: 495 \tTraining Loss: 0.912131 \tValidation Loss: 0.963417\n",
      "Validation loss decreased (0.963439 --> 0.963417).         Saving model ...\n",
      "Epoch: 496 \tTraining Loss: 0.912059 \tValidation Loss: 0.963394\n",
      "Validation loss decreased (0.963417 --> 0.963394).         Saving model ...\n",
      "Epoch: 497 \tTraining Loss: 0.911988 \tValidation Loss: 0.963372\n",
      "Validation loss decreased (0.963394 --> 0.963372).         Saving model ...\n",
      "Epoch: 498 \tTraining Loss: 0.911916 \tValidation Loss: 0.963350\n",
      "Validation loss decreased (0.963372 --> 0.963350).         Saving model ...\n",
      "Epoch: 499 \tTraining Loss: 0.911845 \tValidation Loss: 0.963328\n",
      "Validation loss decreased (0.963350 --> 0.963328).         Saving model ...\n",
      "Epoch: 500 \tTraining Loss: 0.911774 \tValidation Loss: 0.963306\n",
      "Validation loss decreased (0.963328 --> 0.963306).         Saving model ...\n",
      "Epoch: 501 \tTraining Loss: 0.911703 \tValidation Loss: 0.963284\n",
      "Validation loss decreased (0.963306 --> 0.963284).         Saving model ...\n",
      "Epoch: 502 \tTraining Loss: 0.911632 \tValidation Loss: 0.963262\n",
      "Validation loss decreased (0.963284 --> 0.963262).         Saving model ...\n",
      "Epoch: 503 \tTraining Loss: 0.911562 \tValidation Loss: 0.963241\n",
      "Validation loss decreased (0.963262 --> 0.963241).         Saving model ...\n",
      "Epoch: 504 \tTraining Loss: 0.911492 \tValidation Loss: 0.963219\n",
      "Validation loss decreased (0.963241 --> 0.963219).         Saving model ...\n",
      "Epoch: 505 \tTraining Loss: 0.911421 \tValidation Loss: 0.963197\n",
      "Validation loss decreased (0.963219 --> 0.963197).         Saving model ...\n",
      "Epoch: 506 \tTraining Loss: 0.911351 \tValidation Loss: 0.963176\n",
      "Validation loss decreased (0.963197 --> 0.963176).         Saving model ...\n",
      "Epoch: 507 \tTraining Loss: 0.911281 \tValidation Loss: 0.963155\n",
      "Validation loss decreased (0.963176 --> 0.963155).         Saving model ...\n",
      "Epoch: 508 \tTraining Loss: 0.911212 \tValidation Loss: 0.963133\n",
      "Validation loss decreased (0.963155 --> 0.963133).         Saving model ...\n",
      "Epoch: 509 \tTraining Loss: 0.911142 \tValidation Loss: 0.963112\n",
      "Validation loss decreased (0.963133 --> 0.963112).         Saving model ...\n",
      "Epoch: 510 \tTraining Loss: 0.911073 \tValidation Loss: 0.963091\n",
      "Validation loss decreased (0.963112 --> 0.963091).         Saving model ...\n",
      "Epoch: 511 \tTraining Loss: 0.911003 \tValidation Loss: 0.963070\n",
      "Validation loss decreased (0.963091 --> 0.963070).         Saving model ...\n",
      "Epoch: 512 \tTraining Loss: 0.910934 \tValidation Loss: 0.963049\n",
      "Validation loss decreased (0.963070 --> 0.963049).         Saving model ...\n",
      "Epoch: 513 \tTraining Loss: 0.910865 \tValidation Loss: 0.963028\n",
      "Validation loss decreased (0.963049 --> 0.963028).         Saving model ...\n",
      "Epoch: 514 \tTraining Loss: 0.910796 \tValidation Loss: 0.963007\n",
      "Validation loss decreased (0.963028 --> 0.963007).         Saving model ...\n",
      "Epoch: 515 \tTraining Loss: 0.910728 \tValidation Loss: 0.962987\n",
      "Validation loss decreased (0.963007 --> 0.962987).         Saving model ...\n",
      "Epoch: 516 \tTraining Loss: 0.910659 \tValidation Loss: 0.962966\n",
      "Validation loss decreased (0.962987 --> 0.962966).         Saving model ...\n",
      "Epoch: 517 \tTraining Loss: 0.910591 \tValidation Loss: 0.962945\n",
      "Validation loss decreased (0.962966 --> 0.962945).         Saving model ...\n",
      "Epoch: 518 \tTraining Loss: 0.910522 \tValidation Loss: 0.962925\n",
      "Validation loss decreased (0.962945 --> 0.962925).         Saving model ...\n",
      "Epoch: 519 \tTraining Loss: 0.910454 \tValidation Loss: 0.962905\n",
      "Validation loss decreased (0.962925 --> 0.962905).         Saving model ...\n",
      "Epoch: 520 \tTraining Loss: 0.910386 \tValidation Loss: 0.962884\n",
      "Validation loss decreased (0.962905 --> 0.962884).         Saving model ...\n",
      "Epoch: 521 \tTraining Loss: 0.910318 \tValidation Loss: 0.962864\n",
      "Validation loss decreased (0.962884 --> 0.962864).         Saving model ...\n",
      "Epoch: 522 \tTraining Loss: 0.910251 \tValidation Loss: 0.962844\n",
      "Validation loss decreased (0.962864 --> 0.962844).         Saving model ...\n",
      "Epoch: 523 \tTraining Loss: 0.910183 \tValidation Loss: 0.962824\n",
      "Validation loss decreased (0.962844 --> 0.962824).         Saving model ...\n",
      "Epoch: 524 \tTraining Loss: 0.910116 \tValidation Loss: 0.962804\n",
      "Validation loss decreased (0.962824 --> 0.962804).         Saving model ...\n",
      "Epoch: 525 \tTraining Loss: 0.910049 \tValidation Loss: 0.962784\n",
      "Validation loss decreased (0.962804 --> 0.962784).         Saving model ...\n",
      "Epoch: 526 \tTraining Loss: 0.909981 \tValidation Loss: 0.962764\n",
      "Validation loss decreased (0.962784 --> 0.962764).         Saving model ...\n",
      "Epoch: 527 \tTraining Loss: 0.909914 \tValidation Loss: 0.962744\n",
      "Validation loss decreased (0.962764 --> 0.962744).         Saving model ...\n",
      "Epoch: 528 \tTraining Loss: 0.909848 \tValidation Loss: 0.962725\n",
      "Validation loss decreased (0.962744 --> 0.962725).         Saving model ...\n",
      "Epoch: 529 \tTraining Loss: 0.909781 \tValidation Loss: 0.962705\n",
      "Validation loss decreased (0.962725 --> 0.962705).         Saving model ...\n",
      "Epoch: 530 \tTraining Loss: 0.909714 \tValidation Loss: 0.962685\n",
      "Validation loss decreased (0.962705 --> 0.962685).         Saving model ...\n",
      "Epoch: 531 \tTraining Loss: 0.909648 \tValidation Loss: 0.962666\n",
      "Validation loss decreased (0.962685 --> 0.962666).         Saving model ...\n",
      "Epoch: 532 \tTraining Loss: 0.909582 \tValidation Loss: 0.962647\n",
      "Validation loss decreased (0.962666 --> 0.962647).         Saving model ...\n",
      "Epoch: 533 \tTraining Loss: 0.909516 \tValidation Loss: 0.962627\n",
      "Validation loss decreased (0.962647 --> 0.962627).         Saving model ...\n",
      "Epoch: 534 \tTraining Loss: 0.909450 \tValidation Loss: 0.962608\n",
      "Validation loss decreased (0.962627 --> 0.962608).         Saving model ...\n",
      "Epoch: 535 \tTraining Loss: 0.909384 \tValidation Loss: 0.962589\n",
      "Validation loss decreased (0.962608 --> 0.962589).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 536 \tTraining Loss: 0.909318 \tValidation Loss: 0.962570\n",
      "Validation loss decreased (0.962589 --> 0.962570).         Saving model ...\n",
      "Epoch: 537 \tTraining Loss: 0.909252 \tValidation Loss: 0.962551\n",
      "Validation loss decreased (0.962570 --> 0.962551).         Saving model ...\n",
      "Epoch: 538 \tTraining Loss: 0.909187 \tValidation Loss: 0.962532\n",
      "Validation loss decreased (0.962551 --> 0.962532).         Saving model ...\n",
      "Epoch: 539 \tTraining Loss: 0.909122 \tValidation Loss: 0.962513\n",
      "Validation loss decreased (0.962532 --> 0.962513).         Saving model ...\n",
      "Epoch: 540 \tTraining Loss: 0.909057 \tValidation Loss: 0.962494\n",
      "Validation loss decreased (0.962513 --> 0.962494).         Saving model ...\n",
      "Epoch: 541 \tTraining Loss: 0.908991 \tValidation Loss: 0.962475\n",
      "Validation loss decreased (0.962494 --> 0.962475).         Saving model ...\n",
      "Epoch: 542 \tTraining Loss: 0.908927 \tValidation Loss: 0.962457\n",
      "Validation loss decreased (0.962475 --> 0.962457).         Saving model ...\n",
      "Epoch: 543 \tTraining Loss: 0.908862 \tValidation Loss: 0.962438\n",
      "Validation loss decreased (0.962457 --> 0.962438).         Saving model ...\n",
      "Epoch: 544 \tTraining Loss: 0.908797 \tValidation Loss: 0.962420\n",
      "Validation loss decreased (0.962438 --> 0.962420).         Saving model ...\n",
      "Epoch: 545 \tTraining Loss: 0.908733 \tValidation Loss: 0.962401\n",
      "Validation loss decreased (0.962420 --> 0.962401).         Saving model ...\n",
      "Epoch: 546 \tTraining Loss: 0.908668 \tValidation Loss: 0.962383\n",
      "Validation loss decreased (0.962401 --> 0.962383).         Saving model ...\n",
      "Epoch: 547 \tTraining Loss: 0.908604 \tValidation Loss: 0.962365\n",
      "Validation loss decreased (0.962383 --> 0.962365).         Saving model ...\n",
      "Epoch: 548 \tTraining Loss: 0.908540 \tValidation Loss: 0.962346\n",
      "Validation loss decreased (0.962365 --> 0.962346).         Saving model ...\n",
      "Epoch: 549 \tTraining Loss: 0.908476 \tValidation Loss: 0.962328\n",
      "Validation loss decreased (0.962346 --> 0.962328).         Saving model ...\n",
      "Epoch: 550 \tTraining Loss: 0.908412 \tValidation Loss: 0.962310\n",
      "Validation loss decreased (0.962328 --> 0.962310).         Saving model ...\n",
      "Epoch: 551 \tTraining Loss: 0.908348 \tValidation Loss: 0.962292\n",
      "Validation loss decreased (0.962310 --> 0.962292).         Saving model ...\n",
      "Epoch: 552 \tTraining Loss: 0.908285 \tValidation Loss: 0.962274\n",
      "Validation loss decreased (0.962292 --> 0.962274).         Saving model ...\n",
      "Epoch: 553 \tTraining Loss: 0.908221 \tValidation Loss: 0.962256\n",
      "Validation loss decreased (0.962274 --> 0.962256).         Saving model ...\n",
      "Epoch: 554 \tTraining Loss: 0.908158 \tValidation Loss: 0.962238\n",
      "Validation loss decreased (0.962256 --> 0.962238).         Saving model ...\n",
      "Epoch: 555 \tTraining Loss: 0.908095 \tValidation Loss: 0.962221\n",
      "Validation loss decreased (0.962238 --> 0.962221).         Saving model ...\n",
      "Epoch: 556 \tTraining Loss: 0.908032 \tValidation Loss: 0.962203\n",
      "Validation loss decreased (0.962221 --> 0.962203).         Saving model ...\n",
      "Epoch: 557 \tTraining Loss: 0.907969 \tValidation Loss: 0.962185\n",
      "Validation loss decreased (0.962203 --> 0.962185).         Saving model ...\n",
      "Epoch: 558 \tTraining Loss: 0.907906 \tValidation Loss: 0.962168\n",
      "Validation loss decreased (0.962185 --> 0.962168).         Saving model ...\n",
      "Epoch: 559 \tTraining Loss: 0.907843 \tValidation Loss: 0.962150\n",
      "Validation loss decreased (0.962168 --> 0.962150).         Saving model ...\n",
      "Epoch: 560 \tTraining Loss: 0.907780 \tValidation Loss: 0.962133\n",
      "Validation loss decreased (0.962150 --> 0.962133).         Saving model ...\n",
      "Epoch: 561 \tTraining Loss: 0.907718 \tValidation Loss: 0.962116\n",
      "Validation loss decreased (0.962133 --> 0.962116).         Saving model ...\n",
      "Epoch: 562 \tTraining Loss: 0.907656 \tValidation Loss: 0.962098\n",
      "Validation loss decreased (0.962116 --> 0.962098).         Saving model ...\n",
      "Epoch: 563 \tTraining Loss: 0.907593 \tValidation Loss: 0.962081\n",
      "Validation loss decreased (0.962098 --> 0.962081).         Saving model ...\n",
      "Epoch: 564 \tTraining Loss: 0.907531 \tValidation Loss: 0.962064\n",
      "Validation loss decreased (0.962081 --> 0.962064).         Saving model ...\n",
      "Epoch: 565 \tTraining Loss: 0.907469 \tValidation Loss: 0.962047\n",
      "Validation loss decreased (0.962064 --> 0.962047).         Saving model ...\n",
      "Epoch: 566 \tTraining Loss: 0.907407 \tValidation Loss: 0.962030\n",
      "Validation loss decreased (0.962047 --> 0.962030).         Saving model ...\n",
      "Epoch: 567 \tTraining Loss: 0.907346 \tValidation Loss: 0.962013\n",
      "Validation loss decreased (0.962030 --> 0.962013).         Saving model ...\n",
      "Epoch: 568 \tTraining Loss: 0.907284 \tValidation Loss: 0.961996\n",
      "Validation loss decreased (0.962013 --> 0.961996).         Saving model ...\n",
      "Epoch: 569 \tTraining Loss: 0.907223 \tValidation Loss: 0.961979\n",
      "Validation loss decreased (0.961996 --> 0.961979).         Saving model ...\n",
      "Epoch: 570 \tTraining Loss: 0.907161 \tValidation Loss: 0.961962\n",
      "Validation loss decreased (0.961979 --> 0.961962).         Saving model ...\n",
      "Epoch: 571 \tTraining Loss: 0.907100 \tValidation Loss: 0.961945\n",
      "Validation loss decreased (0.961962 --> 0.961945).         Saving model ...\n",
      "Epoch: 572 \tTraining Loss: 0.907039 \tValidation Loss: 0.961929\n",
      "Validation loss decreased (0.961945 --> 0.961929).         Saving model ...\n",
      "Epoch: 573 \tTraining Loss: 0.906978 \tValidation Loss: 0.961912\n",
      "Validation loss decreased (0.961929 --> 0.961912).         Saving model ...\n",
      "Epoch: 574 \tTraining Loss: 0.906917 \tValidation Loss: 0.961896\n",
      "Validation loss decreased (0.961912 --> 0.961896).         Saving model ...\n",
      "Epoch: 575 \tTraining Loss: 0.906856 \tValidation Loss: 0.961879\n",
      "Validation loss decreased (0.961896 --> 0.961879).         Saving model ...\n",
      "Epoch: 576 \tTraining Loss: 0.906795 \tValidation Loss: 0.961863\n",
      "Validation loss decreased (0.961879 --> 0.961863).         Saving model ...\n",
      "Epoch: 577 \tTraining Loss: 0.906735 \tValidation Loss: 0.961846\n",
      "Validation loss decreased (0.961863 --> 0.961846).         Saving model ...\n",
      "Epoch: 578 \tTraining Loss: 0.906674 \tValidation Loss: 0.961830\n",
      "Validation loss decreased (0.961846 --> 0.961830).         Saving model ...\n",
      "Epoch: 579 \tTraining Loss: 0.906614 \tValidation Loss: 0.961814\n",
      "Validation loss decreased (0.961830 --> 0.961814).         Saving model ...\n",
      "Epoch: 580 \tTraining Loss: 0.906554 \tValidation Loss: 0.961798\n",
      "Validation loss decreased (0.961814 --> 0.961798).         Saving model ...\n",
      "Epoch: 581 \tTraining Loss: 0.906494 \tValidation Loss: 0.961781\n",
      "Validation loss decreased (0.961798 --> 0.961781).         Saving model ...\n",
      "Epoch: 582 \tTraining Loss: 0.906434 \tValidation Loss: 0.961765\n",
      "Validation loss decreased (0.961781 --> 0.961765).         Saving model ...\n",
      "Epoch: 583 \tTraining Loss: 0.906374 \tValidation Loss: 0.961749\n",
      "Validation loss decreased (0.961765 --> 0.961749).         Saving model ...\n",
      "Epoch: 584 \tTraining Loss: 0.906314 \tValidation Loss: 0.961733\n",
      "Validation loss decreased (0.961749 --> 0.961733).         Saving model ...\n",
      "Epoch: 585 \tTraining Loss: 0.906255 \tValidation Loss: 0.961718\n",
      "Validation loss decreased (0.961733 --> 0.961718).         Saving model ...\n",
      "Epoch: 586 \tTraining Loss: 0.906195 \tValidation Loss: 0.961702\n",
      "Validation loss decreased (0.961718 --> 0.961702).         Saving model ...\n",
      "Epoch: 587 \tTraining Loss: 0.906136 \tValidation Loss: 0.961686\n",
      "Validation loss decreased (0.961702 --> 0.961686).         Saving model ...\n",
      "Epoch: 588 \tTraining Loss: 0.906076 \tValidation Loss: 0.961670\n",
      "Validation loss decreased (0.961686 --> 0.961670).         Saving model ...\n",
      "Epoch: 589 \tTraining Loss: 0.906017 \tValidation Loss: 0.961655\n",
      "Validation loss decreased (0.961670 --> 0.961655).         Saving model ...\n",
      "Epoch: 590 \tTraining Loss: 0.905958 \tValidation Loss: 0.961639\n",
      "Validation loss decreased (0.961655 --> 0.961639).         Saving model ...\n",
      "Epoch: 591 \tTraining Loss: 0.905899 \tValidation Loss: 0.961623\n",
      "Validation loss decreased (0.961639 --> 0.961623).         Saving model ...\n",
      "Epoch: 592 \tTraining Loss: 0.905840 \tValidation Loss: 0.961608\n",
      "Validation loss decreased (0.961623 --> 0.961608).         Saving model ...\n",
      "Epoch: 593 \tTraining Loss: 0.905782 \tValidation Loss: 0.961593\n",
      "Validation loss decreased (0.961608 --> 0.961593).         Saving model ...\n",
      "Epoch: 594 \tTraining Loss: 0.905723 \tValidation Loss: 0.961577\n",
      "Validation loss decreased (0.961593 --> 0.961577).         Saving model ...\n",
      "Epoch: 595 \tTraining Loss: 0.905664 \tValidation Loss: 0.961562\n",
      "Validation loss decreased (0.961577 --> 0.961562).         Saving model ...\n",
      "Epoch: 596 \tTraining Loss: 0.905606 \tValidation Loss: 0.961547\n",
      "Validation loss decreased (0.961562 --> 0.961547).         Saving model ...\n",
      "Epoch: 597 \tTraining Loss: 0.905548 \tValidation Loss: 0.961531\n",
      "Validation loss decreased (0.961547 --> 0.961531).         Saving model ...\n",
      "Epoch: 598 \tTraining Loss: 0.905489 \tValidation Loss: 0.961516\n",
      "Validation loss decreased (0.961531 --> 0.961516).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 599 \tTraining Loss: 0.905431 \tValidation Loss: 0.961501\n",
      "Validation loss decreased (0.961516 --> 0.961501).         Saving model ...\n",
      "Epoch: 600 \tTraining Loss: 0.905373 \tValidation Loss: 0.961486\n",
      "Validation loss decreased (0.961501 --> 0.961486).         Saving model ...\n",
      "Epoch: 601 \tTraining Loss: 0.905315 \tValidation Loss: 0.961471\n",
      "Validation loss decreased (0.961486 --> 0.961471).         Saving model ...\n",
      "Epoch: 602 \tTraining Loss: 0.905258 \tValidation Loss: 0.961456\n",
      "Validation loss decreased (0.961471 --> 0.961456).         Saving model ...\n",
      "Epoch: 603 \tTraining Loss: 0.905200 \tValidation Loss: 0.961441\n",
      "Validation loss decreased (0.961456 --> 0.961441).         Saving model ...\n",
      "Epoch: 604 \tTraining Loss: 0.905142 \tValidation Loss: 0.961426\n",
      "Validation loss decreased (0.961441 --> 0.961426).         Saving model ...\n",
      "Epoch: 605 \tTraining Loss: 0.905085 \tValidation Loss: 0.961412\n",
      "Validation loss decreased (0.961426 --> 0.961412).         Saving model ...\n",
      "Epoch: 606 \tTraining Loss: 0.905028 \tValidation Loss: 0.961397\n",
      "Validation loss decreased (0.961412 --> 0.961397).         Saving model ...\n",
      "Epoch: 607 \tTraining Loss: 0.904970 \tValidation Loss: 0.961382\n",
      "Validation loss decreased (0.961397 --> 0.961382).         Saving model ...\n",
      "Epoch: 608 \tTraining Loss: 0.904913 \tValidation Loss: 0.961368\n",
      "Validation loss decreased (0.961382 --> 0.961368).         Saving model ...\n",
      "Epoch: 609 \tTraining Loss: 0.904856 \tValidation Loss: 0.961353\n",
      "Validation loss decreased (0.961368 --> 0.961353).         Saving model ...\n",
      "Epoch: 610 \tTraining Loss: 0.904799 \tValidation Loss: 0.961339\n",
      "Validation loss decreased (0.961353 --> 0.961339).         Saving model ...\n",
      "Epoch: 611 \tTraining Loss: 0.904743 \tValidation Loss: 0.961324\n",
      "Validation loss decreased (0.961339 --> 0.961324).         Saving model ...\n",
      "Epoch: 612 \tTraining Loss: 0.904686 \tValidation Loss: 0.961310\n",
      "Validation loss decreased (0.961324 --> 0.961310).         Saving model ...\n",
      "Epoch: 613 \tTraining Loss: 0.904629 \tValidation Loss: 0.961295\n",
      "Validation loss decreased (0.961310 --> 0.961295).         Saving model ...\n",
      "Epoch: 614 \tTraining Loss: 0.904573 \tValidation Loss: 0.961281\n",
      "Validation loss decreased (0.961295 --> 0.961281).         Saving model ...\n",
      "Epoch: 615 \tTraining Loss: 0.904516 \tValidation Loss: 0.961267\n",
      "Validation loss decreased (0.961281 --> 0.961267).         Saving model ...\n",
      "Epoch: 616 \tTraining Loss: 0.904460 \tValidation Loss: 0.961253\n",
      "Validation loss decreased (0.961267 --> 0.961253).         Saving model ...\n",
      "Epoch: 617 \tTraining Loss: 0.904404 \tValidation Loss: 0.961238\n",
      "Validation loss decreased (0.961253 --> 0.961238).         Saving model ...\n",
      "Epoch: 618 \tTraining Loss: 0.904348 \tValidation Loss: 0.961224\n",
      "Validation loss decreased (0.961238 --> 0.961224).         Saving model ...\n",
      "Epoch: 619 \tTraining Loss: 0.904292 \tValidation Loss: 0.961210\n",
      "Validation loss decreased (0.961224 --> 0.961210).         Saving model ...\n",
      "Epoch: 620 \tTraining Loss: 0.904236 \tValidation Loss: 0.961196\n",
      "Validation loss decreased (0.961210 --> 0.961196).         Saving model ...\n",
      "Epoch: 621 \tTraining Loss: 0.904180 \tValidation Loss: 0.961182\n",
      "Validation loss decreased (0.961196 --> 0.961182).         Saving model ...\n",
      "Epoch: 622 \tTraining Loss: 0.904124 \tValidation Loss: 0.961168\n",
      "Validation loss decreased (0.961182 --> 0.961168).         Saving model ...\n",
      "Epoch: 623 \tTraining Loss: 0.904069 \tValidation Loss: 0.961155\n",
      "Validation loss decreased (0.961168 --> 0.961155).         Saving model ...\n",
      "Epoch: 624 \tTraining Loss: 0.904013 \tValidation Loss: 0.961141\n",
      "Validation loss decreased (0.961155 --> 0.961141).         Saving model ...\n",
      "Epoch: 625 \tTraining Loss: 0.903958 \tValidation Loss: 0.961127\n",
      "Validation loss decreased (0.961141 --> 0.961127).         Saving model ...\n",
      "Epoch: 626 \tTraining Loss: 0.903902 \tValidation Loss: 0.961113\n",
      "Validation loss decreased (0.961127 --> 0.961113).         Saving model ...\n",
      "Epoch: 627 \tTraining Loss: 0.903847 \tValidation Loss: 0.961100\n",
      "Validation loss decreased (0.961113 --> 0.961100).         Saving model ...\n",
      "Epoch: 628 \tTraining Loss: 0.903792 \tValidation Loss: 0.961086\n",
      "Validation loss decreased (0.961100 --> 0.961086).         Saving model ...\n",
      "Epoch: 629 \tTraining Loss: 0.903737 \tValidation Loss: 0.961072\n",
      "Validation loss decreased (0.961086 --> 0.961072).         Saving model ...\n",
      "Epoch: 630 \tTraining Loss: 0.903682 \tValidation Loss: 0.961059\n",
      "Validation loss decreased (0.961072 --> 0.961059).         Saving model ...\n",
      "Epoch: 631 \tTraining Loss: 0.903627 \tValidation Loss: 0.961045\n",
      "Validation loss decreased (0.961059 --> 0.961045).         Saving model ...\n",
      "Epoch: 632 \tTraining Loss: 0.903572 \tValidation Loss: 0.961032\n",
      "Validation loss decreased (0.961045 --> 0.961032).         Saving model ...\n",
      "Epoch: 633 \tTraining Loss: 0.903518 \tValidation Loss: 0.961019\n",
      "Validation loss decreased (0.961032 --> 0.961019).         Saving model ...\n",
      "Epoch: 634 \tTraining Loss: 0.903463 \tValidation Loss: 0.961005\n",
      "Validation loss decreased (0.961019 --> 0.961005).         Saving model ...\n",
      "Epoch: 635 \tTraining Loss: 0.903409 \tValidation Loss: 0.960992\n",
      "Validation loss decreased (0.961005 --> 0.960992).         Saving model ...\n",
      "Epoch: 636 \tTraining Loss: 0.903354 \tValidation Loss: 0.960979\n",
      "Validation loss decreased (0.960992 --> 0.960979).         Saving model ...\n",
      "Epoch: 637 \tTraining Loss: 0.903300 \tValidation Loss: 0.960966\n",
      "Validation loss decreased (0.960979 --> 0.960966).         Saving model ...\n",
      "Epoch: 638 \tTraining Loss: 0.903246 \tValidation Loss: 0.960953\n",
      "Validation loss decreased (0.960966 --> 0.960953).         Saving model ...\n",
      "Epoch: 639 \tTraining Loss: 0.903192 \tValidation Loss: 0.960939\n",
      "Validation loss decreased (0.960953 --> 0.960939).         Saving model ...\n",
      "Epoch: 640 \tTraining Loss: 0.903138 \tValidation Loss: 0.960926\n",
      "Validation loss decreased (0.960939 --> 0.960926).         Saving model ...\n",
      "Epoch: 641 \tTraining Loss: 0.903084 \tValidation Loss: 0.960913\n",
      "Validation loss decreased (0.960926 --> 0.960913).         Saving model ...\n",
      "Epoch: 642 \tTraining Loss: 0.903030 \tValidation Loss: 0.960900\n",
      "Validation loss decreased (0.960913 --> 0.960900).         Saving model ...\n",
      "Epoch: 643 \tTraining Loss: 0.902977 \tValidation Loss: 0.960888\n",
      "Validation loss decreased (0.960900 --> 0.960888).         Saving model ...\n",
      "Epoch: 644 \tTraining Loss: 0.902923 \tValidation Loss: 0.960875\n",
      "Validation loss decreased (0.960888 --> 0.960875).         Saving model ...\n",
      "Epoch: 645 \tTraining Loss: 0.902869 \tValidation Loss: 0.960862\n",
      "Validation loss decreased (0.960875 --> 0.960862).         Saving model ...\n",
      "Epoch: 646 \tTraining Loss: 0.902816 \tValidation Loss: 0.960849\n",
      "Validation loss decreased (0.960862 --> 0.960849).         Saving model ...\n",
      "Epoch: 647 \tTraining Loss: 0.902763 \tValidation Loss: 0.960836\n",
      "Validation loss decreased (0.960849 --> 0.960836).         Saving model ...\n",
      "Epoch: 648 \tTraining Loss: 0.902709 \tValidation Loss: 0.960824\n",
      "Validation loss decreased (0.960836 --> 0.960824).         Saving model ...\n",
      "Epoch: 649 \tTraining Loss: 0.902656 \tValidation Loss: 0.960811\n",
      "Validation loss decreased (0.960824 --> 0.960811).         Saving model ...\n",
      "Epoch: 650 \tTraining Loss: 0.902603 \tValidation Loss: 0.960798\n",
      "Validation loss decreased (0.960811 --> 0.960798).         Saving model ...\n",
      "Epoch: 651 \tTraining Loss: 0.902550 \tValidation Loss: 0.960786\n",
      "Validation loss decreased (0.960798 --> 0.960786).         Saving model ...\n",
      "Epoch: 652 \tTraining Loss: 0.902497 \tValidation Loss: 0.960773\n",
      "Validation loss decreased (0.960786 --> 0.960773).         Saving model ...\n",
      "Epoch: 653 \tTraining Loss: 0.902445 \tValidation Loss: 0.960761\n",
      "Validation loss decreased (0.960773 --> 0.960761).         Saving model ...\n",
      "Epoch: 654 \tTraining Loss: 0.902392 \tValidation Loss: 0.960748\n",
      "Validation loss decreased (0.960761 --> 0.960748).         Saving model ...\n",
      "Epoch: 655 \tTraining Loss: 0.902339 \tValidation Loss: 0.960736\n",
      "Validation loss decreased (0.960748 --> 0.960736).         Saving model ...\n",
      "Epoch: 656 \tTraining Loss: 0.902287 \tValidation Loss: 0.960724\n",
      "Validation loss decreased (0.960736 --> 0.960724).         Saving model ...\n",
      "Epoch: 657 \tTraining Loss: 0.902234 \tValidation Loss: 0.960711\n",
      "Validation loss decreased (0.960724 --> 0.960711).         Saving model ...\n",
      "Epoch: 658 \tTraining Loss: 0.902182 \tValidation Loss: 0.960699\n",
      "Validation loss decreased (0.960711 --> 0.960699).         Saving model ...\n",
      "Epoch: 659 \tTraining Loss: 0.902130 \tValidation Loss: 0.960687\n",
      "Validation loss decreased (0.960699 --> 0.960687).         Saving model ...\n",
      "Epoch: 660 \tTraining Loss: 0.902077 \tValidation Loss: 0.960675\n",
      "Validation loss decreased (0.960687 --> 0.960675).         Saving model ...\n",
      "Epoch: 661 \tTraining Loss: 0.902025 \tValidation Loss: 0.960663\n",
      "Validation loss decreased (0.960675 --> 0.960663).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 662 \tTraining Loss: 0.901973 \tValidation Loss: 0.960651\n",
      "Validation loss decreased (0.960663 --> 0.960651).         Saving model ...\n",
      "Epoch: 663 \tTraining Loss: 0.901922 \tValidation Loss: 0.960639\n",
      "Validation loss decreased (0.960651 --> 0.960639).         Saving model ...\n",
      "Epoch: 664 \tTraining Loss: 0.901870 \tValidation Loss: 0.960627\n",
      "Validation loss decreased (0.960639 --> 0.960627).         Saving model ...\n",
      "Epoch: 665 \tTraining Loss: 0.901818 \tValidation Loss: 0.960615\n",
      "Validation loss decreased (0.960627 --> 0.960615).         Saving model ...\n",
      "Epoch: 666 \tTraining Loss: 0.901766 \tValidation Loss: 0.960603\n",
      "Validation loss decreased (0.960615 --> 0.960603).         Saving model ...\n",
      "Epoch: 667 \tTraining Loss: 0.901715 \tValidation Loss: 0.960591\n",
      "Validation loss decreased (0.960603 --> 0.960591).         Saving model ...\n",
      "Epoch: 668 \tTraining Loss: 0.901663 \tValidation Loss: 0.960579\n",
      "Validation loss decreased (0.960591 --> 0.960579).         Saving model ...\n",
      "Epoch: 669 \tTraining Loss: 0.901612 \tValidation Loss: 0.960567\n",
      "Validation loss decreased (0.960579 --> 0.960567).         Saving model ...\n",
      "Epoch: 670 \tTraining Loss: 0.901561 \tValidation Loss: 0.960555\n",
      "Validation loss decreased (0.960567 --> 0.960555).         Saving model ...\n",
      "Epoch: 671 \tTraining Loss: 0.901509 \tValidation Loss: 0.960544\n",
      "Validation loss decreased (0.960555 --> 0.960544).         Saving model ...\n",
      "Epoch: 672 \tTraining Loss: 0.901458 \tValidation Loss: 0.960532\n",
      "Validation loss decreased (0.960544 --> 0.960532).         Saving model ...\n",
      "Epoch: 673 \tTraining Loss: 0.901407 \tValidation Loss: 0.960520\n",
      "Validation loss decreased (0.960532 --> 0.960520).         Saving model ...\n",
      "Epoch: 674 \tTraining Loss: 0.901356 \tValidation Loss: 0.960509\n",
      "Validation loss decreased (0.960520 --> 0.960509).         Saving model ...\n",
      "Epoch: 675 \tTraining Loss: 0.901305 \tValidation Loss: 0.960497\n",
      "Validation loss decreased (0.960509 --> 0.960497).         Saving model ...\n",
      "Epoch: 676 \tTraining Loss: 0.901254 \tValidation Loss: 0.960486\n",
      "Validation loss decreased (0.960497 --> 0.960486).         Saving model ...\n",
      "Epoch: 677 \tTraining Loss: 0.901204 \tValidation Loss: 0.960474\n",
      "Validation loss decreased (0.960486 --> 0.960474).         Saving model ...\n",
      "Epoch: 678 \tTraining Loss: 0.901153 \tValidation Loss: 0.960463\n",
      "Validation loss decreased (0.960474 --> 0.960463).         Saving model ...\n",
      "Epoch: 679 \tTraining Loss: 0.901103 \tValidation Loss: 0.960452\n",
      "Validation loss decreased (0.960463 --> 0.960452).         Saving model ...\n",
      "Epoch: 680 \tTraining Loss: 0.901052 \tValidation Loss: 0.960440\n",
      "Validation loss decreased (0.960452 --> 0.960440).         Saving model ...\n",
      "Epoch: 681 \tTraining Loss: 0.901002 \tValidation Loss: 0.960429\n",
      "Validation loss decreased (0.960440 --> 0.960429).         Saving model ...\n",
      "Epoch: 682 \tTraining Loss: 0.900951 \tValidation Loss: 0.960418\n",
      "Validation loss decreased (0.960429 --> 0.960418).         Saving model ...\n",
      "Epoch: 683 \tTraining Loss: 0.900901 \tValidation Loss: 0.960406\n",
      "Validation loss decreased (0.960418 --> 0.960406).         Saving model ...\n",
      "Epoch: 684 \tTraining Loss: 0.900851 \tValidation Loss: 0.960395\n",
      "Validation loss decreased (0.960406 --> 0.960395).         Saving model ...\n",
      "Epoch: 685 \tTraining Loss: 0.900801 \tValidation Loss: 0.960384\n",
      "Validation loss decreased (0.960395 --> 0.960384).         Saving model ...\n",
      "Epoch: 686 \tTraining Loss: 0.900751 \tValidation Loss: 0.960373\n",
      "Validation loss decreased (0.960384 --> 0.960373).         Saving model ...\n",
      "Epoch: 687 \tTraining Loss: 0.900701 \tValidation Loss: 0.960362\n",
      "Validation loss decreased (0.960373 --> 0.960362).         Saving model ...\n",
      "Epoch: 688 \tTraining Loss: 0.900651 \tValidation Loss: 0.960351\n",
      "Validation loss decreased (0.960362 --> 0.960351).         Saving model ...\n",
      "Epoch: 689 \tTraining Loss: 0.900602 \tValidation Loss: 0.960340\n",
      "Validation loss decreased (0.960351 --> 0.960340).         Saving model ...\n",
      "Epoch: 690 \tTraining Loss: 0.900552 \tValidation Loss: 0.960329\n",
      "Validation loss decreased (0.960340 --> 0.960329).         Saving model ...\n",
      "Epoch: 691 \tTraining Loss: 0.900502 \tValidation Loss: 0.960318\n",
      "Validation loss decreased (0.960329 --> 0.960318).         Saving model ...\n",
      "Epoch: 692 \tTraining Loss: 0.900453 \tValidation Loss: 0.960307\n",
      "Validation loss decreased (0.960318 --> 0.960307).         Saving model ...\n",
      "Epoch: 693 \tTraining Loss: 0.900403 \tValidation Loss: 0.960296\n",
      "Validation loss decreased (0.960307 --> 0.960296).         Saving model ...\n",
      "Epoch: 694 \tTraining Loss: 0.900354 \tValidation Loss: 0.960285\n",
      "Validation loss decreased (0.960296 --> 0.960285).         Saving model ...\n",
      "Epoch: 695 \tTraining Loss: 0.900305 \tValidation Loss: 0.960275\n",
      "Validation loss decreased (0.960285 --> 0.960275).         Saving model ...\n",
      "Epoch: 696 \tTraining Loss: 0.900256 \tValidation Loss: 0.960264\n",
      "Validation loss decreased (0.960275 --> 0.960264).         Saving model ...\n",
      "Epoch: 697 \tTraining Loss: 0.900206 \tValidation Loss: 0.960253\n",
      "Validation loss decreased (0.960264 --> 0.960253).         Saving model ...\n",
      "Epoch: 698 \tTraining Loss: 0.900157 \tValidation Loss: 0.960243\n",
      "Validation loss decreased (0.960253 --> 0.960243).         Saving model ...\n",
      "Epoch: 699 \tTraining Loss: 0.900108 \tValidation Loss: 0.960232\n",
      "Validation loss decreased (0.960243 --> 0.960232).         Saving model ...\n",
      "Epoch: 700 \tTraining Loss: 0.900060 \tValidation Loss: 0.960221\n",
      "Validation loss decreased (0.960232 --> 0.960221).         Saving model ...\n",
      "Epoch: 701 \tTraining Loss: 0.900011 \tValidation Loss: 0.960211\n",
      "Validation loss decreased (0.960221 --> 0.960211).         Saving model ...\n",
      "Epoch: 702 \tTraining Loss: 0.899962 \tValidation Loss: 0.960200\n",
      "Validation loss decreased (0.960211 --> 0.960200).         Saving model ...\n",
      "Epoch: 703 \tTraining Loss: 0.899913 \tValidation Loss: 0.960190\n",
      "Validation loss decreased (0.960200 --> 0.960190).         Saving model ...\n",
      "Epoch: 704 \tTraining Loss: 0.899865 \tValidation Loss: 0.960179\n",
      "Validation loss decreased (0.960190 --> 0.960179).         Saving model ...\n",
      "Epoch: 705 \tTraining Loss: 0.899816 \tValidation Loss: 0.960169\n",
      "Validation loss decreased (0.960179 --> 0.960169).         Saving model ...\n",
      "Epoch: 706 \tTraining Loss: 0.899768 \tValidation Loss: 0.960159\n",
      "Validation loss decreased (0.960169 --> 0.960159).         Saving model ...\n",
      "Epoch: 707 \tTraining Loss: 0.899720 \tValidation Loss: 0.960148\n",
      "Validation loss decreased (0.960159 --> 0.960148).         Saving model ...\n",
      "Epoch: 708 \tTraining Loss: 0.899671 \tValidation Loss: 0.960138\n",
      "Validation loss decreased (0.960148 --> 0.960138).         Saving model ...\n",
      "Epoch: 709 \tTraining Loss: 0.899623 \tValidation Loss: 0.960128\n",
      "Validation loss decreased (0.960138 --> 0.960128).         Saving model ...\n",
      "Epoch: 710 \tTraining Loss: 0.899575 \tValidation Loss: 0.960117\n",
      "Validation loss decreased (0.960128 --> 0.960117).         Saving model ...\n",
      "Epoch: 711 \tTraining Loss: 0.899527 \tValidation Loss: 0.960107\n",
      "Validation loss decreased (0.960117 --> 0.960107).         Saving model ...\n",
      "Epoch: 712 \tTraining Loss: 0.899479 \tValidation Loss: 0.960097\n",
      "Validation loss decreased (0.960107 --> 0.960097).         Saving model ...\n",
      "Epoch: 713 \tTraining Loss: 0.899431 \tValidation Loss: 0.960087\n",
      "Validation loss decreased (0.960097 --> 0.960087).         Saving model ...\n",
      "Epoch: 714 \tTraining Loss: 0.899383 \tValidation Loss: 0.960077\n",
      "Validation loss decreased (0.960087 --> 0.960077).         Saving model ...\n",
      "Epoch: 715 \tTraining Loss: 0.899336 \tValidation Loss: 0.960067\n",
      "Validation loss decreased (0.960077 --> 0.960067).         Saving model ...\n",
      "Epoch: 716 \tTraining Loss: 0.899288 \tValidation Loss: 0.960057\n",
      "Validation loss decreased (0.960067 --> 0.960057).         Saving model ...\n",
      "Epoch: 717 \tTraining Loss: 0.899240 \tValidation Loss: 0.960047\n",
      "Validation loss decreased (0.960057 --> 0.960047).         Saving model ...\n",
      "Epoch: 718 \tTraining Loss: 0.899193 \tValidation Loss: 0.960037\n",
      "Validation loss decreased (0.960047 --> 0.960037).         Saving model ...\n",
      "Epoch: 719 \tTraining Loss: 0.899145 \tValidation Loss: 0.960027\n",
      "Validation loss decreased (0.960037 --> 0.960027).         Saving model ...\n",
      "Epoch: 720 \tTraining Loss: 0.899098 \tValidation Loss: 0.960017\n",
      "Validation loss decreased (0.960027 --> 0.960017).         Saving model ...\n",
      "Epoch: 721 \tTraining Loss: 0.899051 \tValidation Loss: 0.960007\n",
      "Validation loss decreased (0.960017 --> 0.960007).         Saving model ...\n",
      "Epoch: 722 \tTraining Loss: 0.899003 \tValidation Loss: 0.959997\n",
      "Validation loss decreased (0.960007 --> 0.959997).         Saving model ...\n",
      "Epoch: 723 \tTraining Loss: 0.898956 \tValidation Loss: 0.959987\n",
      "Validation loss decreased (0.959997 --> 0.959987).         Saving model ...\n",
      "Epoch: 724 \tTraining Loss: 0.898909 \tValidation Loss: 0.959978\n",
      "Validation loss decreased (0.959987 --> 0.959978).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 725 \tTraining Loss: 0.898862 \tValidation Loss: 0.959968\n",
      "Validation loss decreased (0.959978 --> 0.959968).         Saving model ...\n",
      "Epoch: 726 \tTraining Loss: 0.898815 \tValidation Loss: 0.959958\n",
      "Validation loss decreased (0.959968 --> 0.959958).         Saving model ...\n",
      "Epoch: 727 \tTraining Loss: 0.898768 \tValidation Loss: 0.959949\n",
      "Validation loss decreased (0.959958 --> 0.959949).         Saving model ...\n",
      "Epoch: 728 \tTraining Loss: 0.898722 \tValidation Loss: 0.959939\n",
      "Validation loss decreased (0.959949 --> 0.959939).         Saving model ...\n",
      "Epoch: 729 \tTraining Loss: 0.898675 \tValidation Loss: 0.959929\n",
      "Validation loss decreased (0.959939 --> 0.959929).         Saving model ...\n",
      "Epoch: 730 \tTraining Loss: 0.898628 \tValidation Loss: 0.959920\n",
      "Validation loss decreased (0.959929 --> 0.959920).         Saving model ...\n",
      "Epoch: 731 \tTraining Loss: 0.898582 \tValidation Loss: 0.959910\n",
      "Validation loss decreased (0.959920 --> 0.959910).         Saving model ...\n",
      "Epoch: 732 \tTraining Loss: 0.898535 \tValidation Loss: 0.959901\n",
      "Validation loss decreased (0.959910 --> 0.959901).         Saving model ...\n",
      "Epoch: 733 \tTraining Loss: 0.898489 \tValidation Loss: 0.959891\n",
      "Validation loss decreased (0.959901 --> 0.959891).         Saving model ...\n",
      "Epoch: 734 \tTraining Loss: 0.898442 \tValidation Loss: 0.959882\n",
      "Validation loss decreased (0.959891 --> 0.959882).         Saving model ...\n",
      "Epoch: 735 \tTraining Loss: 0.898396 \tValidation Loss: 0.959873\n",
      "Validation loss decreased (0.959882 --> 0.959873).         Saving model ...\n",
      "Epoch: 736 \tTraining Loss: 0.898350 \tValidation Loss: 0.959863\n",
      "Validation loss decreased (0.959873 --> 0.959863).         Saving model ...\n",
      "Epoch: 737 \tTraining Loss: 0.898304 \tValidation Loss: 0.959854\n",
      "Validation loss decreased (0.959863 --> 0.959854).         Saving model ...\n",
      "Epoch: 738 \tTraining Loss: 0.898258 \tValidation Loss: 0.959845\n",
      "Validation loss decreased (0.959854 --> 0.959845).         Saving model ...\n",
      "Epoch: 739 \tTraining Loss: 0.898212 \tValidation Loss: 0.959835\n",
      "Validation loss decreased (0.959845 --> 0.959835).         Saving model ...\n",
      "Epoch: 740 \tTraining Loss: 0.898166 \tValidation Loss: 0.959826\n",
      "Validation loss decreased (0.959835 --> 0.959826).         Saving model ...\n",
      "Epoch: 741 \tTraining Loss: 0.898120 \tValidation Loss: 0.959817\n",
      "Validation loss decreased (0.959826 --> 0.959817).         Saving model ...\n",
      "Epoch: 742 \tTraining Loss: 0.898074 \tValidation Loss: 0.959808\n",
      "Validation loss decreased (0.959817 --> 0.959808).         Saving model ...\n",
      "Epoch: 743 \tTraining Loss: 0.898028 \tValidation Loss: 0.959799\n",
      "Validation loss decreased (0.959808 --> 0.959799).         Saving model ...\n",
      "Epoch: 744 \tTraining Loss: 0.897982 \tValidation Loss: 0.959790\n",
      "Validation loss decreased (0.959799 --> 0.959790).         Saving model ...\n",
      "Epoch: 745 \tTraining Loss: 0.897937 \tValidation Loss: 0.959780\n",
      "Validation loss decreased (0.959790 --> 0.959780).         Saving model ...\n",
      "Epoch: 746 \tTraining Loss: 0.897891 \tValidation Loss: 0.959771\n",
      "Validation loss decreased (0.959780 --> 0.959771).         Saving model ...\n",
      "Epoch: 747 \tTraining Loss: 0.897846 \tValidation Loss: 0.959762\n",
      "Validation loss decreased (0.959771 --> 0.959762).         Saving model ...\n",
      "Epoch: 748 \tTraining Loss: 0.897800 \tValidation Loss: 0.959753\n",
      "Validation loss decreased (0.959762 --> 0.959753).         Saving model ...\n",
      "Epoch: 749 \tTraining Loss: 0.897755 \tValidation Loss: 0.959744\n",
      "Validation loss decreased (0.959753 --> 0.959744).         Saving model ...\n",
      "Epoch: 750 \tTraining Loss: 0.897710 \tValidation Loss: 0.959736\n",
      "Validation loss decreased (0.959744 --> 0.959736).         Saving model ...\n",
      "Epoch: 751 \tTraining Loss: 0.897665 \tValidation Loss: 0.959727\n",
      "Validation loss decreased (0.959736 --> 0.959727).         Saving model ...\n",
      "Epoch: 752 \tTraining Loss: 0.897619 \tValidation Loss: 0.959718\n",
      "Validation loss decreased (0.959727 --> 0.959718).         Saving model ...\n",
      "Epoch: 753 \tTraining Loss: 0.897574 \tValidation Loss: 0.959709\n",
      "Validation loss decreased (0.959718 --> 0.959709).         Saving model ...\n",
      "Epoch: 754 \tTraining Loss: 0.897529 \tValidation Loss: 0.959700\n",
      "Validation loss decreased (0.959709 --> 0.959700).         Saving model ...\n",
      "Epoch: 755 \tTraining Loss: 0.897484 \tValidation Loss: 0.959691\n",
      "Validation loss decreased (0.959700 --> 0.959691).         Saving model ...\n",
      "Epoch: 756 \tTraining Loss: 0.897440 \tValidation Loss: 0.959683\n",
      "Validation loss decreased (0.959691 --> 0.959683).         Saving model ...\n",
      "Epoch: 757 \tTraining Loss: 0.897395 \tValidation Loss: 0.959674\n",
      "Validation loss decreased (0.959683 --> 0.959674).         Saving model ...\n",
      "Epoch: 758 \tTraining Loss: 0.897350 \tValidation Loss: 0.959665\n",
      "Validation loss decreased (0.959674 --> 0.959665).         Saving model ...\n",
      "Epoch: 759 \tTraining Loss: 0.897305 \tValidation Loss: 0.959657\n",
      "Validation loss decreased (0.959665 --> 0.959657).         Saving model ...\n",
      "Epoch: 760 \tTraining Loss: 0.897261 \tValidation Loss: 0.959648\n",
      "Validation loss decreased (0.959657 --> 0.959648).         Saving model ...\n",
      "Epoch: 761 \tTraining Loss: 0.897216 \tValidation Loss: 0.959639\n",
      "Validation loss decreased (0.959648 --> 0.959639).         Saving model ...\n",
      "Epoch: 762 \tTraining Loss: 0.897172 \tValidation Loss: 0.959631\n",
      "Validation loss decreased (0.959639 --> 0.959631).         Saving model ...\n",
      "Epoch: 763 \tTraining Loss: 0.897127 \tValidation Loss: 0.959622\n",
      "Validation loss decreased (0.959631 --> 0.959622).         Saving model ...\n",
      "Epoch: 764 \tTraining Loss: 0.897083 \tValidation Loss: 0.959614\n",
      "Validation loss decreased (0.959622 --> 0.959614).         Saving model ...\n",
      "Epoch: 765 \tTraining Loss: 0.897039 \tValidation Loss: 0.959605\n",
      "Validation loss decreased (0.959614 --> 0.959605).         Saving model ...\n",
      "Epoch: 766 \tTraining Loss: 0.896994 \tValidation Loss: 0.959597\n",
      "Validation loss decreased (0.959605 --> 0.959597).         Saving model ...\n",
      "Epoch: 767 \tTraining Loss: 0.896950 \tValidation Loss: 0.959589\n",
      "Validation loss decreased (0.959597 --> 0.959589).         Saving model ...\n",
      "Epoch: 768 \tTraining Loss: 0.896906 \tValidation Loss: 0.959580\n",
      "Validation loss decreased (0.959589 --> 0.959580).         Saving model ...\n",
      "Epoch: 769 \tTraining Loss: 0.896862 \tValidation Loss: 0.959572\n",
      "Validation loss decreased (0.959580 --> 0.959572).         Saving model ...\n",
      "Epoch: 770 \tTraining Loss: 0.896818 \tValidation Loss: 0.959563\n",
      "Validation loss decreased (0.959572 --> 0.959563).         Saving model ...\n",
      "Epoch: 771 \tTraining Loss: 0.896774 \tValidation Loss: 0.959555\n",
      "Validation loss decreased (0.959563 --> 0.959555).         Saving model ...\n",
      "Epoch: 772 \tTraining Loss: 0.896730 \tValidation Loss: 0.959547\n",
      "Validation loss decreased (0.959555 --> 0.959547).         Saving model ...\n",
      "Epoch: 773 \tTraining Loss: 0.896687 \tValidation Loss: 0.959539\n",
      "Validation loss decreased (0.959547 --> 0.959539).         Saving model ...\n",
      "Epoch: 774 \tTraining Loss: 0.896643 \tValidation Loss: 0.959530\n",
      "Validation loss decreased (0.959539 --> 0.959530).         Saving model ...\n",
      "Epoch: 775 \tTraining Loss: 0.896599 \tValidation Loss: 0.959522\n",
      "Validation loss decreased (0.959530 --> 0.959522).         Saving model ...\n",
      "Epoch: 776 \tTraining Loss: 0.896556 \tValidation Loss: 0.959514\n",
      "Validation loss decreased (0.959522 --> 0.959514).         Saving model ...\n",
      "Epoch: 777 \tTraining Loss: 0.896512 \tValidation Loss: 0.959506\n",
      "Validation loss decreased (0.959514 --> 0.959506).         Saving model ...\n",
      "Epoch: 778 \tTraining Loss: 0.896469 \tValidation Loss: 0.959498\n",
      "Validation loss decreased (0.959506 --> 0.959498).         Saving model ...\n",
      "Epoch: 779 \tTraining Loss: 0.896425 \tValidation Loss: 0.959490\n",
      "Validation loss decreased (0.959498 --> 0.959490).         Saving model ...\n",
      "Epoch: 780 \tTraining Loss: 0.896382 \tValidation Loss: 0.959482\n",
      "Validation loss decreased (0.959490 --> 0.959482).         Saving model ...\n",
      "Epoch: 781 \tTraining Loss: 0.896339 \tValidation Loss: 0.959474\n",
      "Validation loss decreased (0.959482 --> 0.959474).         Saving model ...\n",
      "Epoch: 782 \tTraining Loss: 0.896295 \tValidation Loss: 0.959466\n",
      "Validation loss decreased (0.959474 --> 0.959466).         Saving model ...\n",
      "Epoch: 783 \tTraining Loss: 0.896252 \tValidation Loss: 0.959458\n",
      "Validation loss decreased (0.959466 --> 0.959458).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 784 \tTraining Loss: 0.896209 \tValidation Loss: 0.959450\n",
      "Validation loss decreased (0.959458 --> 0.959450).         Saving model ...\n",
      "Epoch: 785 \tTraining Loss: 0.896166 \tValidation Loss: 0.959442\n",
      "Validation loss decreased (0.959450 --> 0.959442).         Saving model ...\n",
      "Epoch: 786 \tTraining Loss: 0.896123 \tValidation Loss: 0.959434\n",
      "Validation loss decreased (0.959442 --> 0.959434).         Saving model ...\n",
      "Epoch: 787 \tTraining Loss: 0.896080 \tValidation Loss: 0.959426\n",
      "Validation loss decreased (0.959434 --> 0.959426).         Saving model ...\n",
      "Epoch: 788 \tTraining Loss: 0.896037 \tValidation Loss: 0.959418\n",
      "Validation loss decreased (0.959426 --> 0.959418).         Saving model ...\n",
      "Epoch: 789 \tTraining Loss: 0.895994 \tValidation Loss: 0.959410\n",
      "Validation loss decreased (0.959418 --> 0.959410).         Saving model ...\n",
      "Epoch: 790 \tTraining Loss: 0.895952 \tValidation Loss: 0.959403\n",
      "Validation loss decreased (0.959410 --> 0.959403).         Saving model ...\n",
      "Epoch: 791 \tTraining Loss: 0.895909 \tValidation Loss: 0.959395\n",
      "Validation loss decreased (0.959403 --> 0.959395).         Saving model ...\n",
      "Epoch: 792 \tTraining Loss: 0.895866 \tValidation Loss: 0.959387\n",
      "Validation loss decreased (0.959395 --> 0.959387).         Saving model ...\n",
      "Epoch: 793 \tTraining Loss: 0.895824 \tValidation Loss: 0.959379\n",
      "Validation loss decreased (0.959387 --> 0.959379).         Saving model ...\n",
      "Epoch: 794 \tTraining Loss: 0.895781 \tValidation Loss: 0.959372\n",
      "Validation loss decreased (0.959379 --> 0.959372).         Saving model ...\n",
      "Epoch: 795 \tTraining Loss: 0.895739 \tValidation Loss: 0.959364\n",
      "Validation loss decreased (0.959372 --> 0.959364).         Saving model ...\n",
      "Epoch: 796 \tTraining Loss: 0.895697 \tValidation Loss: 0.959357\n",
      "Validation loss decreased (0.959364 --> 0.959357).         Saving model ...\n",
      "Epoch: 797 \tTraining Loss: 0.895654 \tValidation Loss: 0.959349\n",
      "Validation loss decreased (0.959357 --> 0.959349).         Saving model ...\n",
      "Epoch: 798 \tTraining Loss: 0.895612 \tValidation Loss: 0.959341\n",
      "Validation loss decreased (0.959349 --> 0.959341).         Saving model ...\n",
      "Epoch: 799 \tTraining Loss: 0.895570 \tValidation Loss: 0.959334\n",
      "Validation loss decreased (0.959341 --> 0.959334).         Saving model ...\n",
      "Epoch: 800 \tTraining Loss: 0.895528 \tValidation Loss: 0.959326\n",
      "Validation loss decreased (0.959334 --> 0.959326).         Saving model ...\n",
      "Epoch: 801 \tTraining Loss: 0.895486 \tValidation Loss: 0.959319\n",
      "Validation loss decreased (0.959326 --> 0.959319).         Saving model ...\n",
      "Epoch: 802 \tTraining Loss: 0.895444 \tValidation Loss: 0.959311\n",
      "Validation loss decreased (0.959319 --> 0.959311).         Saving model ...\n",
      "Epoch: 803 \tTraining Loss: 0.895402 \tValidation Loss: 0.959304\n",
      "Validation loss decreased (0.959311 --> 0.959304).         Saving model ...\n",
      "Epoch: 804 \tTraining Loss: 0.895360 \tValidation Loss: 0.959297\n",
      "Validation loss decreased (0.959304 --> 0.959297).         Saving model ...\n",
      "Epoch: 805 \tTraining Loss: 0.895318 \tValidation Loss: 0.959289\n",
      "Validation loss decreased (0.959297 --> 0.959289).         Saving model ...\n",
      "Epoch: 806 \tTraining Loss: 0.895276 \tValidation Loss: 0.959282\n",
      "Validation loss decreased (0.959289 --> 0.959282).         Saving model ...\n",
      "Epoch: 807 \tTraining Loss: 0.895234 \tValidation Loss: 0.959275\n",
      "Validation loss decreased (0.959282 --> 0.959275).         Saving model ...\n",
      "Epoch: 808 \tTraining Loss: 0.895193 \tValidation Loss: 0.959267\n",
      "Validation loss decreased (0.959275 --> 0.959267).         Saving model ...\n",
      "Epoch: 809 \tTraining Loss: 0.895151 \tValidation Loss: 0.959260\n",
      "Validation loss decreased (0.959267 --> 0.959260).         Saving model ...\n",
      "Epoch: 810 \tTraining Loss: 0.895109 \tValidation Loss: 0.959253\n",
      "Validation loss decreased (0.959260 --> 0.959253).         Saving model ...\n",
      "Epoch: 811 \tTraining Loss: 0.895068 \tValidation Loss: 0.959245\n",
      "Validation loss decreased (0.959253 --> 0.959245).         Saving model ...\n",
      "Epoch: 812 \tTraining Loss: 0.895026 \tValidation Loss: 0.959238\n",
      "Validation loss decreased (0.959245 --> 0.959238).         Saving model ...\n",
      "Epoch: 813 \tTraining Loss: 0.894985 \tValidation Loss: 0.959231\n",
      "Validation loss decreased (0.959238 --> 0.959231).         Saving model ...\n",
      "Epoch: 814 \tTraining Loss: 0.894944 \tValidation Loss: 0.959224\n",
      "Validation loss decreased (0.959231 --> 0.959224).         Saving model ...\n",
      "Epoch: 815 \tTraining Loss: 0.894902 \tValidation Loss: 0.959217\n",
      "Validation loss decreased (0.959224 --> 0.959217).         Saving model ...\n",
      "Epoch: 816 \tTraining Loss: 0.894861 \tValidation Loss: 0.959210\n",
      "Validation loss decreased (0.959217 --> 0.959210).         Saving model ...\n",
      "Epoch: 817 \tTraining Loss: 0.894820 \tValidation Loss: 0.959203\n",
      "Validation loss decreased (0.959210 --> 0.959203).         Saving model ...\n",
      "Epoch: 818 \tTraining Loss: 0.894779 \tValidation Loss: 0.959196\n",
      "Validation loss decreased (0.959203 --> 0.959196).         Saving model ...\n",
      "Epoch: 819 \tTraining Loss: 0.894738 \tValidation Loss: 0.959189\n",
      "Validation loss decreased (0.959196 --> 0.959189).         Saving model ...\n",
      "Epoch: 820 \tTraining Loss: 0.894697 \tValidation Loss: 0.959181\n",
      "Validation loss decreased (0.959189 --> 0.959181).         Saving model ...\n",
      "Epoch: 821 \tTraining Loss: 0.894656 \tValidation Loss: 0.959175\n",
      "Validation loss decreased (0.959181 --> 0.959175).         Saving model ...\n",
      "Epoch: 822 \tTraining Loss: 0.894615 \tValidation Loss: 0.959168\n",
      "Validation loss decreased (0.959175 --> 0.959168).         Saving model ...\n",
      "Epoch: 823 \tTraining Loss: 0.894574 \tValidation Loss: 0.959161\n",
      "Validation loss decreased (0.959168 --> 0.959161).         Saving model ...\n",
      "Epoch: 824 \tTraining Loss: 0.894533 \tValidation Loss: 0.959154\n",
      "Validation loss decreased (0.959161 --> 0.959154).         Saving model ...\n",
      "Epoch: 825 \tTraining Loss: 0.894493 \tValidation Loss: 0.959147\n",
      "Validation loss decreased (0.959154 --> 0.959147).         Saving model ...\n",
      "Epoch: 826 \tTraining Loss: 0.894452 \tValidation Loss: 0.959140\n",
      "Validation loss decreased (0.959147 --> 0.959140).         Saving model ...\n",
      "Epoch: 827 \tTraining Loss: 0.894411 \tValidation Loss: 0.959133\n",
      "Validation loss decreased (0.959140 --> 0.959133).         Saving model ...\n",
      "Epoch: 828 \tTraining Loss: 0.894371 \tValidation Loss: 0.959126\n",
      "Validation loss decreased (0.959133 --> 0.959126).         Saving model ...\n",
      "Epoch: 829 \tTraining Loss: 0.894330 \tValidation Loss: 0.959119\n",
      "Validation loss decreased (0.959126 --> 0.959119).         Saving model ...\n",
      "Epoch: 830 \tTraining Loss: 0.894290 \tValidation Loss: 0.959113\n",
      "Validation loss decreased (0.959119 --> 0.959113).         Saving model ...\n",
      "Epoch: 831 \tTraining Loss: 0.894250 \tValidation Loss: 0.959106\n",
      "Validation loss decreased (0.959113 --> 0.959106).         Saving model ...\n",
      "Epoch: 832 \tTraining Loss: 0.894209 \tValidation Loss: 0.959099\n",
      "Validation loss decreased (0.959106 --> 0.959099).         Saving model ...\n",
      "Epoch: 833 \tTraining Loss: 0.894169 \tValidation Loss: 0.959093\n",
      "Validation loss decreased (0.959099 --> 0.959093).         Saving model ...\n",
      "Epoch: 834 \tTraining Loss: 0.894129 \tValidation Loss: 0.959086\n",
      "Validation loss decreased (0.959093 --> 0.959086).         Saving model ...\n",
      "Epoch: 835 \tTraining Loss: 0.894088 \tValidation Loss: 0.959079\n",
      "Validation loss decreased (0.959086 --> 0.959079).         Saving model ...\n",
      "Epoch: 836 \tTraining Loss: 0.894048 \tValidation Loss: 0.959073\n",
      "Validation loss decreased (0.959079 --> 0.959073).         Saving model ...\n",
      "Epoch: 837 \tTraining Loss: 0.894008 \tValidation Loss: 0.959066\n",
      "Validation loss decreased (0.959073 --> 0.959066).         Saving model ...\n",
      "Epoch: 838 \tTraining Loss: 0.893968 \tValidation Loss: 0.959059\n",
      "Validation loss decreased (0.959066 --> 0.959059).         Saving model ...\n",
      "Epoch: 839 \tTraining Loss: 0.893928 \tValidation Loss: 0.959053\n",
      "Validation loss decreased (0.959059 --> 0.959053).         Saving model ...\n",
      "Epoch: 840 \tTraining Loss: 0.893888 \tValidation Loss: 0.959046\n",
      "Validation loss decreased (0.959053 --> 0.959046).         Saving model ...\n",
      "Epoch: 841 \tTraining Loss: 0.893848 \tValidation Loss: 0.959040\n",
      "Validation loss decreased (0.959046 --> 0.959040).         Saving model ...\n",
      "Epoch: 842 \tTraining Loss: 0.893809 \tValidation Loss: 0.959033\n",
      "Validation loss decreased (0.959040 --> 0.959033).         Saving model ...\n",
      "Epoch: 843 \tTraining Loss: 0.893769 \tValidation Loss: 0.959027\n",
      "Validation loss decreased (0.959033 --> 0.959027).         Saving model ...\n",
      "Epoch: 844 \tTraining Loss: 0.893729 \tValidation Loss: 0.959020\n",
      "Validation loss decreased (0.959027 --> 0.959020).         Saving model ...\n",
      "Epoch: 845 \tTraining Loss: 0.893690 \tValidation Loss: 0.959014\n",
      "Validation loss decreased (0.959020 --> 0.959014).         Saving model ...\n",
      "Epoch: 846 \tTraining Loss: 0.893650 \tValidation Loss: 0.959008\n",
      "Validation loss decreased (0.959014 --> 0.959008).         Saving model ...\n",
      "Epoch: 847 \tTraining Loss: 0.893610 \tValidation Loss: 0.959001\n",
      "Validation loss decreased (0.959008 --> 0.959001).         Saving model ...\n",
      "Epoch: 848 \tTraining Loss: 0.893571 \tValidation Loss: 0.958995\n",
      "Validation loss decreased (0.959001 --> 0.958995).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 849 \tTraining Loss: 0.893531 \tValidation Loss: 0.958989\n",
      "Validation loss decreased (0.958995 --> 0.958989).         Saving model ...\n",
      "Epoch: 850 \tTraining Loss: 0.893492 \tValidation Loss: 0.958982\n",
      "Validation loss decreased (0.958989 --> 0.958982).         Saving model ...\n",
      "Epoch: 851 \tTraining Loss: 0.893453 \tValidation Loss: 0.958976\n",
      "Validation loss decreased (0.958982 --> 0.958976).         Saving model ...\n",
      "Epoch: 852 \tTraining Loss: 0.893413 \tValidation Loss: 0.958970\n",
      "Validation loss decreased (0.958976 --> 0.958970).         Saving model ...\n",
      "Epoch: 853 \tTraining Loss: 0.893374 \tValidation Loss: 0.958964\n",
      "Validation loss decreased (0.958970 --> 0.958964).         Saving model ...\n",
      "Epoch: 854 \tTraining Loss: 0.893335 \tValidation Loss: 0.958957\n",
      "Validation loss decreased (0.958964 --> 0.958957).         Saving model ...\n",
      "Epoch: 855 \tTraining Loss: 0.893296 \tValidation Loss: 0.958951\n",
      "Validation loss decreased (0.958957 --> 0.958951).         Saving model ...\n",
      "Epoch: 856 \tTraining Loss: 0.893257 \tValidation Loss: 0.958945\n",
      "Validation loss decreased (0.958951 --> 0.958945).         Saving model ...\n",
      "Epoch: 857 \tTraining Loss: 0.893218 \tValidation Loss: 0.958939\n",
      "Validation loss decreased (0.958945 --> 0.958939).         Saving model ...\n",
      "Epoch: 858 \tTraining Loss: 0.893179 \tValidation Loss: 0.958933\n",
      "Validation loss decreased (0.958939 --> 0.958933).         Saving model ...\n",
      "Epoch: 859 \tTraining Loss: 0.893140 \tValidation Loss: 0.958927\n",
      "Validation loss decreased (0.958933 --> 0.958927).         Saving model ...\n",
      "Epoch: 860 \tTraining Loss: 0.893101 \tValidation Loss: 0.958920\n",
      "Validation loss decreased (0.958927 --> 0.958920).         Saving model ...\n",
      "Epoch: 861 \tTraining Loss: 0.893062 \tValidation Loss: 0.958914\n",
      "Validation loss decreased (0.958920 --> 0.958914).         Saving model ...\n",
      "Epoch: 862 \tTraining Loss: 0.893023 \tValidation Loss: 0.958908\n",
      "Validation loss decreased (0.958914 --> 0.958908).         Saving model ...\n",
      "Epoch: 863 \tTraining Loss: 0.892985 \tValidation Loss: 0.958902\n",
      "Validation loss decreased (0.958908 --> 0.958902).         Saving model ...\n",
      "Epoch: 864 \tTraining Loss: 0.892946 \tValidation Loss: 0.958896\n",
      "Validation loss decreased (0.958902 --> 0.958896).         Saving model ...\n",
      "Epoch: 865 \tTraining Loss: 0.892907 \tValidation Loss: 0.958890\n",
      "Validation loss decreased (0.958896 --> 0.958890).         Saving model ...\n",
      "Epoch: 866 \tTraining Loss: 0.892869 \tValidation Loss: 0.958884\n",
      "Validation loss decreased (0.958890 --> 0.958884).         Saving model ...\n",
      "Epoch: 867 \tTraining Loss: 0.892830 \tValidation Loss: 0.958878\n",
      "Validation loss decreased (0.958884 --> 0.958878).         Saving model ...\n",
      "Epoch: 868 \tTraining Loss: 0.892792 \tValidation Loss: 0.958873\n",
      "Validation loss decreased (0.958878 --> 0.958873).         Saving model ...\n",
      "Epoch: 869 \tTraining Loss: 0.892753 \tValidation Loss: 0.958867\n",
      "Validation loss decreased (0.958873 --> 0.958867).         Saving model ...\n",
      "Epoch: 870 \tTraining Loss: 0.892715 \tValidation Loss: 0.958861\n",
      "Validation loss decreased (0.958867 --> 0.958861).         Saving model ...\n",
      "Epoch: 871 \tTraining Loss: 0.892677 \tValidation Loss: 0.958855\n",
      "Validation loss decreased (0.958861 --> 0.958855).         Saving model ...\n",
      "Epoch: 872 \tTraining Loss: 0.892638 \tValidation Loss: 0.958849\n",
      "Validation loss decreased (0.958855 --> 0.958849).         Saving model ...\n",
      "Epoch: 873 \tTraining Loss: 0.892600 \tValidation Loss: 0.958843\n",
      "Validation loss decreased (0.958849 --> 0.958843).         Saving model ...\n",
      "Epoch: 874 \tTraining Loss: 0.892562 \tValidation Loss: 0.958838\n",
      "Validation loss decreased (0.958843 --> 0.958838).         Saving model ...\n",
      "Epoch: 875 \tTraining Loss: 0.892524 \tValidation Loss: 0.958832\n",
      "Validation loss decreased (0.958838 --> 0.958832).         Saving model ...\n",
      "Epoch: 876 \tTraining Loss: 0.892486 \tValidation Loss: 0.958826\n",
      "Validation loss decreased (0.958832 --> 0.958826).         Saving model ...\n",
      "Epoch: 877 \tTraining Loss: 0.892448 \tValidation Loss: 0.958820\n",
      "Validation loss decreased (0.958826 --> 0.958820).         Saving model ...\n",
      "Epoch: 878 \tTraining Loss: 0.892410 \tValidation Loss: 0.958815\n",
      "Validation loss decreased (0.958820 --> 0.958815).         Saving model ...\n",
      "Epoch: 879 \tTraining Loss: 0.892372 \tValidation Loss: 0.958809\n",
      "Validation loss decreased (0.958815 --> 0.958809).         Saving model ...\n",
      "Epoch: 880 \tTraining Loss: 0.892334 \tValidation Loss: 0.958803\n",
      "Validation loss decreased (0.958809 --> 0.958803).         Saving model ...\n",
      "Epoch: 881 \tTraining Loss: 0.892296 \tValidation Loss: 0.958798\n",
      "Validation loss decreased (0.958803 --> 0.958798).         Saving model ...\n",
      "Epoch: 882 \tTraining Loss: 0.892258 \tValidation Loss: 0.958792\n",
      "Validation loss decreased (0.958798 --> 0.958792).         Saving model ...\n",
      "Epoch: 883 \tTraining Loss: 0.892220 \tValidation Loss: 0.958786\n",
      "Validation loss decreased (0.958792 --> 0.958786).         Saving model ...\n",
      "Epoch: 884 \tTraining Loss: 0.892183 \tValidation Loss: 0.958781\n",
      "Validation loss decreased (0.958786 --> 0.958781).         Saving model ...\n",
      "Epoch: 885 \tTraining Loss: 0.892145 \tValidation Loss: 0.958775\n",
      "Validation loss decreased (0.958781 --> 0.958775).         Saving model ...\n",
      "Epoch: 886 \tTraining Loss: 0.892107 \tValidation Loss: 0.958770\n",
      "Validation loss decreased (0.958775 --> 0.958770).         Saving model ...\n",
      "Epoch: 887 \tTraining Loss: 0.892070 \tValidation Loss: 0.958764\n",
      "Validation loss decreased (0.958770 --> 0.958764).         Saving model ...\n",
      "Epoch: 888 \tTraining Loss: 0.892032 \tValidation Loss: 0.958759\n",
      "Validation loss decreased (0.958764 --> 0.958759).         Saving model ...\n",
      "Epoch: 889 \tTraining Loss: 0.891995 \tValidation Loss: 0.958753\n",
      "Validation loss decreased (0.958759 --> 0.958753).         Saving model ...\n",
      "Epoch: 890 \tTraining Loss: 0.891957 \tValidation Loss: 0.958748\n",
      "Validation loss decreased (0.958753 --> 0.958748).         Saving model ...\n",
      "Epoch: 891 \tTraining Loss: 0.891920 \tValidation Loss: 0.958742\n",
      "Validation loss decreased (0.958748 --> 0.958742).         Saving model ...\n",
      "Epoch: 892 \tTraining Loss: 0.891883 \tValidation Loss: 0.958737\n",
      "Validation loss decreased (0.958742 --> 0.958737).         Saving model ...\n",
      "Epoch: 893 \tTraining Loss: 0.891845 \tValidation Loss: 0.958732\n",
      "Validation loss decreased (0.958737 --> 0.958732).         Saving model ...\n",
      "Epoch: 894 \tTraining Loss: 0.891808 \tValidation Loss: 0.958726\n",
      "Validation loss decreased (0.958732 --> 0.958726).         Saving model ...\n",
      "Epoch: 895 \tTraining Loss: 0.891771 \tValidation Loss: 0.958721\n",
      "Validation loss decreased (0.958726 --> 0.958721).         Saving model ...\n",
      "Epoch: 896 \tTraining Loss: 0.891734 \tValidation Loss: 0.958715\n",
      "Validation loss decreased (0.958721 --> 0.958715).         Saving model ...\n",
      "Epoch: 897 \tTraining Loss: 0.891697 \tValidation Loss: 0.958710\n",
      "Validation loss decreased (0.958715 --> 0.958710).         Saving model ...\n",
      "Epoch: 898 \tTraining Loss: 0.891660 \tValidation Loss: 0.958705\n",
      "Validation loss decreased (0.958710 --> 0.958705).         Saving model ...\n",
      "Epoch: 899 \tTraining Loss: 0.891623 \tValidation Loss: 0.958699\n",
      "Validation loss decreased (0.958705 --> 0.958699).         Saving model ...\n",
      "Epoch: 900 \tTraining Loss: 0.891586 \tValidation Loss: 0.958694\n",
      "Validation loss decreased (0.958699 --> 0.958694).         Saving model ...\n",
      "Epoch: 901 \tTraining Loss: 0.891549 \tValidation Loss: 0.958689\n",
      "Validation loss decreased (0.958694 --> 0.958689).         Saving model ...\n",
      "Epoch: 902 \tTraining Loss: 0.891512 \tValidation Loss: 0.958684\n",
      "Validation loss decreased (0.958689 --> 0.958684).         Saving model ...\n",
      "Epoch: 903 \tTraining Loss: 0.891475 \tValidation Loss: 0.958679\n",
      "Validation loss decreased (0.958684 --> 0.958679).         Saving model ...\n",
      "Epoch: 904 \tTraining Loss: 0.891438 \tValidation Loss: 0.958673\n",
      "Validation loss decreased (0.958679 --> 0.958673).         Saving model ...\n",
      "Epoch: 905 \tTraining Loss: 0.891402 \tValidation Loss: 0.958668\n",
      "Validation loss decreased (0.958673 --> 0.958668).         Saving model ...\n",
      "Epoch: 906 \tTraining Loss: 0.891365 \tValidation Loss: 0.958663\n",
      "Validation loss decreased (0.958668 --> 0.958663).         Saving model ...\n",
      "Epoch: 907 \tTraining Loss: 0.891328 \tValidation Loss: 0.958658\n",
      "Validation loss decreased (0.958663 --> 0.958658).         Saving model ...\n",
      "Epoch: 908 \tTraining Loss: 0.891292 \tValidation Loss: 0.958653\n",
      "Validation loss decreased (0.958658 --> 0.958653).         Saving model ...\n",
      "Epoch: 909 \tTraining Loss: 0.891255 \tValidation Loss: 0.958648\n",
      "Validation loss decreased (0.958653 --> 0.958648).         Saving model ...\n",
      "Epoch: 910 \tTraining Loss: 0.891219 \tValidation Loss: 0.958643\n",
      "Validation loss decreased (0.958648 --> 0.958643).         Saving model ...\n",
      "Epoch: 911 \tTraining Loss: 0.891182 \tValidation Loss: 0.958638\n",
      "Validation loss decreased (0.958643 --> 0.958638).         Saving model ...\n",
      "Epoch: 912 \tTraining Loss: 0.891146 \tValidation Loss: 0.958633\n",
      "Validation loss decreased (0.958638 --> 0.958633).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 913 \tTraining Loss: 0.891109 \tValidation Loss: 0.958628\n",
      "Validation loss decreased (0.958633 --> 0.958628).         Saving model ...\n",
      "Epoch: 914 \tTraining Loss: 0.891073 \tValidation Loss: 0.958623\n",
      "Validation loss decreased (0.958628 --> 0.958623).         Saving model ...\n",
      "Epoch: 915 \tTraining Loss: 0.891037 \tValidation Loss: 0.958618\n",
      "Validation loss decreased (0.958623 --> 0.958618).         Saving model ...\n",
      "Epoch: 916 \tTraining Loss: 0.891001 \tValidation Loss: 0.958613\n",
      "Validation loss decreased (0.958618 --> 0.958613).         Saving model ...\n",
      "Epoch: 917 \tTraining Loss: 0.890964 \tValidation Loss: 0.958608\n",
      "Validation loss decreased (0.958613 --> 0.958608).         Saving model ...\n",
      "Epoch: 918 \tTraining Loss: 0.890928 \tValidation Loss: 0.958603\n",
      "Validation loss decreased (0.958608 --> 0.958603).         Saving model ...\n",
      "Epoch: 919 \tTraining Loss: 0.890892 \tValidation Loss: 0.958598\n",
      "Validation loss decreased (0.958603 --> 0.958598).         Saving model ...\n",
      "Epoch: 920 \tTraining Loss: 0.890856 \tValidation Loss: 0.958593\n",
      "Validation loss decreased (0.958598 --> 0.958593).         Saving model ...\n",
      "Epoch: 921 \tTraining Loss: 0.890820 \tValidation Loss: 0.958588\n",
      "Validation loss decreased (0.958593 --> 0.958588).         Saving model ...\n",
      "Epoch: 922 \tTraining Loss: 0.890784 \tValidation Loss: 0.958583\n",
      "Validation loss decreased (0.958588 --> 0.958583).         Saving model ...\n",
      "Epoch: 923 \tTraining Loss: 0.890748 \tValidation Loss: 0.958578\n",
      "Validation loss decreased (0.958583 --> 0.958578).         Saving model ...\n",
      "Epoch: 924 \tTraining Loss: 0.890712 \tValidation Loss: 0.958574\n",
      "Validation loss decreased (0.958578 --> 0.958574).         Saving model ...\n",
      "Epoch: 925 \tTraining Loss: 0.890676 \tValidation Loss: 0.958569\n",
      "Validation loss decreased (0.958574 --> 0.958569).         Saving model ...\n",
      "Epoch: 926 \tTraining Loss: 0.890641 \tValidation Loss: 0.958564\n",
      "Validation loss decreased (0.958569 --> 0.958564).         Saving model ...\n",
      "Epoch: 927 \tTraining Loss: 0.890605 \tValidation Loss: 0.958559\n",
      "Validation loss decreased (0.958564 --> 0.958559).         Saving model ...\n",
      "Epoch: 928 \tTraining Loss: 0.890569 \tValidation Loss: 0.958555\n",
      "Validation loss decreased (0.958559 --> 0.958555).         Saving model ...\n",
      "Epoch: 929 \tTraining Loss: 0.890534 \tValidation Loss: 0.958550\n",
      "Validation loss decreased (0.958555 --> 0.958550).         Saving model ...\n",
      "Epoch: 930 \tTraining Loss: 0.890498 \tValidation Loss: 0.958545\n",
      "Validation loss decreased (0.958550 --> 0.958545).         Saving model ...\n",
      "Epoch: 931 \tTraining Loss: 0.890462 \tValidation Loss: 0.958540\n",
      "Validation loss decreased (0.958545 --> 0.958540).         Saving model ...\n",
      "Epoch: 932 \tTraining Loss: 0.890427 \tValidation Loss: 0.958536\n",
      "Validation loss decreased (0.958540 --> 0.958536).         Saving model ...\n",
      "Epoch: 933 \tTraining Loss: 0.890391 \tValidation Loss: 0.958531\n",
      "Validation loss decreased (0.958536 --> 0.958531).         Saving model ...\n",
      "Epoch: 934 \tTraining Loss: 0.890356 \tValidation Loss: 0.958527\n",
      "Validation loss decreased (0.958531 --> 0.958527).         Saving model ...\n",
      "Epoch: 935 \tTraining Loss: 0.890320 \tValidation Loss: 0.958522\n",
      "Validation loss decreased (0.958527 --> 0.958522).         Saving model ...\n",
      "Epoch: 936 \tTraining Loss: 0.890285 \tValidation Loss: 0.958517\n",
      "Validation loss decreased (0.958522 --> 0.958517).         Saving model ...\n",
      "Epoch: 937 \tTraining Loss: 0.890250 \tValidation Loss: 0.958513\n",
      "Validation loss decreased (0.958517 --> 0.958513).         Saving model ...\n",
      "Epoch: 938 \tTraining Loss: 0.890214 \tValidation Loss: 0.958508\n",
      "Validation loss decreased (0.958513 --> 0.958508).         Saving model ...\n",
      "Epoch: 939 \tTraining Loss: 0.890179 \tValidation Loss: 0.958504\n",
      "Validation loss decreased (0.958508 --> 0.958504).         Saving model ...\n",
      "Epoch: 940 \tTraining Loss: 0.890144 \tValidation Loss: 0.958499\n",
      "Validation loss decreased (0.958504 --> 0.958499).         Saving model ...\n",
      "Epoch: 941 \tTraining Loss: 0.890109 \tValidation Loss: 0.958495\n",
      "Validation loss decreased (0.958499 --> 0.958495).         Saving model ...\n",
      "Epoch: 942 \tTraining Loss: 0.890074 \tValidation Loss: 0.958490\n",
      "Validation loss decreased (0.958495 --> 0.958490).         Saving model ...\n",
      "Epoch: 943 \tTraining Loss: 0.890039 \tValidation Loss: 0.958486\n",
      "Validation loss decreased (0.958490 --> 0.958486).         Saving model ...\n",
      "Epoch: 944 \tTraining Loss: 0.890004 \tValidation Loss: 0.958481\n",
      "Validation loss decreased (0.958486 --> 0.958481).         Saving model ...\n",
      "Epoch: 945 \tTraining Loss: 0.889969 \tValidation Loss: 0.958477\n",
      "Validation loss decreased (0.958481 --> 0.958477).         Saving model ...\n",
      "Epoch: 946 \tTraining Loss: 0.889934 \tValidation Loss: 0.958472\n",
      "Validation loss decreased (0.958477 --> 0.958472).         Saving model ...\n",
      "Epoch: 947 \tTraining Loss: 0.889899 \tValidation Loss: 0.958468\n",
      "Validation loss decreased (0.958472 --> 0.958468).         Saving model ...\n",
      "Epoch: 948 \tTraining Loss: 0.889864 \tValidation Loss: 0.958463\n",
      "Validation loss decreased (0.958468 --> 0.958463).         Saving model ...\n",
      "Epoch: 949 \tTraining Loss: 0.889829 \tValidation Loss: 0.958459\n",
      "Validation loss decreased (0.958463 --> 0.958459).         Saving model ...\n",
      "Epoch: 950 \tTraining Loss: 0.889794 \tValidation Loss: 0.958455\n",
      "Validation loss decreased (0.958459 --> 0.958455).         Saving model ...\n",
      "Epoch: 951 \tTraining Loss: 0.889759 \tValidation Loss: 0.958450\n",
      "Validation loss decreased (0.958455 --> 0.958450).         Saving model ...\n",
      "Epoch: 952 \tTraining Loss: 0.889725 \tValidation Loss: 0.958446\n",
      "Validation loss decreased (0.958450 --> 0.958446).         Saving model ...\n",
      "Epoch: 953 \tTraining Loss: 0.889690 \tValidation Loss: 0.958442\n",
      "Validation loss decreased (0.958446 --> 0.958442).         Saving model ...\n",
      "Epoch: 954 \tTraining Loss: 0.889655 \tValidation Loss: 0.958438\n",
      "Validation loss decreased (0.958442 --> 0.958438).         Saving model ...\n",
      "Epoch: 955 \tTraining Loss: 0.889621 \tValidation Loss: 0.958433\n",
      "Validation loss decreased (0.958438 --> 0.958433).         Saving model ...\n",
      "Epoch: 956 \tTraining Loss: 0.889586 \tValidation Loss: 0.958429\n",
      "Validation loss decreased (0.958433 --> 0.958429).         Saving model ...\n",
      "Epoch: 957 \tTraining Loss: 0.889552 \tValidation Loss: 0.958425\n",
      "Validation loss decreased (0.958429 --> 0.958425).         Saving model ...\n",
      "Epoch: 958 \tTraining Loss: 0.889517 \tValidation Loss: 0.958421\n",
      "Validation loss decreased (0.958425 --> 0.958421).         Saving model ...\n",
      "Epoch: 959 \tTraining Loss: 0.889483 \tValidation Loss: 0.958416\n",
      "Validation loss decreased (0.958421 --> 0.958416).         Saving model ...\n",
      "Epoch: 960 \tTraining Loss: 0.889449 \tValidation Loss: 0.958412\n",
      "Validation loss decreased (0.958416 --> 0.958412).         Saving model ...\n",
      "Epoch: 961 \tTraining Loss: 0.889414 \tValidation Loss: 0.958408\n",
      "Validation loss decreased (0.958412 --> 0.958408).         Saving model ...\n",
      "Epoch: 962 \tTraining Loss: 0.889380 \tValidation Loss: 0.958404\n",
      "Validation loss decreased (0.958408 --> 0.958404).         Saving model ...\n",
      "Epoch: 963 \tTraining Loss: 0.889346 \tValidation Loss: 0.958400\n",
      "Validation loss decreased (0.958404 --> 0.958400).         Saving model ...\n",
      "Epoch: 964 \tTraining Loss: 0.889311 \tValidation Loss: 0.958396\n",
      "Validation loss decreased (0.958400 --> 0.958396).         Saving model ...\n",
      "Epoch: 965 \tTraining Loss: 0.889277 \tValidation Loss: 0.958392\n",
      "Validation loss decreased (0.958396 --> 0.958392).         Saving model ...\n",
      "Epoch: 966 \tTraining Loss: 0.889243 \tValidation Loss: 0.958387\n",
      "Validation loss decreased (0.958392 --> 0.958387).         Saving model ...\n",
      "Epoch: 967 \tTraining Loss: 0.889209 \tValidation Loss: 0.958383\n",
      "Validation loss decreased (0.958387 --> 0.958383).         Saving model ...\n",
      "Epoch: 968 \tTraining Loss: 0.889175 \tValidation Loss: 0.958379\n",
      "Validation loss decreased (0.958383 --> 0.958379).         Saving model ...\n",
      "Epoch: 969 \tTraining Loss: 0.889141 \tValidation Loss: 0.958375\n",
      "Validation loss decreased (0.958379 --> 0.958375).         Saving model ...\n",
      "Epoch: 970 \tTraining Loss: 0.889107 \tValidation Loss: 0.958371\n",
      "Validation loss decreased (0.958375 --> 0.958371).         Saving model ...\n",
      "Epoch: 971 \tTraining Loss: 0.889073 \tValidation Loss: 0.958367\n",
      "Validation loss decreased (0.958371 --> 0.958367).         Saving model ...\n",
      "Epoch: 972 \tTraining Loss: 0.889039 \tValidation Loss: 0.958363\n",
      "Validation loss decreased (0.958367 --> 0.958363).         Saving model ...\n",
      "Epoch: 973 \tTraining Loss: 0.889005 \tValidation Loss: 0.958359\n",
      "Validation loss decreased (0.958363 --> 0.958359).         Saving model ...\n",
      "Epoch: 974 \tTraining Loss: 0.888971 \tValidation Loss: 0.958355\n",
      "Validation loss decreased (0.958359 --> 0.958355).         Saving model ...\n",
      "Epoch: 975 \tTraining Loss: 0.888938 \tValidation Loss: 0.958351\n",
      "Validation loss decreased (0.958355 --> 0.958351).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 976 \tTraining Loss: 0.888904 \tValidation Loss: 0.958347\n",
      "Validation loss decreased (0.958351 --> 0.958347).         Saving model ...\n",
      "Epoch: 977 \tTraining Loss: 0.888870 \tValidation Loss: 0.958344\n",
      "Validation loss decreased (0.958347 --> 0.958344).         Saving model ...\n",
      "Epoch: 978 \tTraining Loss: 0.888837 \tValidation Loss: 0.958340\n",
      "Validation loss decreased (0.958344 --> 0.958340).         Saving model ...\n",
      "Epoch: 979 \tTraining Loss: 0.888803 \tValidation Loss: 0.958336\n",
      "Validation loss decreased (0.958340 --> 0.958336).         Saving model ...\n",
      "Epoch: 980 \tTraining Loss: 0.888769 \tValidation Loss: 0.958332\n",
      "Validation loss decreased (0.958336 --> 0.958332).         Saving model ...\n",
      "Epoch: 981 \tTraining Loss: 0.888736 \tValidation Loss: 0.958328\n",
      "Validation loss decreased (0.958332 --> 0.958328).         Saving model ...\n",
      "Epoch: 982 \tTraining Loss: 0.888702 \tValidation Loss: 0.958324\n",
      "Validation loss decreased (0.958328 --> 0.958324).         Saving model ...\n",
      "Epoch: 983 \tTraining Loss: 0.888669 \tValidation Loss: 0.958320\n",
      "Validation loss decreased (0.958324 --> 0.958320).         Saving model ...\n",
      "Epoch: 984 \tTraining Loss: 0.888635 \tValidation Loss: 0.958317\n",
      "Validation loss decreased (0.958320 --> 0.958317).         Saving model ...\n",
      "Epoch: 985 \tTraining Loss: 0.888602 \tValidation Loss: 0.958313\n",
      "Validation loss decreased (0.958317 --> 0.958313).         Saving model ...\n",
      "Epoch: 986 \tTraining Loss: 0.888569 \tValidation Loss: 0.958309\n",
      "Validation loss decreased (0.958313 --> 0.958309).         Saving model ...\n",
      "Epoch: 987 \tTraining Loss: 0.888535 \tValidation Loss: 0.958305\n",
      "Validation loss decreased (0.958309 --> 0.958305).         Saving model ...\n",
      "Epoch: 988 \tTraining Loss: 0.888502 \tValidation Loss: 0.958302\n",
      "Validation loss decreased (0.958305 --> 0.958302).         Saving model ...\n",
      "Epoch: 989 \tTraining Loss: 0.888469 \tValidation Loss: 0.958298\n",
      "Validation loss decreased (0.958302 --> 0.958298).         Saving model ...\n",
      "Epoch: 990 \tTraining Loss: 0.888435 \tValidation Loss: 0.958294\n",
      "Validation loss decreased (0.958298 --> 0.958294).         Saving model ...\n",
      "Epoch: 991 \tTraining Loss: 0.888402 \tValidation Loss: 0.958291\n",
      "Validation loss decreased (0.958294 --> 0.958291).         Saving model ...\n",
      "Epoch: 992 \tTraining Loss: 0.888369 \tValidation Loss: 0.958287\n",
      "Validation loss decreased (0.958291 --> 0.958287).         Saving model ...\n",
      "Epoch: 993 \tTraining Loss: 0.888336 \tValidation Loss: 0.958283\n",
      "Validation loss decreased (0.958287 --> 0.958283).         Saving model ...\n",
      "Epoch: 994 \tTraining Loss: 0.888303 \tValidation Loss: 0.958280\n",
      "Validation loss decreased (0.958283 --> 0.958280).         Saving model ...\n",
      "Epoch: 995 \tTraining Loss: 0.888270 \tValidation Loss: 0.958276\n",
      "Validation loss decreased (0.958280 --> 0.958276).         Saving model ...\n",
      "Epoch: 996 \tTraining Loss: 0.888237 \tValidation Loss: 0.958272\n",
      "Validation loss decreased (0.958276 --> 0.958272).         Saving model ...\n",
      "Epoch: 997 \tTraining Loss: 0.888204 \tValidation Loss: 0.958269\n",
      "Validation loss decreased (0.958272 --> 0.958269).         Saving model ...\n",
      "Epoch: 998 \tTraining Loss: 0.888171 \tValidation Loss: 0.958265\n",
      "Validation loss decreased (0.958269 --> 0.958265).         Saving model ...\n",
      "Epoch: 999 \tTraining Loss: 0.888138 \tValidation Loss: 0.958262\n",
      "Validation loss decreased (0.958265 --> 0.958262).         Saving model ...\n",
      "Epoch: 1000 \tTraining Loss: 0.888105 \tValidation Loss: 0.958258\n",
      "Validation loss decreased (0.958262 --> 0.958258).         Saving model ...\n",
      "Epoch: 1001 \tTraining Loss: 0.888073 \tValidation Loss: 0.958254\n",
      "Validation loss decreased (0.958258 --> 0.958254).         Saving model ...\n",
      "Epoch: 1002 \tTraining Loss: 0.888040 \tValidation Loss: 0.958251\n",
      "Validation loss decreased (0.958254 --> 0.958251).         Saving model ...\n",
      "Epoch: 1003 \tTraining Loss: 0.888007 \tValidation Loss: 0.958247\n",
      "Validation loss decreased (0.958251 --> 0.958247).         Saving model ...\n",
      "Epoch: 1004 \tTraining Loss: 0.887974 \tValidation Loss: 0.958244\n",
      "Validation loss decreased (0.958247 --> 0.958244).         Saving model ...\n",
      "Epoch: 1005 \tTraining Loss: 0.887942 \tValidation Loss: 0.958240\n",
      "Validation loss decreased (0.958244 --> 0.958240).         Saving model ...\n",
      "Epoch: 1006 \tTraining Loss: 0.887909 \tValidation Loss: 0.958237\n",
      "Validation loss decreased (0.958240 --> 0.958237).         Saving model ...\n",
      "Epoch: 1007 \tTraining Loss: 0.887877 \tValidation Loss: 0.958233\n",
      "Validation loss decreased (0.958237 --> 0.958233).         Saving model ...\n",
      "Epoch: 1008 \tTraining Loss: 0.887844 \tValidation Loss: 0.958230\n",
      "Validation loss decreased (0.958233 --> 0.958230).         Saving model ...\n",
      "Epoch: 1009 \tTraining Loss: 0.887811 \tValidation Loss: 0.958227\n",
      "Validation loss decreased (0.958230 --> 0.958227).         Saving model ...\n",
      "Epoch: 1010 \tTraining Loss: 0.887779 \tValidation Loss: 0.958223\n",
      "Validation loss decreased (0.958227 --> 0.958223).         Saving model ...\n",
      "Epoch: 1011 \tTraining Loss: 0.887747 \tValidation Loss: 0.958220\n",
      "Validation loss decreased (0.958223 --> 0.958220).         Saving model ...\n",
      "Epoch: 1012 \tTraining Loss: 0.887714 \tValidation Loss: 0.958216\n",
      "Validation loss decreased (0.958220 --> 0.958216).         Saving model ...\n",
      "Epoch: 1013 \tTraining Loss: 0.887682 \tValidation Loss: 0.958213\n",
      "Validation loss decreased (0.958216 --> 0.958213).         Saving model ...\n",
      "Epoch: 1014 \tTraining Loss: 0.887649 \tValidation Loss: 0.958210\n",
      "Validation loss decreased (0.958213 --> 0.958210).         Saving model ...\n",
      "Epoch: 1015 \tTraining Loss: 0.887617 \tValidation Loss: 0.958206\n",
      "Validation loss decreased (0.958210 --> 0.958206).         Saving model ...\n",
      "Epoch: 1016 \tTraining Loss: 0.887585 \tValidation Loss: 0.958203\n",
      "Validation loss decreased (0.958206 --> 0.958203).         Saving model ...\n",
      "Epoch: 1017 \tTraining Loss: 0.887553 \tValidation Loss: 0.958200\n",
      "Validation loss decreased (0.958203 --> 0.958200).         Saving model ...\n",
      "Epoch: 1018 \tTraining Loss: 0.887520 \tValidation Loss: 0.958197\n",
      "Validation loss decreased (0.958200 --> 0.958197).         Saving model ...\n",
      "Epoch: 1019 \tTraining Loss: 0.887488 \tValidation Loss: 0.958193\n",
      "Validation loss decreased (0.958197 --> 0.958193).         Saving model ...\n",
      "Epoch: 1020 \tTraining Loss: 0.887456 \tValidation Loss: 0.958190\n",
      "Validation loss decreased (0.958193 --> 0.958190).         Saving model ...\n",
      "Epoch: 1021 \tTraining Loss: 0.887424 \tValidation Loss: 0.958187\n",
      "Validation loss decreased (0.958190 --> 0.958187).         Saving model ...\n",
      "Epoch: 1022 \tTraining Loss: 0.887392 \tValidation Loss: 0.958184\n",
      "Validation loss decreased (0.958187 --> 0.958184).         Saving model ...\n",
      "Epoch: 1023 \tTraining Loss: 0.887360 \tValidation Loss: 0.958180\n",
      "Validation loss decreased (0.958184 --> 0.958180).         Saving model ...\n",
      "Epoch: 1024 \tTraining Loss: 0.887328 \tValidation Loss: 0.958177\n",
      "Validation loss decreased (0.958180 --> 0.958177).         Saving model ...\n",
      "Epoch: 1025 \tTraining Loss: 0.887296 \tValidation Loss: 0.958174\n",
      "Validation loss decreased (0.958177 --> 0.958174).         Saving model ...\n",
      "Epoch: 1026 \tTraining Loss: 0.887264 \tValidation Loss: 0.958171\n",
      "Validation loss decreased (0.958174 --> 0.958171).         Saving model ...\n",
      "Epoch: 1027 \tTraining Loss: 0.887232 \tValidation Loss: 0.958168\n",
      "Validation loss decreased (0.958171 --> 0.958168).         Saving model ...\n",
      "Epoch: 1028 \tTraining Loss: 0.887201 \tValidation Loss: 0.958164\n",
      "Validation loss decreased (0.958168 --> 0.958164).         Saving model ...\n",
      "Epoch: 1029 \tTraining Loss: 0.887169 \tValidation Loss: 0.958161\n",
      "Validation loss decreased (0.958164 --> 0.958161).         Saving model ...\n",
      "Epoch: 1030 \tTraining Loss: 0.887137 \tValidation Loss: 0.958158\n",
      "Validation loss decreased (0.958161 --> 0.958158).         Saving model ...\n",
      "Epoch: 1031 \tTraining Loss: 0.887105 \tValidation Loss: 0.958155\n",
      "Validation loss decreased (0.958158 --> 0.958155).         Saving model ...\n",
      "Epoch: 1032 \tTraining Loss: 0.887074 \tValidation Loss: 0.958152\n",
      "Validation loss decreased (0.958155 --> 0.958152).         Saving model ...\n",
      "Epoch: 1033 \tTraining Loss: 0.887042 \tValidation Loss: 0.958149\n",
      "Validation loss decreased (0.958152 --> 0.958149).         Saving model ...\n",
      "Epoch: 1034 \tTraining Loss: 0.887010 \tValidation Loss: 0.958146\n",
      "Validation loss decreased (0.958149 --> 0.958146).         Saving model ...\n",
      "Epoch: 1035 \tTraining Loss: 0.886979 \tValidation Loss: 0.958143\n",
      "Validation loss decreased (0.958146 --> 0.958143).         Saving model ...\n",
      "Epoch: 1036 \tTraining Loss: 0.886947 \tValidation Loss: 0.958140\n",
      "Validation loss decreased (0.958143 --> 0.958140).         Saving model ...\n",
      "Epoch: 1037 \tTraining Loss: 0.886916 \tValidation Loss: 0.958137\n",
      "Validation loss decreased (0.958140 --> 0.958137).         Saving model ...\n",
      "Epoch: 1038 \tTraining Loss: 0.886884 \tValidation Loss: 0.958134\n",
      "Validation loss decreased (0.958137 --> 0.958134).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1039 \tTraining Loss: 0.886853 \tValidation Loss: 0.958131\n",
      "Validation loss decreased (0.958134 --> 0.958131).         Saving model ...\n",
      "Epoch: 1040 \tTraining Loss: 0.886821 \tValidation Loss: 0.958128\n",
      "Validation loss decreased (0.958131 --> 0.958128).         Saving model ...\n",
      "Epoch: 1041 \tTraining Loss: 0.886790 \tValidation Loss: 0.958125\n",
      "Validation loss decreased (0.958128 --> 0.958125).         Saving model ...\n",
      "Epoch: 1042 \tTraining Loss: 0.886758 \tValidation Loss: 0.958122\n",
      "Validation loss decreased (0.958125 --> 0.958122).         Saving model ...\n",
      "Epoch: 1043 \tTraining Loss: 0.886727 \tValidation Loss: 0.958119\n",
      "Validation loss decreased (0.958122 --> 0.958119).         Saving model ...\n",
      "Epoch: 1044 \tTraining Loss: 0.886696 \tValidation Loss: 0.958116\n",
      "Validation loss decreased (0.958119 --> 0.958116).         Saving model ...\n",
      "Epoch: 1045 \tTraining Loss: 0.886665 \tValidation Loss: 0.958113\n",
      "Validation loss decreased (0.958116 --> 0.958113).         Saving model ...\n",
      "Epoch: 1046 \tTraining Loss: 0.886633 \tValidation Loss: 0.958110\n",
      "Validation loss decreased (0.958113 --> 0.958110).         Saving model ...\n",
      "Epoch: 1047 \tTraining Loss: 0.886602 \tValidation Loss: 0.958107\n",
      "Validation loss decreased (0.958110 --> 0.958107).         Saving model ...\n",
      "Epoch: 1048 \tTraining Loss: 0.886571 \tValidation Loss: 0.958104\n",
      "Validation loss decreased (0.958107 --> 0.958104).         Saving model ...\n",
      "Epoch: 1049 \tTraining Loss: 0.886540 \tValidation Loss: 0.958102\n",
      "Validation loss decreased (0.958104 --> 0.958102).         Saving model ...\n",
      "Epoch: 1050 \tTraining Loss: 0.886509 \tValidation Loss: 0.958099\n",
      "Validation loss decreased (0.958102 --> 0.958099).         Saving model ...\n",
      "Epoch: 1051 \tTraining Loss: 0.886478 \tValidation Loss: 0.958096\n",
      "Validation loss decreased (0.958099 --> 0.958096).         Saving model ...\n",
      "Epoch: 1052 \tTraining Loss: 0.886447 \tValidation Loss: 0.958093\n",
      "Validation loss decreased (0.958096 --> 0.958093).         Saving model ...\n",
      "Epoch: 1053 \tTraining Loss: 0.886416 \tValidation Loss: 0.958090\n",
      "Validation loss decreased (0.958093 --> 0.958090).         Saving model ...\n",
      "Epoch: 1054 \tTraining Loss: 0.886385 \tValidation Loss: 0.958087\n",
      "Validation loss decreased (0.958090 --> 0.958087).         Saving model ...\n",
      "Epoch: 1055 \tTraining Loss: 0.886354 \tValidation Loss: 0.958085\n",
      "Validation loss decreased (0.958087 --> 0.958085).         Saving model ...\n",
      "Epoch: 1056 \tTraining Loss: 0.886323 \tValidation Loss: 0.958082\n",
      "Validation loss decreased (0.958085 --> 0.958082).         Saving model ...\n",
      "Epoch: 1057 \tTraining Loss: 0.886292 \tValidation Loss: 0.958079\n",
      "Validation loss decreased (0.958082 --> 0.958079).         Saving model ...\n",
      "Epoch: 1058 \tTraining Loss: 0.886261 \tValidation Loss: 0.958076\n",
      "Validation loss decreased (0.958079 --> 0.958076).         Saving model ...\n",
      "Epoch: 1059 \tTraining Loss: 0.886231 \tValidation Loss: 0.958074\n",
      "Validation loss decreased (0.958076 --> 0.958074).         Saving model ...\n",
      "Epoch: 1060 \tTraining Loss: 0.886200 \tValidation Loss: 0.958071\n",
      "Validation loss decreased (0.958074 --> 0.958071).         Saving model ...\n",
      "Epoch: 1061 \tTraining Loss: 0.886169 \tValidation Loss: 0.958068\n",
      "Validation loss decreased (0.958071 --> 0.958068).         Saving model ...\n",
      "Epoch: 1062 \tTraining Loss: 0.886138 \tValidation Loss: 0.958066\n",
      "Validation loss decreased (0.958068 --> 0.958066).         Saving model ...\n",
      "Epoch: 1063 \tTraining Loss: 0.886108 \tValidation Loss: 0.958063\n",
      "Validation loss decreased (0.958066 --> 0.958063).         Saving model ...\n",
      "Epoch: 1064 \tTraining Loss: 0.886077 \tValidation Loss: 0.958060\n",
      "Validation loss decreased (0.958063 --> 0.958060).         Saving model ...\n",
      "Epoch: 1065 \tTraining Loss: 0.886047 \tValidation Loss: 0.958058\n",
      "Validation loss decreased (0.958060 --> 0.958058).         Saving model ...\n",
      "Epoch: 1066 \tTraining Loss: 0.886016 \tValidation Loss: 0.958055\n",
      "Validation loss decreased (0.958058 --> 0.958055).         Saving model ...\n",
      "Epoch: 1067 \tTraining Loss: 0.885985 \tValidation Loss: 0.958052\n",
      "Validation loss decreased (0.958055 --> 0.958052).         Saving model ...\n",
      "Epoch: 1068 \tTraining Loss: 0.885955 \tValidation Loss: 0.958050\n",
      "Validation loss decreased (0.958052 --> 0.958050).         Saving model ...\n",
      "Epoch: 1069 \tTraining Loss: 0.885925 \tValidation Loss: 0.958047\n",
      "Validation loss decreased (0.958050 --> 0.958047).         Saving model ...\n",
      "Epoch: 1070 \tTraining Loss: 0.885894 \tValidation Loss: 0.958045\n",
      "Validation loss decreased (0.958047 --> 0.958045).         Saving model ...\n",
      "Epoch: 1071 \tTraining Loss: 0.885864 \tValidation Loss: 0.958042\n",
      "Validation loss decreased (0.958045 --> 0.958042).         Saving model ...\n",
      "Epoch: 1072 \tTraining Loss: 0.885833 \tValidation Loss: 0.958039\n",
      "Validation loss decreased (0.958042 --> 0.958039).         Saving model ...\n",
      "Epoch: 1073 \tTraining Loss: 0.885803 \tValidation Loss: 0.958037\n",
      "Validation loss decreased (0.958039 --> 0.958037).         Saving model ...\n",
      "Epoch: 1074 \tTraining Loss: 0.885773 \tValidation Loss: 0.958034\n",
      "Validation loss decreased (0.958037 --> 0.958034).         Saving model ...\n",
      "Epoch: 1075 \tTraining Loss: 0.885742 \tValidation Loss: 0.958032\n",
      "Validation loss decreased (0.958034 --> 0.958032).         Saving model ...\n",
      "Epoch: 1076 \tTraining Loss: 0.885712 \tValidation Loss: 0.958029\n",
      "Validation loss decreased (0.958032 --> 0.958029).         Saving model ...\n",
      "Epoch: 1077 \tTraining Loss: 0.885682 \tValidation Loss: 0.958027\n",
      "Validation loss decreased (0.958029 --> 0.958027).         Saving model ...\n",
      "Epoch: 1078 \tTraining Loss: 0.885652 \tValidation Loss: 0.958024\n",
      "Validation loss decreased (0.958027 --> 0.958024).         Saving model ...\n",
      "Epoch: 1079 \tTraining Loss: 0.885622 \tValidation Loss: 0.958022\n",
      "Validation loss decreased (0.958024 --> 0.958022).         Saving model ...\n",
      "Epoch: 1080 \tTraining Loss: 0.885592 \tValidation Loss: 0.958019\n",
      "Validation loss decreased (0.958022 --> 0.958019).         Saving model ...\n",
      "Epoch: 1081 \tTraining Loss: 0.885562 \tValidation Loss: 0.958017\n",
      "Validation loss decreased (0.958019 --> 0.958017).         Saving model ...\n",
      "Epoch: 1082 \tTraining Loss: 0.885532 \tValidation Loss: 0.958015\n",
      "Validation loss decreased (0.958017 --> 0.958015).         Saving model ...\n",
      "Epoch: 1083 \tTraining Loss: 0.885501 \tValidation Loss: 0.958012\n",
      "Validation loss decreased (0.958015 --> 0.958012).         Saving model ...\n",
      "Epoch: 1084 \tTraining Loss: 0.885471 \tValidation Loss: 0.958010\n",
      "Validation loss decreased (0.958012 --> 0.958010).         Saving model ...\n",
      "Epoch: 1085 \tTraining Loss: 0.885442 \tValidation Loss: 0.958007\n",
      "Validation loss decreased (0.958010 --> 0.958007).         Saving model ...\n",
      "Epoch: 1086 \tTraining Loss: 0.885412 \tValidation Loss: 0.958005\n",
      "Validation loss decreased (0.958007 --> 0.958005).         Saving model ...\n",
      "Epoch: 1087 \tTraining Loss: 0.885382 \tValidation Loss: 0.958003\n",
      "Validation loss decreased (0.958005 --> 0.958003).         Saving model ...\n",
      "Epoch: 1088 \tTraining Loss: 0.885352 \tValidation Loss: 0.958000\n",
      "Validation loss decreased (0.958003 --> 0.958000).         Saving model ...\n",
      "Epoch: 1089 \tTraining Loss: 0.885322 \tValidation Loss: 0.957998\n",
      "Validation loss decreased (0.958000 --> 0.957998).         Saving model ...\n",
      "Epoch: 1090 \tTraining Loss: 0.885292 \tValidation Loss: 0.957996\n",
      "Validation loss decreased (0.957998 --> 0.957996).         Saving model ...\n",
      "Epoch: 1091 \tTraining Loss: 0.885262 \tValidation Loss: 0.957993\n",
      "Validation loss decreased (0.957996 --> 0.957993).         Saving model ...\n",
      "Epoch: 1092 \tTraining Loss: 0.885233 \tValidation Loss: 0.957991\n",
      "Validation loss decreased (0.957993 --> 0.957991).         Saving model ...\n",
      "Epoch: 1093 \tTraining Loss: 0.885203 \tValidation Loss: 0.957989\n",
      "Validation loss decreased (0.957991 --> 0.957989).         Saving model ...\n",
      "Epoch: 1094 \tTraining Loss: 0.885173 \tValidation Loss: 0.957986\n",
      "Validation loss decreased (0.957989 --> 0.957986).         Saving model ...\n",
      "Epoch: 1095 \tTraining Loss: 0.885144 \tValidation Loss: 0.957984\n",
      "Validation loss decreased (0.957986 --> 0.957984).         Saving model ...\n",
      "Epoch: 1096 \tTraining Loss: 0.885114 \tValidation Loss: 0.957982\n",
      "Validation loss decreased (0.957984 --> 0.957982).         Saving model ...\n",
      "Epoch: 1097 \tTraining Loss: 0.885085 \tValidation Loss: 0.957979\n",
      "Validation loss decreased (0.957982 --> 0.957979).         Saving model ...\n",
      "Epoch: 1098 \tTraining Loss: 0.885055 \tValidation Loss: 0.957977\n",
      "Validation loss decreased (0.957979 --> 0.957977).         Saving model ...\n",
      "Epoch: 1099 \tTraining Loss: 0.885025 \tValidation Loss: 0.957975\n",
      "Validation loss decreased (0.957977 --> 0.957975).         Saving model ...\n",
      "Epoch: 1100 \tTraining Loss: 0.884996 \tValidation Loss: 0.957973\n",
      "Validation loss decreased (0.957975 --> 0.957973).         Saving model ...\n",
      "Epoch: 1101 \tTraining Loss: 0.884967 \tValidation Loss: 0.957971\n",
      "Validation loss decreased (0.957973 --> 0.957971).         Saving model ...\n",
      "Epoch: 1102 \tTraining Loss: 0.884937 \tValidation Loss: 0.957968\n",
      "Validation loss decreased (0.957971 --> 0.957968).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1103 \tTraining Loss: 0.884908 \tValidation Loss: 0.957966\n",
      "Validation loss decreased (0.957968 --> 0.957966).         Saving model ...\n",
      "Epoch: 1104 \tTraining Loss: 0.884878 \tValidation Loss: 0.957964\n",
      "Validation loss decreased (0.957966 --> 0.957964).         Saving model ...\n",
      "Epoch: 1105 \tTraining Loss: 0.884849 \tValidation Loss: 0.957962\n",
      "Validation loss decreased (0.957964 --> 0.957962).         Saving model ...\n",
      "Epoch: 1106 \tTraining Loss: 0.884820 \tValidation Loss: 0.957960\n",
      "Validation loss decreased (0.957962 --> 0.957960).         Saving model ...\n",
      "Epoch: 1107 \tTraining Loss: 0.884790 \tValidation Loss: 0.957958\n",
      "Validation loss decreased (0.957960 --> 0.957958).         Saving model ...\n",
      "Epoch: 1108 \tTraining Loss: 0.884761 \tValidation Loss: 0.957956\n",
      "Validation loss decreased (0.957958 --> 0.957956).         Saving model ...\n",
      "Epoch: 1109 \tTraining Loss: 0.884732 \tValidation Loss: 0.957953\n",
      "Validation loss decreased (0.957956 --> 0.957953).         Saving model ...\n",
      "Epoch: 1110 \tTraining Loss: 0.884703 \tValidation Loss: 0.957951\n",
      "Validation loss decreased (0.957953 --> 0.957951).         Saving model ...\n",
      "Epoch: 1111 \tTraining Loss: 0.884674 \tValidation Loss: 0.957949\n",
      "Validation loss decreased (0.957951 --> 0.957949).         Saving model ...\n",
      "Epoch: 1112 \tTraining Loss: 0.884644 \tValidation Loss: 0.957947\n",
      "Validation loss decreased (0.957949 --> 0.957947).         Saving model ...\n",
      "Epoch: 1113 \tTraining Loss: 0.884615 \tValidation Loss: 0.957945\n",
      "Validation loss decreased (0.957947 --> 0.957945).         Saving model ...\n",
      "Epoch: 1114 \tTraining Loss: 0.884586 \tValidation Loss: 0.957943\n",
      "Validation loss decreased (0.957945 --> 0.957943).         Saving model ...\n",
      "Epoch: 1115 \tTraining Loss: 0.884557 \tValidation Loss: 0.957941\n",
      "Validation loss decreased (0.957943 --> 0.957941).         Saving model ...\n",
      "Epoch: 1116 \tTraining Loss: 0.884528 \tValidation Loss: 0.957939\n",
      "Validation loss decreased (0.957941 --> 0.957939).         Saving model ...\n",
      "Epoch: 1117 \tTraining Loss: 0.884499 \tValidation Loss: 0.957937\n",
      "Validation loss decreased (0.957939 --> 0.957937).         Saving model ...\n",
      "Epoch: 1118 \tTraining Loss: 0.884470 \tValidation Loss: 0.957935\n",
      "Validation loss decreased (0.957937 --> 0.957935).         Saving model ...\n",
      "Epoch: 1119 \tTraining Loss: 0.884441 \tValidation Loss: 0.957933\n",
      "Validation loss decreased (0.957935 --> 0.957933).         Saving model ...\n",
      "Epoch: 1120 \tTraining Loss: 0.884412 \tValidation Loss: 0.957931\n",
      "Validation loss decreased (0.957933 --> 0.957931).         Saving model ...\n",
      "Epoch: 1121 \tTraining Loss: 0.884384 \tValidation Loss: 0.957929\n",
      "Validation loss decreased (0.957931 --> 0.957929).         Saving model ...\n",
      "Epoch: 1122 \tTraining Loss: 0.884355 \tValidation Loss: 0.957927\n",
      "Validation loss decreased (0.957929 --> 0.957927).         Saving model ...\n",
      "Epoch: 1123 \tTraining Loss: 0.884326 \tValidation Loss: 0.957925\n",
      "Validation loss decreased (0.957927 --> 0.957925).         Saving model ...\n",
      "Epoch: 1124 \tTraining Loss: 0.884297 \tValidation Loss: 0.957923\n",
      "Validation loss decreased (0.957925 --> 0.957923).         Saving model ...\n",
      "Epoch: 1125 \tTraining Loss: 0.884268 \tValidation Loss: 0.957921\n",
      "Validation loss decreased (0.957923 --> 0.957921).         Saving model ...\n",
      "Epoch: 1126 \tTraining Loss: 0.884240 \tValidation Loss: 0.957919\n",
      "Validation loss decreased (0.957921 --> 0.957919).         Saving model ...\n",
      "Epoch: 1127 \tTraining Loss: 0.884211 \tValidation Loss: 0.957917\n",
      "Validation loss decreased (0.957919 --> 0.957917).         Saving model ...\n",
      "Epoch: 1128 \tTraining Loss: 0.884182 \tValidation Loss: 0.957916\n",
      "Validation loss decreased (0.957917 --> 0.957916).         Saving model ...\n",
      "Epoch: 1129 \tTraining Loss: 0.884154 \tValidation Loss: 0.957914\n",
      "Validation loss decreased (0.957916 --> 0.957914).         Saving model ...\n",
      "Epoch: 1130 \tTraining Loss: 0.884125 \tValidation Loss: 0.957912\n",
      "Validation loss decreased (0.957914 --> 0.957912).         Saving model ...\n",
      "Epoch: 1131 \tTraining Loss: 0.884097 \tValidation Loss: 0.957910\n",
      "Validation loss decreased (0.957912 --> 0.957910).         Saving model ...\n",
      "Epoch: 1132 \tTraining Loss: 0.884068 \tValidation Loss: 0.957908\n",
      "Validation loss decreased (0.957910 --> 0.957908).         Saving model ...\n",
      "Epoch: 1133 \tTraining Loss: 0.884040 \tValidation Loss: 0.957906\n",
      "Validation loss decreased (0.957908 --> 0.957906).         Saving model ...\n",
      "Epoch: 1134 \tTraining Loss: 0.884011 \tValidation Loss: 0.957904\n",
      "Validation loss decreased (0.957906 --> 0.957904).         Saving model ...\n",
      "Epoch: 1135 \tTraining Loss: 0.883983 \tValidation Loss: 0.957903\n",
      "Validation loss decreased (0.957904 --> 0.957903).         Saving model ...\n",
      "Epoch: 1136 \tTraining Loss: 0.883954 \tValidation Loss: 0.957901\n",
      "Validation loss decreased (0.957903 --> 0.957901).         Saving model ...\n",
      "Epoch: 1137 \tTraining Loss: 0.883926 \tValidation Loss: 0.957899\n",
      "Validation loss decreased (0.957901 --> 0.957899).         Saving model ...\n",
      "Epoch: 1138 \tTraining Loss: 0.883897 \tValidation Loss: 0.957897\n",
      "Validation loss decreased (0.957899 --> 0.957897).         Saving model ...\n",
      "Epoch: 1139 \tTraining Loss: 0.883869 \tValidation Loss: 0.957896\n",
      "Validation loss decreased (0.957897 --> 0.957896).         Saving model ...\n",
      "Epoch: 1140 \tTraining Loss: 0.883841 \tValidation Loss: 0.957894\n",
      "Validation loss decreased (0.957896 --> 0.957894).         Saving model ...\n",
      "Epoch: 1141 \tTraining Loss: 0.883812 \tValidation Loss: 0.957892\n",
      "Validation loss decreased (0.957894 --> 0.957892).         Saving model ...\n",
      "Epoch: 1142 \tTraining Loss: 0.883784 \tValidation Loss: 0.957890\n",
      "Validation loss decreased (0.957892 --> 0.957890).         Saving model ...\n",
      "Epoch: 1143 \tTraining Loss: 0.883756 \tValidation Loss: 0.957889\n",
      "Validation loss decreased (0.957890 --> 0.957889).         Saving model ...\n",
      "Epoch: 1144 \tTraining Loss: 0.883728 \tValidation Loss: 0.957887\n",
      "Validation loss decreased (0.957889 --> 0.957887).         Saving model ...\n",
      "Epoch: 1145 \tTraining Loss: 0.883699 \tValidation Loss: 0.957885\n",
      "Validation loss decreased (0.957887 --> 0.957885).         Saving model ...\n",
      "Epoch: 1146 \tTraining Loss: 0.883671 \tValidation Loss: 0.957883\n",
      "Validation loss decreased (0.957885 --> 0.957883).         Saving model ...\n",
      "Epoch: 1147 \tTraining Loss: 0.883643 \tValidation Loss: 0.957882\n",
      "Validation loss decreased (0.957883 --> 0.957882).         Saving model ...\n",
      "Epoch: 1148 \tTraining Loss: 0.883615 \tValidation Loss: 0.957880\n",
      "Validation loss decreased (0.957882 --> 0.957880).         Saving model ...\n",
      "Epoch: 1149 \tTraining Loss: 0.883587 \tValidation Loss: 0.957878\n",
      "Validation loss decreased (0.957880 --> 0.957878).         Saving model ...\n",
      "Epoch: 1150 \tTraining Loss: 0.883559 \tValidation Loss: 0.957877\n",
      "Validation loss decreased (0.957878 --> 0.957877).         Saving model ...\n",
      "Epoch: 1151 \tTraining Loss: 0.883531 \tValidation Loss: 0.957875\n",
      "Validation loss decreased (0.957877 --> 0.957875).         Saving model ...\n",
      "Epoch: 1152 \tTraining Loss: 0.883503 \tValidation Loss: 0.957874\n",
      "Validation loss decreased (0.957875 --> 0.957874).         Saving model ...\n",
      "Epoch: 1153 \tTraining Loss: 0.883475 \tValidation Loss: 0.957872\n",
      "Validation loss decreased (0.957874 --> 0.957872).         Saving model ...\n",
      "Epoch: 1154 \tTraining Loss: 0.883447 \tValidation Loss: 0.957870\n",
      "Validation loss decreased (0.957872 --> 0.957870).         Saving model ...\n",
      "Epoch: 1155 \tTraining Loss: 0.883419 \tValidation Loss: 0.957869\n",
      "Validation loss decreased (0.957870 --> 0.957869).         Saving model ...\n",
      "Epoch: 1156 \tTraining Loss: 0.883391 \tValidation Loss: 0.957867\n",
      "Validation loss decreased (0.957869 --> 0.957867).         Saving model ...\n",
      "Epoch: 1157 \tTraining Loss: 0.883364 \tValidation Loss: 0.957865\n",
      "Validation loss decreased (0.957867 --> 0.957865).         Saving model ...\n",
      "Epoch: 1158 \tTraining Loss: 0.883336 \tValidation Loss: 0.957864\n",
      "Validation loss decreased (0.957865 --> 0.957864).         Saving model ...\n",
      "Epoch: 1159 \tTraining Loss: 0.883308 \tValidation Loss: 0.957862\n",
      "Validation loss decreased (0.957864 --> 0.957862).         Saving model ...\n",
      "Epoch: 1160 \tTraining Loss: 0.883280 \tValidation Loss: 0.957861\n",
      "Validation loss decreased (0.957862 --> 0.957861).         Saving model ...\n",
      "Epoch: 1161 \tTraining Loss: 0.883252 \tValidation Loss: 0.957859\n",
      "Validation loss decreased (0.957861 --> 0.957859).         Saving model ...\n",
      "Epoch: 1162 \tTraining Loss: 0.883225 \tValidation Loss: 0.957858\n",
      "Validation loss decreased (0.957859 --> 0.957858).         Saving model ...\n",
      "Epoch: 1163 \tTraining Loss: 0.883197 \tValidation Loss: 0.957856\n",
      "Validation loss decreased (0.957858 --> 0.957856).         Saving model ...\n",
      "Epoch: 1164 \tTraining Loss: 0.883169 \tValidation Loss: 0.957855\n",
      "Validation loss decreased (0.957856 --> 0.957855).         Saving model ...\n",
      "Epoch: 1165 \tTraining Loss: 0.883142 \tValidation Loss: 0.957853\n",
      "Validation loss decreased (0.957855 --> 0.957853).         Saving model ...\n",
      "Epoch: 1166 \tTraining Loss: 0.883114 \tValidation Loss: 0.957852\n",
      "Validation loss decreased (0.957853 --> 0.957852).         Saving model ...\n",
      "Epoch: 1167 \tTraining Loss: 0.883087 \tValidation Loss: 0.957850\n",
      "Validation loss decreased (0.957852 --> 0.957850).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1168 \tTraining Loss: 0.883059 \tValidation Loss: 0.957849\n",
      "Validation loss decreased (0.957850 --> 0.957849).         Saving model ...\n",
      "Epoch: 1169 \tTraining Loss: 0.883032 \tValidation Loss: 0.957847\n",
      "Validation loss decreased (0.957849 --> 0.957847).         Saving model ...\n",
      "Epoch: 1170 \tTraining Loss: 0.883004 \tValidation Loss: 0.957846\n",
      "Validation loss decreased (0.957847 --> 0.957846).         Saving model ...\n",
      "Epoch: 1171 \tTraining Loss: 0.882977 \tValidation Loss: 0.957845\n",
      "Validation loss decreased (0.957846 --> 0.957845).         Saving model ...\n",
      "Epoch: 1172 \tTraining Loss: 0.882949 \tValidation Loss: 0.957843\n",
      "Validation loss decreased (0.957845 --> 0.957843).         Saving model ...\n",
      "Epoch: 1173 \tTraining Loss: 0.882922 \tValidation Loss: 0.957842\n",
      "Validation loss decreased (0.957843 --> 0.957842).         Saving model ...\n",
      "Epoch: 1174 \tTraining Loss: 0.882894 \tValidation Loss: 0.957840\n",
      "Validation loss decreased (0.957842 --> 0.957840).         Saving model ...\n",
      "Epoch: 1175 \tTraining Loss: 0.882867 \tValidation Loss: 0.957839\n",
      "Validation loss decreased (0.957840 --> 0.957839).         Saving model ...\n",
      "Epoch: 1176 \tTraining Loss: 0.882840 \tValidation Loss: 0.957838\n",
      "Validation loss decreased (0.957839 --> 0.957838).         Saving model ...\n",
      "Epoch: 1177 \tTraining Loss: 0.882812 \tValidation Loss: 0.957836\n",
      "Validation loss decreased (0.957838 --> 0.957836).         Saving model ...\n",
      "Epoch: 1178 \tTraining Loss: 0.882785 \tValidation Loss: 0.957835\n",
      "Validation loss decreased (0.957836 --> 0.957835).         Saving model ...\n",
      "Epoch: 1179 \tTraining Loss: 0.882758 \tValidation Loss: 0.957833\n",
      "Validation loss decreased (0.957835 --> 0.957833).         Saving model ...\n",
      "Epoch: 1180 \tTraining Loss: 0.882731 \tValidation Loss: 0.957832\n",
      "Validation loss decreased (0.957833 --> 0.957832).         Saving model ...\n",
      "Epoch: 1181 \tTraining Loss: 0.882703 \tValidation Loss: 0.957831\n",
      "Validation loss decreased (0.957832 --> 0.957831).         Saving model ...\n",
      "Epoch: 1182 \tTraining Loss: 0.882676 \tValidation Loss: 0.957829\n",
      "Validation loss decreased (0.957831 --> 0.957829).         Saving model ...\n",
      "Epoch: 1183 \tTraining Loss: 0.882649 \tValidation Loss: 0.957828\n",
      "Validation loss decreased (0.957829 --> 0.957828).         Saving model ...\n",
      "Epoch: 1184 \tTraining Loss: 0.882622 \tValidation Loss: 0.957827\n",
      "Validation loss decreased (0.957828 --> 0.957827).         Saving model ...\n",
      "Epoch: 1185 \tTraining Loss: 0.882595 \tValidation Loss: 0.957826\n",
      "Validation loss decreased (0.957827 --> 0.957826).         Saving model ...\n",
      "Epoch: 1186 \tTraining Loss: 0.882568 \tValidation Loss: 0.957824\n",
      "Validation loss decreased (0.957826 --> 0.957824).         Saving model ...\n",
      "Epoch: 1187 \tTraining Loss: 0.882541 \tValidation Loss: 0.957823\n",
      "Validation loss decreased (0.957824 --> 0.957823).         Saving model ...\n",
      "Epoch: 1188 \tTraining Loss: 0.882514 \tValidation Loss: 0.957822\n",
      "Validation loss decreased (0.957823 --> 0.957822).         Saving model ...\n",
      "Epoch: 1189 \tTraining Loss: 0.882487 \tValidation Loss: 0.957820\n",
      "Validation loss decreased (0.957822 --> 0.957820).         Saving model ...\n",
      "Epoch: 1190 \tTraining Loss: 0.882460 \tValidation Loss: 0.957819\n",
      "Validation loss decreased (0.957820 --> 0.957819).         Saving model ...\n",
      "Epoch: 1191 \tTraining Loss: 0.882433 \tValidation Loss: 0.957818\n",
      "Validation loss decreased (0.957819 --> 0.957818).         Saving model ...\n",
      "Epoch: 1192 \tTraining Loss: 0.882406 \tValidation Loss: 0.957817\n",
      "Validation loss decreased (0.957818 --> 0.957817).         Saving model ...\n",
      "Epoch: 1193 \tTraining Loss: 0.882379 \tValidation Loss: 0.957816\n",
      "Validation loss decreased (0.957817 --> 0.957816).         Saving model ...\n",
      "Epoch: 1194 \tTraining Loss: 0.882352 \tValidation Loss: 0.957814\n",
      "Validation loss decreased (0.957816 --> 0.957814).         Saving model ...\n",
      "Epoch: 1195 \tTraining Loss: 0.882325 \tValidation Loss: 0.957813\n",
      "Validation loss decreased (0.957814 --> 0.957813).         Saving model ...\n",
      "Epoch: 1196 \tTraining Loss: 0.882298 \tValidation Loss: 0.957812\n",
      "Validation loss decreased (0.957813 --> 0.957812).         Saving model ...\n",
      "Epoch: 1197 \tTraining Loss: 0.882272 \tValidation Loss: 0.957811\n",
      "Validation loss decreased (0.957812 --> 0.957811).         Saving model ...\n",
      "Epoch: 1198 \tTraining Loss: 0.882245 \tValidation Loss: 0.957810\n",
      "Validation loss decreased (0.957811 --> 0.957810).         Saving model ...\n",
      "Epoch: 1199 \tTraining Loss: 0.882218 \tValidation Loss: 0.957808\n",
      "Validation loss decreased (0.957810 --> 0.957808).         Saving model ...\n",
      "Epoch: 1200 \tTraining Loss: 0.882191 \tValidation Loss: 0.957807\n",
      "Validation loss decreased (0.957808 --> 0.957807).         Saving model ...\n",
      "Epoch: 1201 \tTraining Loss: 0.882165 \tValidation Loss: 0.957806\n",
      "Validation loss decreased (0.957807 --> 0.957806).         Saving model ...\n",
      "Epoch: 1202 \tTraining Loss: 0.882138 \tValidation Loss: 0.957805\n",
      "Validation loss decreased (0.957806 --> 0.957805).         Saving model ...\n",
      "Epoch: 1203 \tTraining Loss: 0.882111 \tValidation Loss: 0.957804\n",
      "Validation loss decreased (0.957805 --> 0.957804).         Saving model ...\n",
      "Epoch: 1204 \tTraining Loss: 0.882085 \tValidation Loss: 0.957803\n",
      "Validation loss decreased (0.957804 --> 0.957803).         Saving model ...\n",
      "Epoch: 1205 \tTraining Loss: 0.882058 \tValidation Loss: 0.957802\n",
      "Validation loss decreased (0.957803 --> 0.957802).         Saving model ...\n",
      "Epoch: 1206 \tTraining Loss: 0.882032 \tValidation Loss: 0.957801\n",
      "Validation loss decreased (0.957802 --> 0.957801).         Saving model ...\n",
      "Epoch: 1207 \tTraining Loss: 0.882005 \tValidation Loss: 0.957800\n",
      "Validation loss decreased (0.957801 --> 0.957800).         Saving model ...\n",
      "Epoch: 1208 \tTraining Loss: 0.881979 \tValidation Loss: 0.957799\n",
      "Validation loss decreased (0.957800 --> 0.957799).         Saving model ...\n",
      "Epoch: 1209 \tTraining Loss: 0.881952 \tValidation Loss: 0.957798\n",
      "Validation loss decreased (0.957799 --> 0.957798).         Saving model ...\n",
      "Epoch: 1210 \tTraining Loss: 0.881926 \tValidation Loss: 0.957796\n",
      "Validation loss decreased (0.957798 --> 0.957796).         Saving model ...\n",
      "Epoch: 1211 \tTraining Loss: 0.881899 \tValidation Loss: 0.957795\n",
      "Validation loss decreased (0.957796 --> 0.957795).         Saving model ...\n",
      "Epoch: 1212 \tTraining Loss: 0.881873 \tValidation Loss: 0.957794\n",
      "Validation loss decreased (0.957795 --> 0.957794).         Saving model ...\n",
      "Epoch: 1213 \tTraining Loss: 0.881847 \tValidation Loss: 0.957793\n",
      "Validation loss decreased (0.957794 --> 0.957793).         Saving model ...\n",
      "Epoch: 1214 \tTraining Loss: 0.881820 \tValidation Loss: 0.957792\n",
      "Validation loss decreased (0.957793 --> 0.957792).         Saving model ...\n",
      "Epoch: 1215 \tTraining Loss: 0.881794 \tValidation Loss: 0.957791\n",
      "Validation loss decreased (0.957792 --> 0.957791).         Saving model ...\n",
      "Epoch: 1216 \tTraining Loss: 0.881768 \tValidation Loss: 0.957790\n",
      "Validation loss decreased (0.957791 --> 0.957790).         Saving model ...\n",
      "Epoch: 1217 \tTraining Loss: 0.881741 \tValidation Loss: 0.957789\n",
      "Validation loss decreased (0.957790 --> 0.957789).         Saving model ...\n",
      "Epoch: 1218 \tTraining Loss: 0.881715 \tValidation Loss: 0.957788\n",
      "Validation loss decreased (0.957789 --> 0.957788).         Saving model ...\n",
      "Epoch: 1219 \tTraining Loss: 0.881689 \tValidation Loss: 0.957787\n",
      "Validation loss decreased (0.957788 --> 0.957787).         Saving model ...\n",
      "Epoch: 1220 \tTraining Loss: 0.881663 \tValidation Loss: 0.957787\n",
      "Validation loss decreased (0.957787 --> 0.957787).         Saving model ...\n",
      "Epoch: 1221 \tTraining Loss: 0.881636 \tValidation Loss: 0.957786\n",
      "Validation loss decreased (0.957787 --> 0.957786).         Saving model ...\n",
      "Epoch: 1222 \tTraining Loss: 0.881610 \tValidation Loss: 0.957785\n",
      "Validation loss decreased (0.957786 --> 0.957785).         Saving model ...\n",
      "Epoch: 1223 \tTraining Loss: 0.881584 \tValidation Loss: 0.957784\n",
      "Validation loss decreased (0.957785 --> 0.957784).         Saving model ...\n",
      "Epoch: 1224 \tTraining Loss: 0.881558 \tValidation Loss: 0.957783\n",
      "Validation loss decreased (0.957784 --> 0.957783).         Saving model ...\n",
      "Epoch: 1225 \tTraining Loss: 0.881532 \tValidation Loss: 0.957782\n",
      "Validation loss decreased (0.957783 --> 0.957782).         Saving model ...\n",
      "Epoch: 1226 \tTraining Loss: 0.881506 \tValidation Loss: 0.957781\n",
      "Validation loss decreased (0.957782 --> 0.957781).         Saving model ...\n",
      "Epoch: 1227 \tTraining Loss: 0.881480 \tValidation Loss: 0.957780\n",
      "Validation loss decreased (0.957781 --> 0.957780).         Saving model ...\n",
      "Epoch: 1228 \tTraining Loss: 0.881454 \tValidation Loss: 0.957779\n",
      "Validation loss decreased (0.957780 --> 0.957779).         Saving model ...\n",
      "Epoch: 1229 \tTraining Loss: 0.881428 \tValidation Loss: 0.957778\n",
      "Validation loss decreased (0.957779 --> 0.957778).         Saving model ...\n",
      "Epoch: 1230 \tTraining Loss: 0.881402 \tValidation Loss: 0.957777\n",
      "Validation loss decreased (0.957778 --> 0.957777).         Saving model ...\n",
      "Epoch: 1231 \tTraining Loss: 0.881376 \tValidation Loss: 0.957777\n",
      "Validation loss decreased (0.957777 --> 0.957777).         Saving model ...\n",
      "Epoch: 1232 \tTraining Loss: 0.881350 \tValidation Loss: 0.957776\n",
      "Validation loss decreased (0.957777 --> 0.957776).         Saving model ...\n",
      "Epoch: 1233 \tTraining Loss: 0.881324 \tValidation Loss: 0.957775\n",
      "Validation loss decreased (0.957776 --> 0.957775).         Saving model ...\n",
      "Epoch: 1234 \tTraining Loss: 0.881298 \tValidation Loss: 0.957774\n",
      "Validation loss decreased (0.957775 --> 0.957774).         Saving model ...\n",
      "Epoch: 1235 \tTraining Loss: 0.881272 \tValidation Loss: 0.957773\n",
      "Validation loss decreased (0.957774 --> 0.957773).         Saving model ...\n",
      "Epoch: 1236 \tTraining Loss: 0.881246 \tValidation Loss: 0.957772\n",
      "Validation loss decreased (0.957773 --> 0.957772).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1237 \tTraining Loss: 0.881221 \tValidation Loss: 0.957772\n",
      "Validation loss decreased (0.957772 --> 0.957772).         Saving model ...\n",
      "Epoch: 1238 \tTraining Loss: 0.881195 \tValidation Loss: 0.957771\n",
      "Validation loss decreased (0.957772 --> 0.957771).         Saving model ...\n",
      "Epoch: 1239 \tTraining Loss: 0.881169 \tValidation Loss: 0.957770\n",
      "Validation loss decreased (0.957771 --> 0.957770).         Saving model ...\n",
      "Epoch: 1240 \tTraining Loss: 0.881143 \tValidation Loss: 0.957769\n",
      "Validation loss decreased (0.957770 --> 0.957769).         Saving model ...\n",
      "Epoch: 1241 \tTraining Loss: 0.881118 \tValidation Loss: 0.957769\n",
      "Validation loss decreased (0.957769 --> 0.957769).         Saving model ...\n",
      "Epoch: 1242 \tTraining Loss: 0.881092 \tValidation Loss: 0.957768\n",
      "Validation loss decreased (0.957769 --> 0.957768).         Saving model ...\n",
      "Epoch: 1243 \tTraining Loss: 0.881066 \tValidation Loss: 0.957767\n",
      "Validation loss decreased (0.957768 --> 0.957767).         Saving model ...\n",
      "Epoch: 1244 \tTraining Loss: 0.881041 \tValidation Loss: 0.957766\n",
      "Validation loss decreased (0.957767 --> 0.957766).         Saving model ...\n",
      "Epoch: 1245 \tTraining Loss: 0.881015 \tValidation Loss: 0.957766\n",
      "Validation loss decreased (0.957766 --> 0.957766).         Saving model ...\n",
      "Epoch: 1246 \tTraining Loss: 0.880990 \tValidation Loss: 0.957765\n",
      "Validation loss decreased (0.957766 --> 0.957765).         Saving model ...\n",
      "Epoch: 1247 \tTraining Loss: 0.880964 \tValidation Loss: 0.957764\n",
      "Validation loss decreased (0.957765 --> 0.957764).         Saving model ...\n",
      "Epoch: 1248 \tTraining Loss: 0.880938 \tValidation Loss: 0.957763\n",
      "Validation loss decreased (0.957764 --> 0.957763).         Saving model ...\n",
      "Epoch: 1249 \tTraining Loss: 0.880913 \tValidation Loss: 0.957763\n",
      "Validation loss decreased (0.957763 --> 0.957763).         Saving model ...\n",
      "Epoch: 1250 \tTraining Loss: 0.880887 \tValidation Loss: 0.957762\n",
      "Validation loss decreased (0.957763 --> 0.957762).         Saving model ...\n",
      "Epoch: 1251 \tTraining Loss: 0.880862 \tValidation Loss: 0.957761\n",
      "Validation loss decreased (0.957762 --> 0.957761).         Saving model ...\n",
      "Epoch: 1252 \tTraining Loss: 0.880837 \tValidation Loss: 0.957761\n",
      "Validation loss decreased (0.957761 --> 0.957761).         Saving model ...\n",
      "Epoch: 1253 \tTraining Loss: 0.880811 \tValidation Loss: 0.957760\n",
      "Validation loss decreased (0.957761 --> 0.957760).         Saving model ...\n",
      "Epoch: 1254 \tTraining Loss: 0.880786 \tValidation Loss: 0.957759\n",
      "Validation loss decreased (0.957760 --> 0.957759).         Saving model ...\n",
      "Epoch: 1255 \tTraining Loss: 0.880760 \tValidation Loss: 0.957759\n",
      "Validation loss decreased (0.957759 --> 0.957759).         Saving model ...\n",
      "Epoch: 1256 \tTraining Loss: 0.880735 \tValidation Loss: 0.957758\n",
      "Validation loss decreased (0.957759 --> 0.957758).         Saving model ...\n",
      "Epoch: 1257 \tTraining Loss: 0.880710 \tValidation Loss: 0.957758\n",
      "Validation loss decreased (0.957758 --> 0.957758).         Saving model ...\n",
      "Epoch: 1258 \tTraining Loss: 0.880684 \tValidation Loss: 0.957757\n",
      "Validation loss decreased (0.957758 --> 0.957757).         Saving model ...\n",
      "Epoch: 1259 \tTraining Loss: 0.880659 \tValidation Loss: 0.957756\n",
      "Validation loss decreased (0.957757 --> 0.957756).         Saving model ...\n",
      "Epoch: 1260 \tTraining Loss: 0.880634 \tValidation Loss: 0.957756\n",
      "Validation loss decreased (0.957756 --> 0.957756).         Saving model ...\n",
      "Epoch: 1261 \tTraining Loss: 0.880609 \tValidation Loss: 0.957755\n",
      "Validation loss decreased (0.957756 --> 0.957755).         Saving model ...\n",
      "Epoch: 1262 \tTraining Loss: 0.880583 \tValidation Loss: 0.957755\n",
      "Validation loss decreased (0.957755 --> 0.957755).         Saving model ...\n",
      "Epoch: 1263 \tTraining Loss: 0.880558 \tValidation Loss: 0.957754\n",
      "Validation loss decreased (0.957755 --> 0.957754).         Saving model ...\n",
      "Epoch: 1264 \tTraining Loss: 0.880533 \tValidation Loss: 0.957753\n",
      "Validation loss decreased (0.957754 --> 0.957753).         Saving model ...\n",
      "Epoch: 1265 \tTraining Loss: 0.880508 \tValidation Loss: 0.957753\n",
      "Validation loss decreased (0.957753 --> 0.957753).         Saving model ...\n",
      "Epoch: 1266 \tTraining Loss: 0.880483 \tValidation Loss: 0.957752\n",
      "Validation loss decreased (0.957753 --> 0.957752).         Saving model ...\n",
      "Epoch: 1267 \tTraining Loss: 0.880458 \tValidation Loss: 0.957752\n",
      "Validation loss decreased (0.957752 --> 0.957752).         Saving model ...\n",
      "Epoch: 1268 \tTraining Loss: 0.880433 \tValidation Loss: 0.957751\n",
      "Validation loss decreased (0.957752 --> 0.957751).         Saving model ...\n",
      "Epoch: 1269 \tTraining Loss: 0.880408 \tValidation Loss: 0.957751\n",
      "Validation loss decreased (0.957751 --> 0.957751).         Saving model ...\n",
      "Epoch: 1270 \tTraining Loss: 0.880382 \tValidation Loss: 0.957750\n",
      "Validation loss decreased (0.957751 --> 0.957750).         Saving model ...\n",
      "Epoch: 1271 \tTraining Loss: 0.880357 \tValidation Loss: 0.957750\n",
      "Validation loss decreased (0.957750 --> 0.957750).         Saving model ...\n",
      "Epoch: 1272 \tTraining Loss: 0.880332 \tValidation Loss: 0.957749\n",
      "Validation loss decreased (0.957750 --> 0.957749).         Saving model ...\n",
      "Epoch: 1273 \tTraining Loss: 0.880308 \tValidation Loss: 0.957749\n",
      "Validation loss decreased (0.957749 --> 0.957749).         Saving model ...\n",
      "Epoch: 1274 \tTraining Loss: 0.880283 \tValidation Loss: 0.957748\n",
      "Validation loss decreased (0.957749 --> 0.957748).         Saving model ...\n",
      "Epoch: 1275 \tTraining Loss: 0.880258 \tValidation Loss: 0.957748\n",
      "Validation loss decreased (0.957748 --> 0.957748).         Saving model ...\n",
      "Epoch: 1276 \tTraining Loss: 0.880233 \tValidation Loss: 0.957747\n",
      "Validation loss decreased (0.957748 --> 0.957747).         Saving model ...\n",
      "Epoch: 1277 \tTraining Loss: 0.880208 \tValidation Loss: 0.957747\n",
      "Validation loss decreased (0.957747 --> 0.957747).         Saving model ...\n",
      "Epoch: 1278 \tTraining Loss: 0.880183 \tValidation Loss: 0.957746\n",
      "Validation loss decreased (0.957747 --> 0.957746).         Saving model ...\n",
      "Epoch: 1279 \tTraining Loss: 0.880158 \tValidation Loss: 0.957746\n",
      "Validation loss decreased (0.957746 --> 0.957746).         Saving model ...\n",
      "Epoch: 1280 \tTraining Loss: 0.880133 \tValidation Loss: 0.957746\n",
      "Validation loss decreased (0.957746 --> 0.957746).         Saving model ...\n",
      "Epoch: 1281 \tTraining Loss: 0.880109 \tValidation Loss: 0.957745\n",
      "Validation loss decreased (0.957746 --> 0.957745).         Saving model ...\n",
      "Epoch: 1282 \tTraining Loss: 0.880084 \tValidation Loss: 0.957745\n",
      "Validation loss decreased (0.957745 --> 0.957745).         Saving model ...\n",
      "Epoch: 1283 \tTraining Loss: 0.880059 \tValidation Loss: 0.957744\n",
      "Validation loss decreased (0.957745 --> 0.957744).         Saving model ...\n",
      "Epoch: 1284 \tTraining Loss: 0.880034 \tValidation Loss: 0.957744\n",
      "Validation loss decreased (0.957744 --> 0.957744).         Saving model ...\n",
      "Epoch: 1285 \tTraining Loss: 0.880010 \tValidation Loss: 0.957743\n",
      "Validation loss decreased (0.957744 --> 0.957743).         Saving model ...\n",
      "Epoch: 1286 \tTraining Loss: 0.879985 \tValidation Loss: 0.957743\n",
      "Validation loss decreased (0.957743 --> 0.957743).         Saving model ...\n",
      "Epoch: 1287 \tTraining Loss: 0.879960 \tValidation Loss: 0.957743\n",
      "Validation loss decreased (0.957743 --> 0.957743).         Saving model ...\n",
      "Epoch: 1288 \tTraining Loss: 0.879936 \tValidation Loss: 0.957742\n",
      "Validation loss decreased (0.957743 --> 0.957742).         Saving model ...\n",
      "Epoch: 1289 \tTraining Loss: 0.879911 \tValidation Loss: 0.957742\n",
      "Validation loss decreased (0.957742 --> 0.957742).         Saving model ...\n",
      "Epoch: 1290 \tTraining Loss: 0.879887 \tValidation Loss: 0.957742\n",
      "Validation loss decreased (0.957742 --> 0.957742).         Saving model ...\n",
      "Epoch: 1291 \tTraining Loss: 0.879862 \tValidation Loss: 0.957741\n",
      "Validation loss decreased (0.957742 --> 0.957741).         Saving model ...\n",
      "Epoch: 1292 \tTraining Loss: 0.879837 \tValidation Loss: 0.957741\n",
      "Validation loss decreased (0.957741 --> 0.957741).         Saving model ...\n",
      "Epoch: 1293 \tTraining Loss: 0.879813 \tValidation Loss: 0.957741\n",
      "Validation loss decreased (0.957741 --> 0.957741).         Saving model ...\n",
      "Epoch: 1294 \tTraining Loss: 0.879788 \tValidation Loss: 0.957740\n",
      "Validation loss decreased (0.957741 --> 0.957740).         Saving model ...\n",
      "Epoch: 1295 \tTraining Loss: 0.879764 \tValidation Loss: 0.957740\n",
      "Validation loss decreased (0.957740 --> 0.957740).         Saving model ...\n",
      "Epoch: 1296 \tTraining Loss: 0.879740 \tValidation Loss: 0.957740\n",
      "Validation loss decreased (0.957740 --> 0.957740).         Saving model ...\n",
      "Epoch: 1297 \tTraining Loss: 0.879715 \tValidation Loss: 0.957739\n",
      "Validation loss decreased (0.957740 --> 0.957739).         Saving model ...\n",
      "Epoch: 1298 \tTraining Loss: 0.879691 \tValidation Loss: 0.957739\n",
      "Validation loss decreased (0.957739 --> 0.957739).         Saving model ...\n",
      "Epoch: 1299 \tTraining Loss: 0.879666 \tValidation Loss: 0.957739\n",
      "Validation loss decreased (0.957739 --> 0.957739).         Saving model ...\n",
      "Epoch: 1300 \tTraining Loss: 0.879642 \tValidation Loss: 0.957739\n",
      "Validation loss decreased (0.957739 --> 0.957739).         Saving model ...\n",
      "Epoch: 1301 \tTraining Loss: 0.879618 \tValidation Loss: 0.957738\n",
      "Validation loss decreased (0.957739 --> 0.957738).         Saving model ...\n",
      "Epoch: 1302 \tTraining Loss: 0.879593 \tValidation Loss: 0.957738\n",
      "Validation loss decreased (0.957738 --> 0.957738).         Saving model ...\n",
      "Epoch: 1303 \tTraining Loss: 0.879569 \tValidation Loss: 0.957738\n",
      "Validation loss decreased (0.957738 --> 0.957738).         Saving model ...\n",
      "Epoch: 1304 \tTraining Loss: 0.879545 \tValidation Loss: 0.957737\n",
      "Validation loss decreased (0.957738 --> 0.957737).         Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1305 \tTraining Loss: 0.879520 \tValidation Loss: 0.957737\n",
      "Validation loss decreased (0.957737 --> 0.957737).         Saving model ...\n",
      "Epoch: 1306 \tTraining Loss: 0.879496 \tValidation Loss: 0.957737\n",
      "Validation loss decreased (0.957737 --> 0.957737).         Saving model ...\n",
      "Epoch: 1307 \tTraining Loss: 0.879472 \tValidation Loss: 0.957737\n",
      "Validation loss decreased (0.957737 --> 0.957737).         Saving model ...\n",
      "Epoch: 1308 \tTraining Loss: 0.879448 \tValidation Loss: 0.957737\n",
      "Validation loss decreased (0.957737 --> 0.957737).         Saving model ...\n",
      "Epoch: 1309 \tTraining Loss: 0.879424 \tValidation Loss: 0.957736\n",
      "Validation loss decreased (0.957737 --> 0.957736).         Saving model ...\n",
      "Epoch: 1310 \tTraining Loss: 0.879399 \tValidation Loss: 0.957736\n",
      "Validation loss decreased (0.957736 --> 0.957736).         Saving model ...\n",
      "Epoch: 1311 \tTraining Loss: 0.879375 \tValidation Loss: 0.957736\n",
      "Validation loss decreased (0.957736 --> 0.957736).         Saving model ...\n",
      "Epoch: 1312 \tTraining Loss: 0.879351 \tValidation Loss: 0.957736\n",
      "Validation loss decreased (0.957736 --> 0.957736).         Saving model ...\n",
      "Epoch: 1313 \tTraining Loss: 0.879327 \tValidation Loss: 0.957736\n",
      "Validation loss decreased (0.957736 --> 0.957736).         Saving model ...\n",
      "Epoch: 1314 \tTraining Loss: 0.879303 \tValidation Loss: 0.957736\n",
      "Validation loss decreased (0.957736 --> 0.957736).         Saving model ...\n",
      "Epoch: 1315 \tTraining Loss: 0.879279 \tValidation Loss: 0.957735\n",
      "Validation loss decreased (0.957736 --> 0.957735).         Saving model ...\n",
      "Epoch: 1316 \tTraining Loss: 0.879255 \tValidation Loss: 0.957735\n",
      "Validation loss decreased (0.957735 --> 0.957735).         Saving model ...\n",
      "Epoch: 1317 \tTraining Loss: 0.879231 \tValidation Loss: 0.957735\n",
      "Validation loss decreased (0.957735 --> 0.957735).         Saving model ...\n",
      "Epoch: 1318 \tTraining Loss: 0.879207 \tValidation Loss: 0.957735\n",
      "Validation loss decreased (0.957735 --> 0.957735).         Saving model ...\n",
      "Epoch: 1319 \tTraining Loss: 0.879183 \tValidation Loss: 0.957735\n",
      "Validation loss decreased (0.957735 --> 0.957735).         Saving model ...\n",
      "Epoch: 1320 \tTraining Loss: 0.879159 \tValidation Loss: 0.957735\n",
      "Validation loss decreased (0.957735 --> 0.957735).         Saving model ...\n",
      "Epoch: 1321 \tTraining Loss: 0.879135 \tValidation Loss: 0.957735\n",
      "Validation loss decreased (0.957735 --> 0.957735).         Saving model ...\n",
      "Epoch: 1322 \tTraining Loss: 0.879111 \tValidation Loss: 0.957734\n",
      "Validation loss decreased (0.957735 --> 0.957734).         Saving model ...\n",
      "Epoch: 1323 \tTraining Loss: 0.879087 \tValidation Loss: 0.957734\n",
      "Validation loss decreased (0.957734 --> 0.957734).         Saving model ...\n",
      "Epoch: 1324 \tTraining Loss: 0.879064 \tValidation Loss: 0.957734\n",
      "Validation loss decreased (0.957734 --> 0.957734).         Saving model ...\n",
      "Epoch: 1325 \tTraining Loss: 0.879040 \tValidation Loss: 0.957734\n",
      "Validation loss decreased (0.957734 --> 0.957734).         Saving model ...\n",
      "Epoch: 1326 \tTraining Loss: 0.879016 \tValidation Loss: 0.957734\n",
      "Validation loss decreased (0.957734 --> 0.957734).         Saving model ...\n",
      "Epoch: 1327 \tTraining Loss: 0.878992 \tValidation Loss: 0.957734\n",
      "Validation loss decreased (0.957734 --> 0.957734).         Saving model ...\n",
      "Epoch: 1328 \tTraining Loss: 0.878968 \tValidation Loss: 0.957734\n",
      "Validation loss decreased (0.957734 --> 0.957734).         Saving model ...\n",
      "Epoch: 1329 \tTraining Loss: 0.878945 \tValidation Loss: 0.957734\n",
      "Validation loss decreased (0.957734 --> 0.957734).         Saving model ...\n",
      "Epoch: 1330 \tTraining Loss: 0.878921 \tValidation Loss: 0.957734\n",
      "Validation loss decreased (0.957734 --> 0.957734).         Saving model ...\n",
      "Epoch: 1331 \tTraining Loss: 0.878897 \tValidation Loss: 0.957734\n",
      "Validation loss decreased (0.957734 --> 0.957734).         Saving model ...\n",
      "Epoch: 1332 \tTraining Loss: 0.878873 \tValidation Loss: 0.957734\n",
      "Validation loss decreased (0.957734 --> 0.957734).         Saving model ...\n",
      "Epoch: 1333 \tTraining Loss: 0.878850 \tValidation Loss: 0.957734\n",
      "Epoch: 1334 \tTraining Loss: 0.878826 \tValidation Loss: 0.957734\n",
      "Validation loss decreased (0.957734 --> 0.957734).         Saving model ...\n",
      "Epoch: 1335 \tTraining Loss: 0.878802 \tValidation Loss: 0.957734\n",
      "Epoch: 1336 \tTraining Loss: 0.878779 \tValidation Loss: 0.957734\n",
      "Epoch: 1337 \tTraining Loss: 0.878755 \tValidation Loss: 0.957734\n",
      "Epoch: 1338 \tTraining Loss: 0.878732 \tValidation Loss: 0.957734\n",
      "Validation loss decreased (0.957734 --> 0.957734).         Saving model ...\n",
      "Epoch: 1339 \tTraining Loss: 0.878708 \tValidation Loss: 0.957734\n",
      "Epoch: 1340 \tTraining Loss: 0.878685 \tValidation Loss: 0.957734\n",
      "Epoch: 1341 \tTraining Loss: 0.878661 \tValidation Loss: 0.957734\n",
      "Epoch: 1342 \tTraining Loss: 0.878638 \tValidation Loss: 0.957734\n",
      "Epoch: 1343 \tTraining Loss: 0.878614 \tValidation Loss: 0.957734\n",
      "Epoch: 1344 \tTraining Loss: 0.878591 \tValidation Loss: 0.957734\n",
      "Epoch: 1345 \tTraining Loss: 0.878567 \tValidation Loss: 0.957734\n",
      "Epoch: 1346 \tTraining Loss: 0.878544 \tValidation Loss: 0.957734\n",
      "Epoch: 1347 \tTraining Loss: 0.878520 \tValidation Loss: 0.957734\n",
      "Epoch: 1348 \tTraining Loss: 0.878497 \tValidation Loss: 0.957734\n",
      "Epoch: 1349 \tTraining Loss: 0.878474 \tValidation Loss: 0.957735\n",
      "Epoch: 1350 \tTraining Loss: 0.878450 \tValidation Loss: 0.957735\n",
      "Epoch: 1351 \tTraining Loss: 0.878427 \tValidation Loss: 0.957735\n",
      "Epoch: 1352 \tTraining Loss: 0.878404 \tValidation Loss: 0.957735\n",
      "Epoch: 1353 \tTraining Loss: 0.878381 \tValidation Loss: 0.957735\n",
      "Epoch: 1354 \tTraining Loss: 0.878357 \tValidation Loss: 0.957735\n",
      "Epoch: 1355 \tTraining Loss: 0.878334 \tValidation Loss: 0.957735\n",
      "Epoch: 1356 \tTraining Loss: 0.878311 \tValidation Loss: 0.957735\n",
      "Epoch: 1357 \tTraining Loss: 0.878288 \tValidation Loss: 0.957736\n",
      "Epoch: 1358 \tTraining Loss: 0.878265 \tValidation Loss: 0.957736\n",
      "Epoch: 1359 \tTraining Loss: 0.878241 \tValidation Loss: 0.957736\n",
      "Epoch: 1360 \tTraining Loss: 0.878218 \tValidation Loss: 0.957736\n",
      "Epoch: 1361 \tTraining Loss: 0.878195 \tValidation Loss: 0.957736\n",
      "Epoch: 1362 \tTraining Loss: 0.878172 \tValidation Loss: 0.957736\n",
      "Epoch: 1363 \tTraining Loss: 0.878149 \tValidation Loss: 0.957737\n",
      "Epoch: 1364 \tTraining Loss: 0.878126 \tValidation Loss: 0.957737\n",
      "Epoch: 1365 \tTraining Loss: 0.878103 \tValidation Loss: 0.957737\n",
      "Epoch: 1366 \tTraining Loss: 0.878080 \tValidation Loss: 0.957737\n",
      "Epoch: 1367 \tTraining Loss: 0.878057 \tValidation Loss: 0.957737\n",
      "Epoch: 1368 \tTraining Loss: 0.878034 \tValidation Loss: 0.957738\n",
      "Epoch: 1369 \tTraining Loss: 0.878011 \tValidation Loss: 0.957738\n",
      "Epoch: 1370 \tTraining Loss: 0.877988 \tValidation Loss: 0.957738\n",
      "Epoch: 1371 \tTraining Loss: 0.877965 \tValidation Loss: 0.957738\n",
      "Epoch: 1372 \tTraining Loss: 0.877942 \tValidation Loss: 0.957739\n",
      "Epoch: 1373 \tTraining Loss: 0.877919 \tValidation Loss: 0.957739\n",
      "Epoch: 1374 \tTraining Loss: 0.877896 \tValidation Loss: 0.957739\n",
      "Epoch: 1375 \tTraining Loss: 0.877874 \tValidation Loss: 0.957740\n",
      "Epoch: 1376 \tTraining Loss: 0.877851 \tValidation Loss: 0.957740\n",
      "Epoch: 1377 \tTraining Loss: 0.877828 \tValidation Loss: 0.957740\n",
      "Epoch: 1378 \tTraining Loss: 0.877805 \tValidation Loss: 0.957740\n",
      "Epoch: 1379 \tTraining Loss: 0.877783 \tValidation Loss: 0.957741\n",
      "Epoch: 1380 \tTraining Loss: 0.877760 \tValidation Loss: 0.957741\n",
      "Epoch: 1381 \tTraining Loss: 0.877737 \tValidation Loss: 0.957741\n",
      "Epoch: 1382 \tTraining Loss: 0.877714 \tValidation Loss: 0.957742\n",
      "Epoch: 1383 \tTraining Loss: 0.877692 \tValidation Loss: 0.957742\n",
      "Epoch: 1384 \tTraining Loss: 0.877669 \tValidation Loss: 0.957742\n",
      "Epoch: 1385 \tTraining Loss: 0.877646 \tValidation Loss: 0.957743\n",
      "Epoch: 1386 \tTraining Loss: 0.877624 \tValidation Loss: 0.957743\n",
      "Epoch: 1387 \tTraining Loss: 0.877601 \tValidation Loss: 0.957743\n",
      "Epoch: 1388 \tTraining Loss: 0.877579 \tValidation Loss: 0.957744\n",
      "Epoch: 1389 \tTraining Loss: 0.877556 \tValidation Loss: 0.957744\n",
      "Epoch: 1390 \tTraining Loss: 0.877533 \tValidation Loss: 0.957745\n",
      "Epoch: 1391 \tTraining Loss: 0.877511 \tValidation Loss: 0.957745\n",
      "Epoch: 1392 \tTraining Loss: 0.877488 \tValidation Loss: 0.957745\n",
      "Epoch: 1393 \tTraining Loss: 0.877466 \tValidation Loss: 0.957746\n",
      "Epoch: 1394 \tTraining Loss: 0.877443 \tValidation Loss: 0.957746\n",
      "Epoch: 1395 \tTraining Loss: 0.877421 \tValidation Loss: 0.957746\n",
      "Epoch: 1396 \tTraining Loss: 0.877398 \tValidation Loss: 0.957747\n",
      "Epoch: 1397 \tTraining Loss: 0.877376 \tValidation Loss: 0.957747\n",
      "Epoch: 1398 \tTraining Loss: 0.877354 \tValidation Loss: 0.957748\n",
      "Epoch: 1399 \tTraining Loss: 0.877331 \tValidation Loss: 0.957748\n",
      "Epoch: 1400 \tTraining Loss: 0.877309 \tValidation Loss: 0.957749\n",
      "Epoch: 1401 \tTraining Loss: 0.877287 \tValidation Loss: 0.957749\n",
      "Epoch: 1402 \tTraining Loss: 0.877264 \tValidation Loss: 0.957750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1403 \tTraining Loss: 0.877242 \tValidation Loss: 0.957750\n",
      "Epoch: 1404 \tTraining Loss: 0.877220 \tValidation Loss: 0.957751\n",
      "Epoch: 1405 \tTraining Loss: 0.877197 \tValidation Loss: 0.957751\n",
      "Epoch: 1406 \tTraining Loss: 0.877175 \tValidation Loss: 0.957751\n",
      "Epoch: 1407 \tTraining Loss: 0.877153 \tValidation Loss: 0.957752\n",
      "Epoch: 1408 \tTraining Loss: 0.877131 \tValidation Loss: 0.957752\n",
      "Epoch: 1409 \tTraining Loss: 0.877108 \tValidation Loss: 0.957753\n",
      "Epoch: 1410 \tTraining Loss: 0.877086 \tValidation Loss: 0.957753\n",
      "Epoch: 1411 \tTraining Loss: 0.877064 \tValidation Loss: 0.957754\n",
      "Epoch: 1412 \tTraining Loss: 0.877042 \tValidation Loss: 0.957755\n",
      "Epoch: 1413 \tTraining Loss: 0.877020 \tValidation Loss: 0.957755\n",
      "Epoch: 1414 \tTraining Loss: 0.876998 \tValidation Loss: 0.957756\n",
      "Epoch: 1415 \tTraining Loss: 0.876976 \tValidation Loss: 0.957756\n",
      "Epoch: 1416 \tTraining Loss: 0.876954 \tValidation Loss: 0.957757\n",
      "Epoch: 1417 \tTraining Loss: 0.876932 \tValidation Loss: 0.957757\n",
      "Epoch: 1418 \tTraining Loss: 0.876910 \tValidation Loss: 0.957758\n",
      "Epoch: 1419 \tTraining Loss: 0.876887 \tValidation Loss: 0.957758\n",
      "Epoch: 1420 \tTraining Loss: 0.876865 \tValidation Loss: 0.957759\n",
      "Epoch: 1421 \tTraining Loss: 0.876844 \tValidation Loss: 0.957760\n",
      "Epoch: 1422 \tTraining Loss: 0.876822 \tValidation Loss: 0.957760\n",
      "Epoch: 1423 \tTraining Loss: 0.876800 \tValidation Loss: 0.957761\n",
      "Epoch: 1424 \tTraining Loss: 0.876778 \tValidation Loss: 0.957761\n",
      "Epoch: 1425 \tTraining Loss: 0.876756 \tValidation Loss: 0.957762\n",
      "Epoch: 1426 \tTraining Loss: 0.876734 \tValidation Loss: 0.957762\n",
      "Epoch: 1427 \tTraining Loss: 0.876712 \tValidation Loss: 0.957763\n",
      "Epoch: 1428 \tTraining Loss: 0.876690 \tValidation Loss: 0.957764\n",
      "Epoch: 1429 \tTraining Loss: 0.876668 \tValidation Loss: 0.957764\n",
      "Epoch: 1430 \tTraining Loss: 0.876646 \tValidation Loss: 0.957765\n",
      "Epoch: 1431 \tTraining Loss: 0.876625 \tValidation Loss: 0.957766\n",
      "Epoch: 1432 \tTraining Loss: 0.876603 \tValidation Loss: 0.957766\n",
      "Epoch: 1433 \tTraining Loss: 0.876581 \tValidation Loss: 0.957767\n",
      "Epoch: 1434 \tTraining Loss: 0.876559 \tValidation Loss: 0.957768\n",
      "Epoch: 1435 \tTraining Loss: 0.876538 \tValidation Loss: 0.957768\n",
      "Epoch: 1436 \tTraining Loss: 0.876516 \tValidation Loss: 0.957769\n",
      "Epoch: 1437 \tTraining Loss: 0.876494 \tValidation Loss: 0.957770\n",
      "Epoch: 1438 \tTraining Loss: 0.876473 \tValidation Loss: 0.957770\n",
      "Epoch: 1439 \tTraining Loss: 0.876451 \tValidation Loss: 0.957771\n",
      "Epoch: 1440 \tTraining Loss: 0.876429 \tValidation Loss: 0.957772\n",
      "Epoch: 1441 \tTraining Loss: 0.876408 \tValidation Loss: 0.957772\n",
      "Epoch: 1442 \tTraining Loss: 0.876386 \tValidation Loss: 0.957773\n",
      "Epoch: 1443 \tTraining Loss: 0.876364 \tValidation Loss: 0.957774\n",
      "Epoch: 1444 \tTraining Loss: 0.876343 \tValidation Loss: 0.957775\n",
      "Epoch: 1445 \tTraining Loss: 0.876321 \tValidation Loss: 0.957775\n",
      "Epoch: 1446 \tTraining Loss: 0.876300 \tValidation Loss: 0.957776\n",
      "Epoch: 1447 \tTraining Loss: 0.876278 \tValidation Loss: 0.957777\n",
      "Epoch: 1448 \tTraining Loss: 0.876257 \tValidation Loss: 0.957777\n",
      "Epoch: 1449 \tTraining Loss: 0.876235 \tValidation Loss: 0.957778\n",
      "Epoch: 1450 \tTraining Loss: 0.876214 \tValidation Loss: 0.957779\n",
      "Epoch: 1451 \tTraining Loss: 0.876192 \tValidation Loss: 0.957780\n",
      "Epoch: 1452 \tTraining Loss: 0.876171 \tValidation Loss: 0.957781\n",
      "Epoch: 1453 \tTraining Loss: 0.876150 \tValidation Loss: 0.957781\n",
      "Epoch: 1454 \tTraining Loss: 0.876128 \tValidation Loss: 0.957782\n",
      "Epoch: 1455 \tTraining Loss: 0.876107 \tValidation Loss: 0.957783\n",
      "Epoch: 1456 \tTraining Loss: 0.876085 \tValidation Loss: 0.957784\n",
      "Epoch: 1457 \tTraining Loss: 0.876064 \tValidation Loss: 0.957784\n",
      "Epoch: 1458 \tTraining Loss: 0.876043 \tValidation Loss: 0.957785\n",
      "Epoch: 1459 \tTraining Loss: 0.876021 \tValidation Loss: 0.957786\n",
      "Epoch: 1460 \tTraining Loss: 0.876000 \tValidation Loss: 0.957787\n",
      "Epoch: 1461 \tTraining Loss: 0.875979 \tValidation Loss: 0.957788\n",
      "Epoch: 1462 \tTraining Loss: 0.875958 \tValidation Loss: 0.957788\n",
      "Epoch: 1463 \tTraining Loss: 0.875936 \tValidation Loss: 0.957789\n",
      "Epoch: 1464 \tTraining Loss: 0.875915 \tValidation Loss: 0.957790\n",
      "Epoch: 1465 \tTraining Loss: 0.875894 \tValidation Loss: 0.957791\n",
      "Epoch: 1466 \tTraining Loss: 0.875873 \tValidation Loss: 0.957792\n",
      "Epoch: 1467 \tTraining Loss: 0.875851 \tValidation Loss: 0.957793\n",
      "Epoch: 1468 \tTraining Loss: 0.875830 \tValidation Loss: 0.957794\n",
      "Epoch: 1469 \tTraining Loss: 0.875809 \tValidation Loss: 0.957794\n",
      "Epoch: 1470 \tTraining Loss: 0.875788 \tValidation Loss: 0.957795\n",
      "Epoch: 1471 \tTraining Loss: 0.875767 \tValidation Loss: 0.957796\n",
      "Epoch: 1472 \tTraining Loss: 0.875746 \tValidation Loss: 0.957797\n",
      "Epoch: 1473 \tTraining Loss: 0.875725 \tValidation Loss: 0.957798\n",
      "Epoch: 1474 \tTraining Loss: 0.875704 \tValidation Loss: 0.957799\n",
      "Epoch: 1475 \tTraining Loss: 0.875683 \tValidation Loss: 0.957800\n",
      "Epoch: 1476 \tTraining Loss: 0.875662 \tValidation Loss: 0.957801\n",
      "Epoch: 1477 \tTraining Loss: 0.875641 \tValidation Loss: 0.957802\n",
      "Epoch: 1478 \tTraining Loss: 0.875620 \tValidation Loss: 0.957803\n",
      "Epoch: 1479 \tTraining Loss: 0.875599 \tValidation Loss: 0.957803\n",
      "Epoch: 1480 \tTraining Loss: 0.875578 \tValidation Loss: 0.957804\n",
      "Epoch: 1481 \tTraining Loss: 0.875557 \tValidation Loss: 0.957805\n",
      "Epoch: 1482 \tTraining Loss: 0.875536 \tValidation Loss: 0.957806\n",
      "Epoch: 1483 \tTraining Loss: 0.875515 \tValidation Loss: 0.957807\n",
      "Epoch: 1484 \tTraining Loss: 0.875494 \tValidation Loss: 0.957808\n",
      "Epoch: 1485 \tTraining Loss: 0.875473 \tValidation Loss: 0.957809\n",
      "Epoch: 1486 \tTraining Loss: 0.875452 \tValidation Loss: 0.957810\n",
      "Epoch: 1487 \tTraining Loss: 0.875431 \tValidation Loss: 0.957811\n",
      "Epoch: 1488 \tTraining Loss: 0.875411 \tValidation Loss: 0.957812\n",
      "Epoch: 1489 \tTraining Loss: 0.875390 \tValidation Loss: 0.957813\n",
      "Epoch: 1490 \tTraining Loss: 0.875369 \tValidation Loss: 0.957814\n",
      "Epoch: 1491 \tTraining Loss: 0.875348 \tValidation Loss: 0.957815\n",
      "Epoch: 1492 \tTraining Loss: 0.875327 \tValidation Loss: 0.957816\n",
      "Epoch: 1493 \tTraining Loss: 0.875307 \tValidation Loss: 0.957817\n",
      "Epoch: 1494 \tTraining Loss: 0.875286 \tValidation Loss: 0.957818\n",
      "Epoch: 1495 \tTraining Loss: 0.875265 \tValidation Loss: 0.957819\n",
      "Epoch: 1496 \tTraining Loss: 0.875245 \tValidation Loss: 0.957820\n",
      "Epoch: 1497 \tTraining Loss: 0.875224 \tValidation Loss: 0.957821\n",
      "Epoch: 1498 \tTraining Loss: 0.875203 \tValidation Loss: 0.957822\n",
      "Epoch: 1499 \tTraining Loss: 0.875183 \tValidation Loss: 0.957823\n",
      "Epoch: 1500 \tTraining Loss: 0.875162 \tValidation Loss: 0.957824\n",
      "Epoch: 1501 \tTraining Loss: 0.875141 \tValidation Loss: 0.957825\n",
      "Epoch: 1502 \tTraining Loss: 0.875121 \tValidation Loss: 0.957826\n",
      "Epoch: 1503 \tTraining Loss: 0.875100 \tValidation Loss: 0.957827\n",
      "Epoch: 1504 \tTraining Loss: 0.875080 \tValidation Loss: 0.957828\n",
      "Epoch: 1505 \tTraining Loss: 0.875059 \tValidation Loss: 0.957829\n",
      "Epoch: 1506 \tTraining Loss: 0.875038 \tValidation Loss: 0.957830\n",
      "Epoch: 1507 \tTraining Loss: 0.875018 \tValidation Loss: 0.957832\n",
      "Epoch: 1508 \tTraining Loss: 0.874997 \tValidation Loss: 0.957833\n",
      "Epoch: 1509 \tTraining Loss: 0.874977 \tValidation Loss: 0.957834\n",
      "Epoch: 1510 \tTraining Loss: 0.874956 \tValidation Loss: 0.957835\n",
      "Epoch: 1511 \tTraining Loss: 0.874936 \tValidation Loss: 0.957836\n",
      "Epoch: 1512 \tTraining Loss: 0.874916 \tValidation Loss: 0.957837\n",
      "Epoch: 1513 \tTraining Loss: 0.874895 \tValidation Loss: 0.957838\n",
      "Epoch: 1514 \tTraining Loss: 0.874875 \tValidation Loss: 0.957839\n",
      "Epoch: 1515 \tTraining Loss: 0.874854 \tValidation Loss: 0.957840\n",
      "Epoch: 1516 \tTraining Loss: 0.874834 \tValidation Loss: 0.957842\n",
      "Epoch: 1517 \tTraining Loss: 0.874814 \tValidation Loss: 0.957843\n",
      "Epoch: 1518 \tTraining Loss: 0.874793 \tValidation Loss: 0.957844\n",
      "Epoch: 1519 \tTraining Loss: 0.874773 \tValidation Loss: 0.957845\n",
      "Epoch: 1520 \tTraining Loss: 0.874753 \tValidation Loss: 0.957846\n",
      "Epoch: 1521 \tTraining Loss: 0.874732 \tValidation Loss: 0.957847\n",
      "Epoch: 1522 \tTraining Loss: 0.874712 \tValidation Loss: 0.957848\n",
      "Epoch: 1523 \tTraining Loss: 0.874692 \tValidation Loss: 0.957850\n",
      "Epoch: 1524 \tTraining Loss: 0.874672 \tValidation Loss: 0.957851\n",
      "Epoch: 1525 \tTraining Loss: 0.874651 \tValidation Loss: 0.957852\n",
      "Epoch: 1526 \tTraining Loss: 0.874631 \tValidation Loss: 0.957853\n",
      "Epoch: 1527 \tTraining Loss: 0.874611 \tValidation Loss: 0.957854\n",
      "Epoch: 1528 \tTraining Loss: 0.874591 \tValidation Loss: 0.957855\n",
      "Epoch: 1529 \tTraining Loss: 0.874571 \tValidation Loss: 0.957857\n",
      "Epoch: 1530 \tTraining Loss: 0.874550 \tValidation Loss: 0.957858\n",
      "Epoch: 1531 \tTraining Loss: 0.874530 \tValidation Loss: 0.957859\n",
      "Epoch: 1532 \tTraining Loss: 0.874510 \tValidation Loss: 0.957860\n",
      "Epoch: 1533 \tTraining Loss: 0.874490 \tValidation Loss: 0.957861\n",
      "Epoch: 1534 \tTraining Loss: 0.874470 \tValidation Loss: 0.957863\n",
      "Epoch: 1535 \tTraining Loss: 0.874450 \tValidation Loss: 0.957864\n",
      "Epoch: 1536 \tTraining Loss: 0.874430 \tValidation Loss: 0.957865\n",
      "Epoch: 1537 \tTraining Loss: 0.874410 \tValidation Loss: 0.957866\n",
      "Epoch: 1538 \tTraining Loss: 0.874390 \tValidation Loss: 0.957868\n",
      "Epoch: 1539 \tTraining Loss: 0.874370 \tValidation Loss: 0.957869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1540 \tTraining Loss: 0.874350 \tValidation Loss: 0.957870\n",
      "Epoch: 1541 \tTraining Loss: 0.874330 \tValidation Loss: 0.957871\n",
      "Epoch: 1542 \tTraining Loss: 0.874310 \tValidation Loss: 0.957873\n",
      "Epoch: 1543 \tTraining Loss: 0.874290 \tValidation Loss: 0.957874\n",
      "Epoch: 1544 \tTraining Loss: 0.874270 \tValidation Loss: 0.957875\n",
      "Epoch: 1545 \tTraining Loss: 0.874250 \tValidation Loss: 0.957876\n",
      "Epoch: 1546 \tTraining Loss: 0.874230 \tValidation Loss: 0.957878\n",
      "Epoch: 1547 \tTraining Loss: 0.874210 \tValidation Loss: 0.957879\n",
      "Epoch: 1548 \tTraining Loss: 0.874190 \tValidation Loss: 0.957880\n",
      "Epoch: 1549 \tTraining Loss: 0.874170 \tValidation Loss: 0.957882\n",
      "Epoch: 1550 \tTraining Loss: 0.874150 \tValidation Loss: 0.957883\n",
      "Epoch: 1551 \tTraining Loss: 0.874131 \tValidation Loss: 0.957884\n",
      "Epoch: 1552 \tTraining Loss: 0.874111 \tValidation Loss: 0.957886\n",
      "Epoch: 1553 \tTraining Loss: 0.874091 \tValidation Loss: 0.957887\n",
      "Epoch: 1554 \tTraining Loss: 0.874071 \tValidation Loss: 0.957888\n",
      "Epoch: 1555 \tTraining Loss: 0.874051 \tValidation Loss: 0.957889\n",
      "Epoch: 1556 \tTraining Loss: 0.874032 \tValidation Loss: 0.957891\n",
      "Epoch: 1557 \tTraining Loss: 0.874012 \tValidation Loss: 0.957892\n",
      "Epoch: 1558 \tTraining Loss: 0.873992 \tValidation Loss: 0.957893\n",
      "Epoch: 1559 \tTraining Loss: 0.873972 \tValidation Loss: 0.957895\n",
      "Epoch: 1560 \tTraining Loss: 0.873953 \tValidation Loss: 0.957896\n",
      "Epoch: 1561 \tTraining Loss: 0.873933 \tValidation Loss: 0.957898\n",
      "Epoch: 1562 \tTraining Loss: 0.873913 \tValidation Loss: 0.957899\n",
      "Epoch: 1563 \tTraining Loss: 0.873894 \tValidation Loss: 0.957900\n",
      "Epoch: 1564 \tTraining Loss: 0.873874 \tValidation Loss: 0.957902\n",
      "Epoch: 1565 \tTraining Loss: 0.873854 \tValidation Loss: 0.957903\n",
      "Epoch: 1566 \tTraining Loss: 0.873835 \tValidation Loss: 0.957904\n",
      "Epoch: 1567 \tTraining Loss: 0.873815 \tValidation Loss: 0.957906\n",
      "Epoch: 1568 \tTraining Loss: 0.873796 \tValidation Loss: 0.957907\n",
      "Epoch: 1569 \tTraining Loss: 0.873776 \tValidation Loss: 0.957909\n",
      "Epoch: 1570 \tTraining Loss: 0.873756 \tValidation Loss: 0.957910\n",
      "Epoch: 1571 \tTraining Loss: 0.873737 \tValidation Loss: 0.957911\n",
      "Epoch: 1572 \tTraining Loss: 0.873717 \tValidation Loss: 0.957913\n",
      "Epoch: 1573 \tTraining Loss: 0.873698 \tValidation Loss: 0.957914\n",
      "Epoch: 1574 \tTraining Loss: 0.873678 \tValidation Loss: 0.957916\n",
      "Epoch: 1575 \tTraining Loss: 0.873659 \tValidation Loss: 0.957917\n",
      "Epoch: 1576 \tTraining Loss: 0.873639 \tValidation Loss: 0.957918\n",
      "Epoch: 1577 \tTraining Loss: 0.873620 \tValidation Loss: 0.957920\n",
      "Epoch: 1578 \tTraining Loss: 0.873601 \tValidation Loss: 0.957921\n",
      "Epoch: 1579 \tTraining Loss: 0.873581 \tValidation Loss: 0.957923\n",
      "Epoch: 1580 \tTraining Loss: 0.873562 \tValidation Loss: 0.957924\n",
      "Epoch: 1581 \tTraining Loss: 0.873542 \tValidation Loss: 0.957926\n",
      "Epoch: 1582 \tTraining Loss: 0.873523 \tValidation Loss: 0.957927\n",
      "Epoch: 1583 \tTraining Loss: 0.873504 \tValidation Loss: 0.957929\n",
      "Epoch: 1584 \tTraining Loss: 0.873484 \tValidation Loss: 0.957930\n",
      "Epoch: 1585 \tTraining Loss: 0.873465 \tValidation Loss: 0.957932\n",
      "Epoch: 1586 \tTraining Loss: 0.873446 \tValidation Loss: 0.957933\n",
      "Epoch: 1587 \tTraining Loss: 0.873426 \tValidation Loss: 0.957934\n",
      "Epoch: 1588 \tTraining Loss: 0.873407 \tValidation Loss: 0.957936\n",
      "Epoch: 1589 \tTraining Loss: 0.873388 \tValidation Loss: 0.957937\n",
      "Epoch: 1590 \tTraining Loss: 0.873368 \tValidation Loss: 0.957939\n",
      "Epoch: 1591 \tTraining Loss: 0.873349 \tValidation Loss: 0.957940\n",
      "Epoch: 1592 \tTraining Loss: 0.873330 \tValidation Loss: 0.957942\n",
      "Epoch: 1593 \tTraining Loss: 0.873311 \tValidation Loss: 0.957943\n",
      "Epoch: 1594 \tTraining Loss: 0.873292 \tValidation Loss: 0.957945\n",
      "Epoch: 1595 \tTraining Loss: 0.873272 \tValidation Loss: 0.957947\n",
      "Epoch: 1596 \tTraining Loss: 0.873253 \tValidation Loss: 0.957948\n",
      "Epoch: 1597 \tTraining Loss: 0.873234 \tValidation Loss: 0.957950\n",
      "Epoch: 1598 \tTraining Loss: 0.873215 \tValidation Loss: 0.957951\n",
      "Epoch: 1599 \tTraining Loss: 0.873196 \tValidation Loss: 0.957953\n",
      "Epoch: 1600 \tTraining Loss: 0.873177 \tValidation Loss: 0.957954\n",
      "Epoch: 1601 \tTraining Loss: 0.873157 \tValidation Loss: 0.957956\n",
      "Epoch: 1602 \tTraining Loss: 0.873138 \tValidation Loss: 0.957957\n",
      "Epoch: 1603 \tTraining Loss: 0.873119 \tValidation Loss: 0.957959\n",
      "Epoch: 1604 \tTraining Loss: 0.873100 \tValidation Loss: 0.957960\n",
      "Epoch: 1605 \tTraining Loss: 0.873081 \tValidation Loss: 0.957962\n",
      "Epoch: 1606 \tTraining Loss: 0.873062 \tValidation Loss: 0.957963\n",
      "Epoch: 1607 \tTraining Loss: 0.873043 \tValidation Loss: 0.957965\n",
      "Epoch: 1608 \tTraining Loss: 0.873024 \tValidation Loss: 0.957967\n",
      "Epoch: 1609 \tTraining Loss: 0.873005 \tValidation Loss: 0.957968\n",
      "Epoch: 1610 \tTraining Loss: 0.872986 \tValidation Loss: 0.957970\n",
      "Epoch: 1611 \tTraining Loss: 0.872967 \tValidation Loss: 0.957971\n",
      "Epoch: 1612 \tTraining Loss: 0.872948 \tValidation Loss: 0.957973\n",
      "Epoch: 1613 \tTraining Loss: 0.872929 \tValidation Loss: 0.957975\n",
      "Epoch: 1614 \tTraining Loss: 0.872910 \tValidation Loss: 0.957976\n",
      "Epoch: 1615 \tTraining Loss: 0.872891 \tValidation Loss: 0.957978\n",
      "Epoch: 1616 \tTraining Loss: 0.872873 \tValidation Loss: 0.957979\n",
      "Epoch: 1617 \tTraining Loss: 0.872854 \tValidation Loss: 0.957981\n",
      "Epoch: 1618 \tTraining Loss: 0.872835 \tValidation Loss: 0.957983\n",
      "Epoch: 1619 \tTraining Loss: 0.872816 \tValidation Loss: 0.957984\n",
      "Epoch: 1620 \tTraining Loss: 0.872797 \tValidation Loss: 0.957986\n",
      "Epoch: 1621 \tTraining Loss: 0.872778 \tValidation Loss: 0.957987\n",
      "Epoch: 1622 \tTraining Loss: 0.872759 \tValidation Loss: 0.957989\n",
      "Epoch: 1623 \tTraining Loss: 0.872741 \tValidation Loss: 0.957991\n",
      "Epoch: 1624 \tTraining Loss: 0.872722 \tValidation Loss: 0.957992\n",
      "Epoch: 1625 \tTraining Loss: 0.872703 \tValidation Loss: 0.957994\n",
      "Epoch: 1626 \tTraining Loss: 0.872684 \tValidation Loss: 0.957996\n",
      "Epoch: 1627 \tTraining Loss: 0.872666 \tValidation Loss: 0.957997\n",
      "Epoch: 1628 \tTraining Loss: 0.872647 \tValidation Loss: 0.957999\n",
      "Epoch: 1629 \tTraining Loss: 0.872628 \tValidation Loss: 0.958001\n",
      "Epoch: 1630 \tTraining Loss: 0.872609 \tValidation Loss: 0.958002\n",
      "Epoch: 1631 \tTraining Loss: 0.872591 \tValidation Loss: 0.958004\n",
      "Epoch: 1632 \tTraining Loss: 0.872572 \tValidation Loss: 0.958006\n",
      "Epoch: 1633 \tTraining Loss: 0.872553 \tValidation Loss: 0.958007\n",
      "Epoch: 1634 \tTraining Loss: 0.872535 \tValidation Loss: 0.958009\n",
      "Epoch: 1635 \tTraining Loss: 0.872516 \tValidation Loss: 0.958011\n",
      "Epoch: 1636 \tTraining Loss: 0.872497 \tValidation Loss: 0.958012\n",
      "Epoch: 1637 \tTraining Loss: 0.872479 \tValidation Loss: 0.958014\n",
      "Epoch: 1638 \tTraining Loss: 0.872460 \tValidation Loss: 0.958016\n",
      "Epoch: 1639 \tTraining Loss: 0.872442 \tValidation Loss: 0.958018\n",
      "Epoch: 1640 \tTraining Loss: 0.872423 \tValidation Loss: 0.958019\n",
      "Epoch: 1641 \tTraining Loss: 0.872405 \tValidation Loss: 0.958021\n",
      "Epoch: 1642 \tTraining Loss: 0.872386 \tValidation Loss: 0.958023\n",
      "Epoch: 1643 \tTraining Loss: 0.872367 \tValidation Loss: 0.958025\n",
      "Epoch: 1644 \tTraining Loss: 0.872349 \tValidation Loss: 0.958026\n",
      "Epoch: 1645 \tTraining Loss: 0.872330 \tValidation Loss: 0.958028\n",
      "Epoch: 1646 \tTraining Loss: 0.872312 \tValidation Loss: 0.958030\n",
      "Epoch: 1647 \tTraining Loss: 0.872294 \tValidation Loss: 0.958031\n",
      "Epoch: 1648 \tTraining Loss: 0.872275 \tValidation Loss: 0.958033\n",
      "Epoch: 1649 \tTraining Loss: 0.872257 \tValidation Loss: 0.958035\n",
      "Epoch: 1650 \tTraining Loss: 0.872238 \tValidation Loss: 0.958037\n",
      "Epoch: 1651 \tTraining Loss: 0.872220 \tValidation Loss: 0.958038\n",
      "Epoch: 1652 \tTraining Loss: 0.872201 \tValidation Loss: 0.958040\n",
      "Epoch: 1653 \tTraining Loss: 0.872183 \tValidation Loss: 0.958042\n",
      "Epoch: 1654 \tTraining Loss: 0.872165 \tValidation Loss: 0.958044\n",
      "Epoch: 1655 \tTraining Loss: 0.872146 \tValidation Loss: 0.958046\n",
      "Epoch: 1656 \tTraining Loss: 0.872128 \tValidation Loss: 0.958047\n",
      "Epoch: 1657 \tTraining Loss: 0.872109 \tValidation Loss: 0.958049\n",
      "Epoch: 1658 \tTraining Loss: 0.872091 \tValidation Loss: 0.958051\n",
      "Epoch: 1659 \tTraining Loss: 0.872073 \tValidation Loss: 0.958053\n",
      "Epoch: 1660 \tTraining Loss: 0.872055 \tValidation Loss: 0.958055\n",
      "Epoch: 1661 \tTraining Loss: 0.872036 \tValidation Loss: 0.958056\n",
      "Epoch: 1662 \tTraining Loss: 0.872018 \tValidation Loss: 0.958058\n",
      "Epoch: 1663 \tTraining Loss: 0.872000 \tValidation Loss: 0.958060\n",
      "Epoch: 1664 \tTraining Loss: 0.871981 \tValidation Loss: 0.958062\n",
      "Epoch: 1665 \tTraining Loss: 0.871963 \tValidation Loss: 0.958064\n",
      "Epoch: 1666 \tTraining Loss: 0.871945 \tValidation Loss: 0.958065\n",
      "Epoch: 1667 \tTraining Loss: 0.871927 \tValidation Loss: 0.958067\n",
      "Epoch: 1668 \tTraining Loss: 0.871909 \tValidation Loss: 0.958069\n",
      "Epoch: 1669 \tTraining Loss: 0.871890 \tValidation Loss: 0.958071\n",
      "Epoch: 1670 \tTraining Loss: 0.871872 \tValidation Loss: 0.958073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1671 \tTraining Loss: 0.871854 \tValidation Loss: 0.958075\n",
      "Epoch: 1672 \tTraining Loss: 0.871836 \tValidation Loss: 0.958076\n",
      "Epoch: 1673 \tTraining Loss: 0.871818 \tValidation Loss: 0.958078\n",
      "Epoch: 1674 \tTraining Loss: 0.871800 \tValidation Loss: 0.958080\n",
      "Epoch: 1675 \tTraining Loss: 0.871782 \tValidation Loss: 0.958082\n",
      "Epoch: 1676 \tTraining Loss: 0.871763 \tValidation Loss: 0.958084\n",
      "Epoch: 1677 \tTraining Loss: 0.871745 \tValidation Loss: 0.958086\n",
      "Epoch: 1678 \tTraining Loss: 0.871727 \tValidation Loss: 0.958088\n",
      "Epoch: 1679 \tTraining Loss: 0.871709 \tValidation Loss: 0.958089\n",
      "Epoch: 1680 \tTraining Loss: 0.871691 \tValidation Loss: 0.958091\n",
      "Epoch: 1681 \tTraining Loss: 0.871673 \tValidation Loss: 0.958093\n",
      "Epoch: 1682 \tTraining Loss: 0.871655 \tValidation Loss: 0.958095\n",
      "Epoch: 1683 \tTraining Loss: 0.871637 \tValidation Loss: 0.958097\n",
      "Epoch: 1684 \tTraining Loss: 0.871619 \tValidation Loss: 0.958099\n",
      "Epoch: 1685 \tTraining Loss: 0.871601 \tValidation Loss: 0.958101\n",
      "Epoch: 1686 \tTraining Loss: 0.871583 \tValidation Loss: 0.958103\n",
      "Epoch: 1687 \tTraining Loss: 0.871565 \tValidation Loss: 0.958105\n",
      "Epoch: 1688 \tTraining Loss: 0.871547 \tValidation Loss: 0.958107\n",
      "Epoch: 1689 \tTraining Loss: 0.871529 \tValidation Loss: 0.958108\n",
      "Epoch: 1690 \tTraining Loss: 0.871511 \tValidation Loss: 0.958110\n",
      "Epoch: 1691 \tTraining Loss: 0.871494 \tValidation Loss: 0.958112\n",
      "Epoch: 1692 \tTraining Loss: 0.871476 \tValidation Loss: 0.958114\n",
      "Epoch: 1693 \tTraining Loss: 0.871458 \tValidation Loss: 0.958116\n",
      "Epoch: 1694 \tTraining Loss: 0.871440 \tValidation Loss: 0.958118\n",
      "Epoch: 1695 \tTraining Loss: 0.871422 \tValidation Loss: 0.958120\n",
      "Epoch: 1696 \tTraining Loss: 0.871404 \tValidation Loss: 0.958122\n",
      "Epoch: 1697 \tTraining Loss: 0.871386 \tValidation Loss: 0.958124\n",
      "Epoch: 1698 \tTraining Loss: 0.871369 \tValidation Loss: 0.958126\n",
      "Epoch: 1699 \tTraining Loss: 0.871351 \tValidation Loss: 0.958128\n",
      "Epoch: 1700 \tTraining Loss: 0.871333 \tValidation Loss: 0.958130\n",
      "Epoch: 1701 \tTraining Loss: 0.871315 \tValidation Loss: 0.958132\n",
      "Epoch: 1702 \tTraining Loss: 0.871297 \tValidation Loss: 0.958134\n",
      "Epoch: 1703 \tTraining Loss: 0.871280 \tValidation Loss: 0.958136\n",
      "Epoch: 1704 \tTraining Loss: 0.871262 \tValidation Loss: 0.958138\n",
      "Epoch: 1705 \tTraining Loss: 0.871244 \tValidation Loss: 0.958140\n",
      "Epoch: 1706 \tTraining Loss: 0.871226 \tValidation Loss: 0.958142\n",
      "Epoch: 1707 \tTraining Loss: 0.871209 \tValidation Loss: 0.958144\n",
      "Epoch: 1708 \tTraining Loss: 0.871191 \tValidation Loss: 0.958146\n",
      "Epoch: 1709 \tTraining Loss: 0.871173 \tValidation Loss: 0.958148\n",
      "Epoch: 1710 \tTraining Loss: 0.871156 \tValidation Loss: 0.958149\n",
      "Epoch: 1711 \tTraining Loss: 0.871138 \tValidation Loss: 0.958152\n",
      "Epoch: 1712 \tTraining Loss: 0.871120 \tValidation Loss: 0.958154\n",
      "Epoch: 1713 \tTraining Loss: 0.871103 \tValidation Loss: 0.958156\n",
      "Epoch: 1714 \tTraining Loss: 0.871085 \tValidation Loss: 0.958158\n",
      "Epoch: 1715 \tTraining Loss: 0.871068 \tValidation Loss: 0.958160\n",
      "Epoch: 1716 \tTraining Loss: 0.871050 \tValidation Loss: 0.958162\n",
      "Epoch: 1717 \tTraining Loss: 0.871032 \tValidation Loss: 0.958164\n",
      "Epoch: 1718 \tTraining Loss: 0.871015 \tValidation Loss: 0.958166\n",
      "Epoch: 1719 \tTraining Loss: 0.870997 \tValidation Loss: 0.958168\n",
      "Epoch: 1720 \tTraining Loss: 0.870980 \tValidation Loss: 0.958170\n",
      "Epoch: 1721 \tTraining Loss: 0.870962 \tValidation Loss: 0.958172\n",
      "Epoch: 1722 \tTraining Loss: 0.870945 \tValidation Loss: 0.958174\n",
      "Epoch: 1723 \tTraining Loss: 0.870927 \tValidation Loss: 0.958176\n",
      "Epoch: 1724 \tTraining Loss: 0.870910 \tValidation Loss: 0.958178\n",
      "Epoch: 1725 \tTraining Loss: 0.870892 \tValidation Loss: 0.958180\n",
      "Epoch: 1726 \tTraining Loss: 0.870875 \tValidation Loss: 0.958182\n",
      "Epoch: 1727 \tTraining Loss: 0.870857 \tValidation Loss: 0.958184\n",
      "Epoch: 1728 \tTraining Loss: 0.870840 \tValidation Loss: 0.958186\n",
      "Epoch: 1729 \tTraining Loss: 0.870822 \tValidation Loss: 0.958188\n",
      "Epoch: 1730 \tTraining Loss: 0.870805 \tValidation Loss: 0.958190\n",
      "Epoch: 1731 \tTraining Loss: 0.870787 \tValidation Loss: 0.958192\n",
      "Epoch: 1732 \tTraining Loss: 0.870770 \tValidation Loss: 0.958194\n",
      "Epoch: 1733 \tTraining Loss: 0.870753 \tValidation Loss: 0.958196\n",
      "Epoch: 1734 \tTraining Loss: 0.870735 \tValidation Loss: 0.958199\n",
      "Epoch: 1735 \tTraining Loss: 0.870718 \tValidation Loss: 0.958201\n",
      "Epoch: 1736 \tTraining Loss: 0.870701 \tValidation Loss: 0.958203\n",
      "Epoch: 1737 \tTraining Loss: 0.870683 \tValidation Loss: 0.958205\n",
      "Epoch: 1738 \tTraining Loss: 0.870666 \tValidation Loss: 0.958207\n",
      "Epoch: 1739 \tTraining Loss: 0.870649 \tValidation Loss: 0.958209\n",
      "Epoch: 1740 \tTraining Loss: 0.870631 \tValidation Loss: 0.958211\n",
      "Epoch: 1741 \tTraining Loss: 0.870614 \tValidation Loss: 0.958213\n",
      "Epoch: 1742 \tTraining Loss: 0.870597 \tValidation Loss: 0.958215\n",
      "Epoch: 1743 \tTraining Loss: 0.870579 \tValidation Loss: 0.958217\n",
      "Epoch: 1744 \tTraining Loss: 0.870562 \tValidation Loss: 0.958220\n",
      "Epoch: 1745 \tTraining Loss: 0.870545 \tValidation Loss: 0.958222\n",
      "Epoch: 1746 \tTraining Loss: 0.870528 \tValidation Loss: 0.958224\n",
      "Epoch: 1747 \tTraining Loss: 0.870511 \tValidation Loss: 0.958226\n",
      "Epoch: 1748 \tTraining Loss: 0.870493 \tValidation Loss: 0.958228\n",
      "Epoch: 1749 \tTraining Loss: 0.870476 \tValidation Loss: 0.958230\n",
      "Epoch: 1750 \tTraining Loss: 0.870459 \tValidation Loss: 0.958232\n",
      "Epoch: 1751 \tTraining Loss: 0.870442 \tValidation Loss: 0.958234\n",
      "Epoch: 1752 \tTraining Loss: 0.870425 \tValidation Loss: 0.958237\n",
      "Epoch: 1753 \tTraining Loss: 0.870407 \tValidation Loss: 0.958239\n",
      "Epoch: 1754 \tTraining Loss: 0.870390 \tValidation Loss: 0.958241\n",
      "Epoch: 1755 \tTraining Loss: 0.870373 \tValidation Loss: 0.958243\n",
      "Epoch: 1756 \tTraining Loss: 0.870356 \tValidation Loss: 0.958245\n",
      "Epoch: 1757 \tTraining Loss: 0.870339 \tValidation Loss: 0.958247\n",
      "Epoch: 1758 \tTraining Loss: 0.870322 \tValidation Loss: 0.958250\n",
      "Epoch: 1759 \tTraining Loss: 0.870305 \tValidation Loss: 0.958252\n",
      "Epoch: 1760 \tTraining Loss: 0.870288 \tValidation Loss: 0.958254\n",
      "Epoch: 1761 \tTraining Loss: 0.870271 \tValidation Loss: 0.958256\n",
      "Epoch: 1762 \tTraining Loss: 0.870254 \tValidation Loss: 0.958258\n",
      "Epoch: 1763 \tTraining Loss: 0.870237 \tValidation Loss: 0.958261\n",
      "Epoch: 1764 \tTraining Loss: 0.870220 \tValidation Loss: 0.958263\n",
      "Epoch: 1765 \tTraining Loss: 0.870203 \tValidation Loss: 0.958265\n",
      "Epoch: 1766 \tTraining Loss: 0.870186 \tValidation Loss: 0.958267\n",
      "Epoch: 1767 \tTraining Loss: 0.870169 \tValidation Loss: 0.958269\n",
      "Epoch: 1768 \tTraining Loss: 0.870152 \tValidation Loss: 0.958271\n",
      "Epoch: 1769 \tTraining Loss: 0.870135 \tValidation Loss: 0.958274\n",
      "Epoch: 1770 \tTraining Loss: 0.870118 \tValidation Loss: 0.958276\n",
      "Epoch: 1771 \tTraining Loss: 0.870101 \tValidation Loss: 0.958278\n",
      "Epoch: 1772 \tTraining Loss: 0.870084 \tValidation Loss: 0.958280\n",
      "Epoch: 1773 \tTraining Loss: 0.870067 \tValidation Loss: 0.958283\n",
      "Epoch: 1774 \tTraining Loss: 0.870050 \tValidation Loss: 0.958285\n",
      "Epoch: 1775 \tTraining Loss: 0.870033 \tValidation Loss: 0.958287\n",
      "Epoch: 1776 \tTraining Loss: 0.870016 \tValidation Loss: 0.958289\n",
      "Epoch: 1777 \tTraining Loss: 0.869999 \tValidation Loss: 0.958291\n",
      "Epoch: 1778 \tTraining Loss: 0.869982 \tValidation Loss: 0.958294\n",
      "Epoch: 1779 \tTraining Loss: 0.869966 \tValidation Loss: 0.958296\n",
      "Epoch: 1780 \tTraining Loss: 0.869949 \tValidation Loss: 0.958298\n",
      "Epoch: 1781 \tTraining Loss: 0.869932 \tValidation Loss: 0.958300\n",
      "Epoch: 1782 \tTraining Loss: 0.869915 \tValidation Loss: 0.958303\n",
      "Epoch: 1783 \tTraining Loss: 0.869898 \tValidation Loss: 0.958305\n",
      "Epoch: 1784 \tTraining Loss: 0.869882 \tValidation Loss: 0.958307\n",
      "Epoch: 1785 \tTraining Loss: 0.869865 \tValidation Loss: 0.958309\n",
      "Epoch: 1786 \tTraining Loss: 0.869848 \tValidation Loss: 0.958312\n",
      "Epoch: 1787 \tTraining Loss: 0.869831 \tValidation Loss: 0.958314\n",
      "Epoch: 1788 \tTraining Loss: 0.869814 \tValidation Loss: 0.958316\n",
      "Epoch: 1789 \tTraining Loss: 0.869798 \tValidation Loss: 0.958318\n",
      "Epoch: 1790 \tTraining Loss: 0.869781 \tValidation Loss: 0.958321\n",
      "Epoch: 1791 \tTraining Loss: 0.869764 \tValidation Loss: 0.958323\n",
      "Epoch: 1792 \tTraining Loss: 0.869748 \tValidation Loss: 0.958325\n",
      "Epoch: 1793 \tTraining Loss: 0.869731 \tValidation Loss: 0.958328\n",
      "Epoch: 1794 \tTraining Loss: 0.869714 \tValidation Loss: 0.958330\n",
      "Epoch: 1795 \tTraining Loss: 0.869698 \tValidation Loss: 0.958332\n",
      "Epoch: 1796 \tTraining Loss: 0.869681 \tValidation Loss: 0.958335\n",
      "Epoch: 1797 \tTraining Loss: 0.869664 \tValidation Loss: 0.958337\n",
      "Epoch: 1798 \tTraining Loss: 0.869648 \tValidation Loss: 0.958339\n",
      "Epoch: 1799 \tTraining Loss: 0.869631 \tValidation Loss: 0.958341\n",
      "Epoch: 1800 \tTraining Loss: 0.869614 \tValidation Loss: 0.958344\n",
      "Epoch: 1801 \tTraining Loss: 0.869598 \tValidation Loss: 0.958346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1802 \tTraining Loss: 0.869581 \tValidation Loss: 0.958348\n",
      "Epoch: 1803 \tTraining Loss: 0.869565 \tValidation Loss: 0.958351\n",
      "Epoch: 1804 \tTraining Loss: 0.869548 \tValidation Loss: 0.958353\n",
      "Epoch: 1805 \tTraining Loss: 0.869532 \tValidation Loss: 0.958355\n",
      "Epoch: 1806 \tTraining Loss: 0.869515 \tValidation Loss: 0.958358\n",
      "Epoch: 1807 \tTraining Loss: 0.869498 \tValidation Loss: 0.958360\n",
      "Epoch: 1808 \tTraining Loss: 0.869482 \tValidation Loss: 0.958362\n",
      "Epoch: 1809 \tTraining Loss: 0.869465 \tValidation Loss: 0.958365\n",
      "Epoch: 1810 \tTraining Loss: 0.869449 \tValidation Loss: 0.958367\n",
      "Epoch: 1811 \tTraining Loss: 0.869432 \tValidation Loss: 0.958369\n",
      "Epoch: 1812 \tTraining Loss: 0.869416 \tValidation Loss: 0.958372\n",
      "Epoch: 1813 \tTraining Loss: 0.869399 \tValidation Loss: 0.958374\n",
      "Epoch: 1814 \tTraining Loss: 0.869383 \tValidation Loss: 0.958376\n",
      "Epoch: 1815 \tTraining Loss: 0.869367 \tValidation Loss: 0.958379\n",
      "Epoch: 1816 \tTraining Loss: 0.869350 \tValidation Loss: 0.958381\n",
      "Epoch: 1817 \tTraining Loss: 0.869334 \tValidation Loss: 0.958383\n",
      "Epoch: 1818 \tTraining Loss: 0.869317 \tValidation Loss: 0.958386\n",
      "Epoch: 1819 \tTraining Loss: 0.869301 \tValidation Loss: 0.958388\n",
      "Epoch: 1820 \tTraining Loss: 0.869284 \tValidation Loss: 0.958391\n",
      "Epoch: 1821 \tTraining Loss: 0.869268 \tValidation Loss: 0.958393\n",
      "Epoch: 1822 \tTraining Loss: 0.869252 \tValidation Loss: 0.958395\n",
      "Epoch: 1823 \tTraining Loss: 0.869235 \tValidation Loss: 0.958398\n",
      "Epoch: 1824 \tTraining Loss: 0.869219 \tValidation Loss: 0.958400\n",
      "Epoch: 1825 \tTraining Loss: 0.869203 \tValidation Loss: 0.958402\n",
      "Epoch: 1826 \tTraining Loss: 0.869186 \tValidation Loss: 0.958405\n",
      "Epoch: 1827 \tTraining Loss: 0.869170 \tValidation Loss: 0.958407\n",
      "Epoch: 1828 \tTraining Loss: 0.869154 \tValidation Loss: 0.958410\n",
      "Epoch: 1829 \tTraining Loss: 0.869137 \tValidation Loss: 0.958412\n",
      "Epoch: 1830 \tTraining Loss: 0.869121 \tValidation Loss: 0.958414\n",
      "Epoch: 1831 \tTraining Loss: 0.869105 \tValidation Loss: 0.958417\n",
      "Epoch: 1832 \tTraining Loss: 0.869089 \tValidation Loss: 0.958419\n",
      "Epoch: 1833 \tTraining Loss: 0.869072 \tValidation Loss: 0.958422\n",
      "Epoch: 1834 \tTraining Loss: 0.869056 \tValidation Loss: 0.958424\n",
      "Epoch: 1835 \tTraining Loss: 0.869040 \tValidation Loss: 0.958427\n",
      "Epoch: 1836 \tTraining Loss: 0.869024 \tValidation Loss: 0.958429\n",
      "Epoch: 1837 \tTraining Loss: 0.869007 \tValidation Loss: 0.958431\n",
      "Epoch: 1838 \tTraining Loss: 0.868991 \tValidation Loss: 0.958434\n",
      "Epoch: 1839 \tTraining Loss: 0.868975 \tValidation Loss: 0.958436\n",
      "Epoch: 1840 \tTraining Loss: 0.868959 \tValidation Loss: 0.958439\n",
      "Epoch: 1841 \tTraining Loss: 0.868943 \tValidation Loss: 0.958441\n",
      "Epoch: 1842 \tTraining Loss: 0.868927 \tValidation Loss: 0.958444\n",
      "Epoch: 1843 \tTraining Loss: 0.868910 \tValidation Loss: 0.958446\n",
      "Epoch: 1844 \tTraining Loss: 0.868894 \tValidation Loss: 0.958448\n",
      "Epoch: 1845 \tTraining Loss: 0.868878 \tValidation Loss: 0.958451\n",
      "Epoch: 1846 \tTraining Loss: 0.868862 \tValidation Loss: 0.958453\n",
      "Epoch: 1847 \tTraining Loss: 0.868846 \tValidation Loss: 0.958456\n",
      "Epoch: 1848 \tTraining Loss: 0.868830 \tValidation Loss: 0.958458\n",
      "Epoch: 1849 \tTraining Loss: 0.868814 \tValidation Loss: 0.958461\n",
      "Epoch: 1850 \tTraining Loss: 0.868798 \tValidation Loss: 0.958463\n",
      "Epoch: 1851 \tTraining Loss: 0.868782 \tValidation Loss: 0.958466\n",
      "Epoch: 1852 \tTraining Loss: 0.868766 \tValidation Loss: 0.958468\n",
      "Epoch: 1853 \tTraining Loss: 0.868750 \tValidation Loss: 0.958471\n",
      "Epoch: 1854 \tTraining Loss: 0.868733 \tValidation Loss: 0.958473\n",
      "Epoch: 1855 \tTraining Loss: 0.868717 \tValidation Loss: 0.958476\n",
      "Epoch: 1856 \tTraining Loss: 0.868701 \tValidation Loss: 0.958478\n",
      "Epoch: 1857 \tTraining Loss: 0.868685 \tValidation Loss: 0.958481\n",
      "Epoch: 1858 \tTraining Loss: 0.868669 \tValidation Loss: 0.958483\n",
      "Epoch: 1859 \tTraining Loss: 0.868654 \tValidation Loss: 0.958486\n",
      "Epoch: 1860 \tTraining Loss: 0.868638 \tValidation Loss: 0.958488\n",
      "Epoch: 1861 \tTraining Loss: 0.868622 \tValidation Loss: 0.958490\n",
      "Epoch: 1862 \tTraining Loss: 0.868606 \tValidation Loss: 0.958493\n",
      "Epoch: 1863 \tTraining Loss: 0.868590 \tValidation Loss: 0.958495\n",
      "Epoch: 1864 \tTraining Loss: 0.868574 \tValidation Loss: 0.958498\n",
      "Epoch: 1865 \tTraining Loss: 0.868558 \tValidation Loss: 0.958501\n",
      "Epoch: 1866 \tTraining Loss: 0.868542 \tValidation Loss: 0.958503\n",
      "Epoch: 1867 \tTraining Loss: 0.868526 \tValidation Loss: 0.958506\n",
      "Epoch: 1868 \tTraining Loss: 0.868510 \tValidation Loss: 0.958508\n",
      "Epoch: 1869 \tTraining Loss: 0.868494 \tValidation Loss: 0.958511\n",
      "Epoch: 1870 \tTraining Loss: 0.868478 \tValidation Loss: 0.958513\n",
      "Epoch: 1871 \tTraining Loss: 0.868463 \tValidation Loss: 0.958516\n",
      "Epoch: 1872 \tTraining Loss: 0.868447 \tValidation Loss: 0.958518\n",
      "Epoch: 1873 \tTraining Loss: 0.868431 \tValidation Loss: 0.958521\n",
      "Epoch: 1874 \tTraining Loss: 0.868415 \tValidation Loss: 0.958523\n",
      "Epoch: 1875 \tTraining Loss: 0.868399 \tValidation Loss: 0.958526\n",
      "Epoch: 1876 \tTraining Loss: 0.868383 \tValidation Loss: 0.958528\n",
      "Epoch: 1877 \tTraining Loss: 0.868368 \tValidation Loss: 0.958531\n",
      "Epoch: 1878 \tTraining Loss: 0.868352 \tValidation Loss: 0.958533\n",
      "Epoch: 1879 \tTraining Loss: 0.868336 \tValidation Loss: 0.958536\n",
      "Epoch: 1880 \tTraining Loss: 0.868320 \tValidation Loss: 0.958538\n",
      "Epoch: 1881 \tTraining Loss: 0.868305 \tValidation Loss: 0.958541\n",
      "Epoch: 1882 \tTraining Loss: 0.868289 \tValidation Loss: 0.958544\n",
      "Epoch: 1883 \tTraining Loss: 0.868273 \tValidation Loss: 0.958546\n",
      "Epoch: 1884 \tTraining Loss: 0.868257 \tValidation Loss: 0.958549\n",
      "Epoch: 1885 \tTraining Loss: 0.868242 \tValidation Loss: 0.958551\n",
      "Epoch: 1886 \tTraining Loss: 0.868226 \tValidation Loss: 0.958554\n",
      "Epoch: 1887 \tTraining Loss: 0.868210 \tValidation Loss: 0.958556\n",
      "Epoch: 1888 \tTraining Loss: 0.868195 \tValidation Loss: 0.958559\n",
      "Epoch: 1889 \tTraining Loss: 0.868179 \tValidation Loss: 0.958562\n",
      "Epoch: 1890 \tTraining Loss: 0.868163 \tValidation Loss: 0.958564\n",
      "Epoch: 1891 \tTraining Loss: 0.868148 \tValidation Loss: 0.958567\n",
      "Epoch: 1892 \tTraining Loss: 0.868132 \tValidation Loss: 0.958569\n",
      "Epoch: 1893 \tTraining Loss: 0.868116 \tValidation Loss: 0.958572\n",
      "Epoch: 1894 \tTraining Loss: 0.868101 \tValidation Loss: 0.958575\n",
      "Epoch: 1895 \tTraining Loss: 0.868085 \tValidation Loss: 0.958577\n",
      "Epoch: 1896 \tTraining Loss: 0.868070 \tValidation Loss: 0.958580\n",
      "Epoch: 1897 \tTraining Loss: 0.868054 \tValidation Loss: 0.958582\n",
      "Epoch: 1898 \tTraining Loss: 0.868038 \tValidation Loss: 0.958585\n",
      "Epoch: 1899 \tTraining Loss: 0.868023 \tValidation Loss: 0.958587\n",
      "Epoch: 1900 \tTraining Loss: 0.868007 \tValidation Loss: 0.958590\n",
      "Epoch: 1901 \tTraining Loss: 0.867992 \tValidation Loss: 0.958593\n",
      "Epoch: 1902 \tTraining Loss: 0.867976 \tValidation Loss: 0.958595\n",
      "Epoch: 1903 \tTraining Loss: 0.867961 \tValidation Loss: 0.958598\n",
      "Epoch: 1904 \tTraining Loss: 0.867945 \tValidation Loss: 0.958601\n",
      "Epoch: 1905 \tTraining Loss: 0.867930 \tValidation Loss: 0.958603\n",
      "Epoch: 1906 \tTraining Loss: 0.867914 \tValidation Loss: 0.958606\n",
      "Epoch: 1907 \tTraining Loss: 0.867899 \tValidation Loss: 0.958608\n",
      "Epoch: 1908 \tTraining Loss: 0.867883 \tValidation Loss: 0.958611\n",
      "Epoch: 1909 \tTraining Loss: 0.867868 \tValidation Loss: 0.958614\n",
      "Epoch: 1910 \tTraining Loss: 0.867852 \tValidation Loss: 0.958616\n",
      "Epoch: 1911 \tTraining Loss: 0.867837 \tValidation Loss: 0.958619\n",
      "Epoch: 1912 \tTraining Loss: 0.867821 \tValidation Loss: 0.958622\n",
      "Epoch: 1913 \tTraining Loss: 0.867806 \tValidation Loss: 0.958624\n",
      "Epoch: 1914 \tTraining Loss: 0.867790 \tValidation Loss: 0.958627\n",
      "Epoch: 1915 \tTraining Loss: 0.867775 \tValidation Loss: 0.958630\n",
      "Epoch: 1916 \tTraining Loss: 0.867760 \tValidation Loss: 0.958632\n",
      "Epoch: 1917 \tTraining Loss: 0.867744 \tValidation Loss: 0.958635\n",
      "Epoch: 1918 \tTraining Loss: 0.867729 \tValidation Loss: 0.958637\n",
      "Epoch: 1919 \tTraining Loss: 0.867713 \tValidation Loss: 0.958640\n",
      "Epoch: 1920 \tTraining Loss: 0.867698 \tValidation Loss: 0.958643\n",
      "Epoch: 1921 \tTraining Loss: 0.867683 \tValidation Loss: 0.958645\n",
      "Epoch: 1922 \tTraining Loss: 0.867667 \tValidation Loss: 0.958648\n",
      "Epoch: 1923 \tTraining Loss: 0.867652 \tValidation Loss: 0.958651\n",
      "Epoch: 1924 \tTraining Loss: 0.867637 \tValidation Loss: 0.958653\n",
      "Epoch: 1925 \tTraining Loss: 0.867621 \tValidation Loss: 0.958656\n",
      "Epoch: 1926 \tTraining Loss: 0.867606 \tValidation Loss: 0.958659\n",
      "Epoch: 1927 \tTraining Loss: 0.867591 \tValidation Loss: 0.958661\n",
      "Epoch: 1928 \tTraining Loss: 0.867576 \tValidation Loss: 0.958664\n",
      "Epoch: 1929 \tTraining Loss: 0.867560 \tValidation Loss: 0.958667\n",
      "Epoch: 1930 \tTraining Loss: 0.867545 \tValidation Loss: 0.958670\n",
      "Epoch: 1931 \tTraining Loss: 0.867530 \tValidation Loss: 0.958672\n",
      "Epoch: 1932 \tTraining Loss: 0.867515 \tValidation Loss: 0.958675\n",
      "Epoch: 1933 \tTraining Loss: 0.867499 \tValidation Loss: 0.958678\n",
      "Epoch: 1934 \tTraining Loss: 0.867484 \tValidation Loss: 0.958680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1935 \tTraining Loss: 0.867469 \tValidation Loss: 0.958683\n",
      "Epoch: 1936 \tTraining Loss: 0.867454 \tValidation Loss: 0.958686\n",
      "Epoch: 1937 \tTraining Loss: 0.867438 \tValidation Loss: 0.958688\n",
      "Epoch: 1938 \tTraining Loss: 0.867423 \tValidation Loss: 0.958691\n",
      "Epoch: 1939 \tTraining Loss: 0.867408 \tValidation Loss: 0.958694\n",
      "Epoch: 1940 \tTraining Loss: 0.867393 \tValidation Loss: 0.958697\n",
      "Epoch: 1941 \tTraining Loss: 0.867378 \tValidation Loss: 0.958699\n",
      "Epoch: 1942 \tTraining Loss: 0.867363 \tValidation Loss: 0.958702\n",
      "Epoch: 1943 \tTraining Loss: 0.867347 \tValidation Loss: 0.958705\n",
      "Epoch: 1944 \tTraining Loss: 0.867332 \tValidation Loss: 0.958707\n",
      "Epoch: 1945 \tTraining Loss: 0.867317 \tValidation Loss: 0.958710\n",
      "Epoch: 1946 \tTraining Loss: 0.867302 \tValidation Loss: 0.958713\n",
      "Epoch: 1947 \tTraining Loss: 0.867287 \tValidation Loss: 0.958716\n",
      "Epoch: 1948 \tTraining Loss: 0.867272 \tValidation Loss: 0.958718\n",
      "Epoch: 1949 \tTraining Loss: 0.867257 \tValidation Loss: 0.958721\n",
      "Epoch: 1950 \tTraining Loss: 0.867242 \tValidation Loss: 0.958724\n",
      "Epoch: 1951 \tTraining Loss: 0.867227 \tValidation Loss: 0.958727\n",
      "Epoch: 1952 \tTraining Loss: 0.867212 \tValidation Loss: 0.958729\n",
      "Epoch: 1953 \tTraining Loss: 0.867197 \tValidation Loss: 0.958732\n",
      "Epoch: 1954 \tTraining Loss: 0.867181 \tValidation Loss: 0.958735\n",
      "Epoch: 1955 \tTraining Loss: 0.867166 \tValidation Loss: 0.958738\n",
      "Epoch: 1956 \tTraining Loss: 0.867151 \tValidation Loss: 0.958740\n",
      "Epoch: 1957 \tTraining Loss: 0.867136 \tValidation Loss: 0.958743\n",
      "Epoch: 1958 \tTraining Loss: 0.867121 \tValidation Loss: 0.958746\n",
      "Epoch: 1959 \tTraining Loss: 0.867106 \tValidation Loss: 0.958749\n",
      "Epoch: 1960 \tTraining Loss: 0.867091 \tValidation Loss: 0.958751\n",
      "Epoch: 1961 \tTraining Loss: 0.867076 \tValidation Loss: 0.958754\n",
      "Epoch: 1962 \tTraining Loss: 0.867062 \tValidation Loss: 0.958757\n",
      "Epoch: 1963 \tTraining Loss: 0.867047 \tValidation Loss: 0.958760\n",
      "Epoch: 1964 \tTraining Loss: 0.867032 \tValidation Loss: 0.958762\n",
      "Epoch: 1965 \tTraining Loss: 0.867017 \tValidation Loss: 0.958765\n",
      "Epoch: 1966 \tTraining Loss: 0.867002 \tValidation Loss: 0.958768\n",
      "Epoch: 1967 \tTraining Loss: 0.866987 \tValidation Loss: 0.958771\n",
      "Epoch: 1968 \tTraining Loss: 0.866972 \tValidation Loss: 0.958773\n",
      "Epoch: 1969 \tTraining Loss: 0.866957 \tValidation Loss: 0.958776\n",
      "Epoch: 1970 \tTraining Loss: 0.866942 \tValidation Loss: 0.958779\n",
      "Epoch: 1971 \tTraining Loss: 0.866927 \tValidation Loss: 0.958782\n",
      "Epoch: 1972 \tTraining Loss: 0.866912 \tValidation Loss: 0.958785\n",
      "Epoch: 1973 \tTraining Loss: 0.866898 \tValidation Loss: 0.958787\n",
      "Epoch: 1974 \tTraining Loss: 0.866883 \tValidation Loss: 0.958790\n",
      "Epoch: 1975 \tTraining Loss: 0.866868 \tValidation Loss: 0.958793\n",
      "Epoch: 1976 \tTraining Loss: 0.866853 \tValidation Loss: 0.958796\n",
      "Epoch: 1977 \tTraining Loss: 0.866838 \tValidation Loss: 0.958799\n",
      "Epoch: 1978 \tTraining Loss: 0.866823 \tValidation Loss: 0.958801\n",
      "Epoch: 1979 \tTraining Loss: 0.866809 \tValidation Loss: 0.958804\n",
      "Epoch: 1980 \tTraining Loss: 0.866794 \tValidation Loss: 0.958807\n",
      "Epoch: 1981 \tTraining Loss: 0.866779 \tValidation Loss: 0.958810\n",
      "Epoch: 1982 \tTraining Loss: 0.866764 \tValidation Loss: 0.958813\n",
      "Epoch: 1983 \tTraining Loss: 0.866749 \tValidation Loss: 0.958815\n",
      "Epoch: 1984 \tTraining Loss: 0.866735 \tValidation Loss: 0.958818\n",
      "Epoch: 1985 \tTraining Loss: 0.866720 \tValidation Loss: 0.958821\n",
      "Epoch: 1986 \tTraining Loss: 0.866705 \tValidation Loss: 0.958824\n",
      "Epoch: 1987 \tTraining Loss: 0.866690 \tValidation Loss: 0.958827\n",
      "Epoch: 1988 \tTraining Loss: 0.866676 \tValidation Loss: 0.958830\n",
      "Epoch: 1989 \tTraining Loss: 0.866661 \tValidation Loss: 0.958832\n",
      "Epoch: 1990 \tTraining Loss: 0.866646 \tValidation Loss: 0.958835\n",
      "Epoch: 1991 \tTraining Loss: 0.866632 \tValidation Loss: 0.958838\n",
      "Epoch: 1992 \tTraining Loss: 0.866617 \tValidation Loss: 0.958841\n",
      "Epoch: 1993 \tTraining Loss: 0.866602 \tValidation Loss: 0.958844\n",
      "Epoch: 1994 \tTraining Loss: 0.866588 \tValidation Loss: 0.958847\n",
      "Epoch: 1995 \tTraining Loss: 0.866573 \tValidation Loss: 0.958850\n",
      "Epoch: 1996 \tTraining Loss: 0.866558 \tValidation Loss: 0.958852\n",
      "Epoch: 1997 \tTraining Loss: 0.866544 \tValidation Loss: 0.958855\n",
      "Epoch: 1998 \tTraining Loss: 0.866529 \tValidation Loss: 0.958858\n",
      "Epoch: 1999 \tTraining Loss: 0.866514 \tValidation Loss: 0.958861\n",
      "Epoch: 2000 \tTraining Loss: 0.866500 \tValidation Loss: 0.958864\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 1500\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    linear_model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = linear_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    linear_model.eval() # prep model for evaluation\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = linear_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(test_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}' \\\n",
    "          .format(epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). \\\n",
    "        Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "        \n",
    "        torch.save(linear_model.state_dict(), 'mbert_model_linear.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fb3124d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.60      0.63      3679\n",
      "           1       0.55      0.61      0.58      2809\n",
      "           2       0.58      0.59      0.58      3208\n",
      "\n",
      "    accuracy                           0.60      9696\n",
      "   macro avg       0.60      0.60      0.60      9696\n",
      "weighted avg       0.60      0.60      0.60      9696\n",
      "\n",
      "Testing Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.54      0.57       922\n",
      "           1       0.49      0.55      0.52       702\n",
      "           2       0.52      0.53      0.52       800\n",
      "\n",
      "    accuracy                           0.54      2424\n",
      "   macro avg       0.54      0.54      0.54      2424\n",
      "weighted avg       0.54      0.54      0.54      2424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linear_model = Net()\n",
    "linear_model.load_state_dict(torch.load(\"mbert_model_linear.pt\"))\n",
    "linear_model.eval()\n",
    "\n",
    "pred_list = torch.zeros(0, dtype=torch.long)\n",
    "target_list = torch.zeros(0, dtype=torch.long)\n",
    "train_count = 0\n",
    "count = 0\n",
    "\n",
    "train_pred_list = torch.zeros(0, dtype=torch.long)\n",
    "train_target_list = torch.zeros(0, dtype=torch.long)\n",
    "\n",
    "for data, target in train_loader:\n",
    "    output = linear_model(data)\n",
    "    _, preds = torch.max(output, 1)\n",
    "    train_pred_list = torch.cat([train_pred_list, preds.view(-1)])\n",
    "    train_target_list = torch.cat([train_target_list, target.view(-1)])\n",
    "#     for x in range(16):\n",
    "#         if preds[x] == target[x]: \n",
    "#             train_count+=1\n",
    "            \n",
    "# print(\"Training Accuracy =\", train_count / len(train_loader.dataset))\n",
    "train_result = classification_report(train_pred_list.numpy(), \n",
    "                                     train_target_list.numpy())\n",
    "print(\"Training Classification report: \\n\", train_result)\n",
    "            \n",
    "for data, target in test_loader:\n",
    "    output = linear_model(data)\n",
    "    _, preds = torch.max(output, 1) \n",
    "    pred_list = torch.cat([pred_list, preds.view(-1)])\n",
    "    target_list = torch.cat([target_list, target.view(-1)])\n",
    "#     for x in range(16):\n",
    "#         if preds[x] == target[x]: \n",
    "#             count+=1\n",
    "\n",
    "# print(\"Testing Accuracy =\", count / len(test_loader.dataset))\n",
    "\n",
    "result = classification_report(pred_list.numpy(), target_list.numpy())\n",
    "print(\"Testing Classification report: \\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e23212c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
